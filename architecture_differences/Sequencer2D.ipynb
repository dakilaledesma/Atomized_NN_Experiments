{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dakilaledesma/Atomized_NN_Experiments/blob/main/architecture_differences/Sequencer2D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RBv2CGN3gJv",
        "outputId": "d70eade4-0b2d-4a8b-8f13-8353e2152801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 136 ms, sys: 21.8 ms, total: 157 ms\n",
            "Wall time: 19.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "! unzip -q drive/MyDrive/UNC/H2022/orchidaceae_train.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXOHMDgapMby",
        "outputId": "b65d869c-dd26-4532-a06e-c31e332db006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-image-models'...\n",
            "remote: Enumerating objects: 10677, done.\u001b[K\n",
            "remote: Counting objects: 100% (208/208), done.\u001b[K\n",
            "remote: Compressing objects: 100% (125/125), done.\u001b[K\n",
            "remote: Total 10677 (delta 100), reused 152 (delta 78), pack-reused 10469\u001b[K\n",
            "Receiving objects: 100% (10677/10677), 20.34 MiB | 25.16 MiB/s, done.\n",
            "Resolving deltas: 100% (7813/7813), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/rwightman/pytorch-image-models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slckThLBilky"
      },
      "outputs": [],
      "source": [
        "import fileinput\n",
        "import sys\n",
        "\n",
        "def replacement(file, previousw, nextw):\n",
        "   for line in fileinput.input(file, inplace=1):\n",
        "       line = line.replace(previousw, nextw)\n",
        "       sys.stdout.write(line)\n",
        "\n",
        "file = \"/content/pytorch-image-models/timm/utils/checkpoint_saver.py\"\n",
        "replacement(file, \"if os.path.exists(last_save_path):\", \"# if os.path.exists(last_save_path):\")\n",
        "replacement(file, \"os.unlink(last_save_path)  # required for Windows support.\", \"# os.unlink(last_save_path)  # required for Windows support.\")\n",
        "replacement(file, \"os.link(last_save_path, save_path)\", \"# os.link(last_save_path, save_path)\")\n",
        "replacement(file, \"os.unlink(best_save_path)\", \"# os.unlink(best_save_path)\")\n",
        "replacement(file, \"os.link(last_save_path, best_save_path)\", \"# os.link(last_save_path, best_save_path)\")\n",
        "replacement(file, \"if os.path.exists(best_save_path):\", \"# if os.path.exists(best_save_path):\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kIBgHy6pSFD",
        "outputId": "0277dbdf-8bed-4eb5-a9ea-e502036ca5f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use_env is set by default in torchrun.\n",
            "If your script expects `--local_rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  FutureWarning,\n",
            "Training with a single process on 1 GPUs.\n",
            "Loading pretrained weights from url (https://github.com/okojoalg/sequencer/releases/download/weights/sequencer2d_l.pth)\n",
            "Model sequencer2d_l created, param count:54028716\n",
            "Data processing configuration for current model + dataset:\n",
            "\tinput_size: (3, 600, 600)\n",
            "\tinterpolation: bicubic\n",
            "\tmean: (0.485, 0.456, 0.406)\n",
            "\tstd: (0.229, 0.224, 0.225)\n",
            "\tcrop_pct: 0.875\n",
            "Using native Torch AMP. Training in mixed precision.\n",
            "Scheduled epochs: 80\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Train: 0 [   0/2354 (  0%)]  Loss: 5.704 (5.70)  Time: 3.076s,    2.60/s  (3.076s,    2.60/s)  LR: 1.000e-04  Data: 0.821 (0.821)\n",
            "Train: 0 [  50/2354 (  2%)]  Loss: 5.526 (5.70)  Time: 0.465s,   17.20/s  (0.521s,   15.36/s)  LR: 1.000e-04  Data: 0.006 (0.023)\n",
            "Train: 0 [ 100/2354 (  4%)]  Loss: 5.794 (5.70)  Time: 0.475s,   16.83/s  (0.494s,   16.19/s)  LR: 1.000e-04  Data: 0.006 (0.016)\n",
            "Train: 0 [ 150/2354 (  6%)]  Loss: 5.565 (5.69)  Time: 0.463s,   17.28/s  (0.493s,   16.24/s)  LR: 1.000e-04  Data: 0.006 (0.013)\n",
            "Train: 0 [ 200/2354 (  8%)]  Loss: 5.395 (5.68)  Time: 0.474s,   16.88/s  (0.487s,   16.44/s)  LR: 1.000e-04  Data: 0.009 (0.012)\n",
            "Train: 0 [ 250/2354 ( 11%)]  Loss: 5.681 (5.68)  Time: 0.470s,   17.03/s  (0.483s,   16.57/s)  LR: 1.000e-04  Data: 0.007 (0.011)\n",
            "Train: 0 [ 300/2354 ( 13%)]  Loss: 5.693 (5.68)  Time: 0.468s,   17.09/s  (0.480s,   16.65/s)  LR: 1.000e-04  Data: 0.006 (0.010)\n",
            "Train: 0 [ 350/2354 ( 15%)]  Loss: 5.852 (5.68)  Time: 0.472s,   16.95/s  (0.479s,   16.71/s)  LR: 1.000e-04  Data: 0.009 (0.010)\n",
            "Train: 0 [ 400/2354 ( 17%)]  Loss: 6.047 (5.67)  Time: 0.466s,   17.17/s  (0.477s,   16.76/s)  LR: 1.000e-04  Data: 0.009 (0.010)\n",
            "Train: 0 [ 450/2354 ( 19%)]  Loss: 5.535 (5.67)  Time: 0.462s,   17.31/s  (0.479s,   16.70/s)  LR: 1.000e-04  Data: 0.007 (0.010)\n",
            "Train: 0 [ 500/2354 ( 21%)]  Loss: 5.650 (5.67)  Time: 0.464s,   17.23/s  (0.478s,   16.74/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 0 [ 550/2354 ( 23%)]  Loss: 5.564 (5.67)  Time: 0.477s,   16.78/s  (0.477s,   16.76/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 0 [ 600/2354 ( 25%)]  Loss: 5.337 (5.66)  Time: 0.470s,   17.02/s  (0.477s,   16.79/s)  LR: 1.000e-04  Data: 0.006 (0.009)\n",
            "Train: 0 [ 650/2354 ( 28%)]  Loss: 6.046 (5.66)  Time: 0.475s,   16.84/s  (0.476s,   16.81/s)  LR: 1.000e-04  Data: 0.006 (0.009)\n",
            "Train: 0 [ 700/2354 ( 30%)]  Loss: 5.664 (5.65)  Time: 0.467s,   17.13/s  (0.475s,   16.83/s)  LR: 1.000e-04  Data: 0.010 (0.009)\n",
            "Train: 0 [ 750/2354 ( 32%)]  Loss: 5.363 (5.65)  Time: 0.464s,   17.25/s  (0.476s,   16.79/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 0 [ 800/2354 ( 34%)]  Loss: 5.649 (5.65)  Time: 0.462s,   17.32/s  (0.476s,   16.81/s)  LR: 1.000e-04  Data: 0.006 (0.009)\n",
            "Train: 0 [ 850/2354 ( 36%)]  Loss: 5.598 (5.64)  Time: 0.478s,   16.73/s  (0.476s,   16.82/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 0 [ 900/2354 ( 38%)]  Loss: 5.520 (5.64)  Time: 0.467s,   17.12/s  (0.475s,   16.84/s)  LR: 1.000e-04  Data: 0.008 (0.009)\n",
            "Train: 0 [ 950/2354 ( 40%)]  Loss: 5.618 (5.64)  Time: 0.476s,   16.79/s  (0.475s,   16.85/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 0 [1000/2354 ( 42%)]  Loss: 5.741 (5.64)  Time: 0.483s,   16.55/s  (0.475s,   16.86/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 0 [1050/2354 ( 45%)]  Loss: 5.766 (5.64)  Time: 0.470s,   17.04/s  (0.475s,   16.83/s)  LR: 1.000e-04  Data: 0.010 (0.009)\n",
            "Train: 0 [1100/2354 ( 47%)]  Loss: 5.597 (5.64)  Time: 0.470s,   17.02/s  (0.475s,   16.84/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 0 [1150/2354 ( 49%)]  Loss: 5.645 (5.64)  Time: 0.464s,   17.26/s  (0.475s,   16.85/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 0 [1200/2354 ( 51%)]  Loss: 5.610 (5.64)  Time: 0.467s,   17.14/s  (0.474s,   16.86/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 0 [1250/2354 ( 53%)]  Loss: 5.447 (5.64)  Time: 0.467s,   17.12/s  (0.474s,   16.87/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 0 [1300/2354 ( 55%)]  Loss: 5.381 (5.63)  Time: 0.476s,   16.79/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 0 [1350/2354 ( 57%)]  Loss: 5.451 (5.63)  Time: 0.466s,   17.15/s  (0.475s,   16.86/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 0 [1400/2354 ( 59%)]  Loss: 5.867 (5.63)  Time: 0.464s,   17.25/s  (0.474s,   16.86/s)  LR: 1.000e-04  Data: 0.006 (0.008)\n",
            "Train: 0 [1450/2354 ( 62%)]  Loss: 5.616 (5.63)  Time: 0.473s,   16.93/s  (0.474s,   16.87/s)  LR: 1.000e-04  Data: 0.006 (0.008)\n",
            "Train: 0 [1500/2354 ( 64%)]  Loss: 5.367 (5.63)  Time: 0.462s,   17.32/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 0 [1550/2354 ( 66%)]  Loss: 5.508 (5.63)  Time: 0.471s,   16.97/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.008 (0.008)\n",
            "Train: 0 [1600/2354 ( 68%)]  Loss: 5.566 (5.63)  Time: 0.469s,   17.05/s  (0.474s,   16.89/s)  LR: 1.000e-04  Data: 0.009 (0.008)\n",
            "Train: 0 [1650/2354 ( 70%)]  Loss: 5.409 (5.62)  Time: 0.471s,   16.97/s  (0.474s,   16.87/s)  LR: 1.000e-04  Data: 0.009 (0.008)\n",
            "Train: 0 [1700/2354 ( 72%)]  Loss: 5.726 (5.62)  Time: 0.481s,   16.64/s  (0.474s,   16.87/s)  LR: 1.000e-04  Data: 0.019 (0.008)\n",
            "Train: 0 [1750/2354 ( 74%)]  Loss: 5.369 (5.62)  Time: 0.471s,   16.97/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 0 [1800/2354 ( 76%)]  Loss: 5.657 (5.62)  Time: 0.466s,   17.16/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.009 (0.008)\n",
            "Train: 0 [1850/2354 ( 79%)]  Loss: 5.884 (5.62)  Time: 0.462s,   17.33/s  (0.474s,   16.89/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 0 [1900/2354 ( 81%)]  Loss: 5.682 (5.62)  Time: 0.584s,   13.71/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 0 [1950/2354 ( 83%)]  Loss: 5.611 (5.61)  Time: 0.472s,   16.94/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 0 [2000/2354 ( 85%)]  Loss: 5.467 (5.61)  Time: 0.470s,   17.02/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.006 (0.008)\n",
            "Train: 0 [2050/2354 ( 87%)]  Loss: 5.457 (5.61)  Time: 0.477s,   16.76/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 0 [2100/2354 ( 89%)]  Loss: 5.486 (5.61)  Time: 0.465s,   17.21/s  (0.474s,   16.89/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 0 [2150/2354 ( 91%)]  Loss: 5.393 (5.61)  Time: 0.465s,   17.22/s  (0.474s,   16.89/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 0 [2200/2354 ( 93%)]  Loss: 5.777 (5.61)  Time: 0.467s,   17.15/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 0 [2250/2354 ( 96%)]  Loss: 5.854 (5.61)  Time: 0.465s,   17.21/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 0 [2300/2354 ( 98%)]  Loss: 5.255 (5.61)  Time: 0.477s,   16.76/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 0 [2350/2354 (100%)]  Loss: 5.511 (5.60)  Time: 0.462s,   17.30/s  (0.474s,   16.89/s)  LR: 1.000e-04  Data: 0.006 (0.008)\n",
            "Train: 0 [2353/2354 (100%)]  Loss: 5.354 (5.60)  Time: 0.458s,   17.48/s  (0.474s,   16.89/s)  LR: 1.000e-04  Data: 0.000 (0.008)\n",
            "Test: [   0/2354]  Time: 1.127 (1.127)  Loss:  7.1641 (7.1641)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test: [  50/2354]  Time: 0.098 (0.159)  Loss:  5.8672 (5.6546)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test: [ 100/2354]  Time: 0.170 (0.149)  Loss:  6.6133 (5.7577)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test: [ 150/2354]  Time: 0.126 (0.146)  Loss:  4.8984 (5.5588)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test: [ 200/2354]  Time: 0.136 (0.146)  Loss:  6.9844 (5.5860)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test: [ 250/2354]  Time: 0.164 (0.146)  Loss:  5.1602 (5.7108)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test: [ 300/2354]  Time: 0.102 (0.145)  Loss:  5.1523 (5.6437)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 1.4120)\n",
            "Test: [ 350/2354]  Time: 0.110 (0.144)  Loss:  5.5156 (5.5628)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 1.2108)\n",
            "Test: [ 400/2354]  Time: 0.158 (0.143)  Loss:  5.1328 (5.5318)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 1.0599)\n",
            "Test: [ 450/2354]  Time: 0.175 (0.147)  Loss:  5.2734 (5.4981)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.9424)\n",
            "Test: [ 500/2354]  Time: 0.155 (0.146)  Loss:  4.8984 (5.4844)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 1.4970)\n",
            "Test: [ 550/2354]  Time: 0.128 (0.146)  Loss:  5.1445 (5.4173)  Acc@1:  0.0000 ( 1.9964)  Acc@5:  0.0000 ( 4.2650)\n",
            "Test: [ 600/2354]  Time: 0.099 (0.145)  Loss:  5.2305 (5.4299)  Acc@1:  0.0000 ( 1.8303)  Acc@5:  0.0000 ( 3.9101)\n",
            "Test: [ 650/2354]  Time: 0.174 (0.144)  Loss:  5.4492 (5.4426)  Acc@1:  0.0000 ( 1.7665)  Acc@5:  0.0000 ( 4.3779)\n",
            "Test: [ 700/2354]  Time: 0.125 (0.144)  Loss:  5.2344 (5.4247)  Acc@1:  0.0000 ( 2.1755)  Acc@5:  0.0000 ( 5.3138)\n",
            "Test: [ 750/2354]  Time: 0.178 (0.144)  Loss:  5.2578 (5.4186)  Acc@1:  0.0000 ( 2.0306)  Acc@5:  0.0000 ( 4.9601)\n",
            "Test: [ 800/2354]  Time: 0.110 (0.143)  Loss:  5.0820 (5.4165)  Acc@1:  0.0000 ( 1.9039)  Acc@5:  0.0000 ( 4.7441)\n",
            "Test: [ 850/2354]  Time: 0.171 (0.143)  Loss:  5.2344 (5.4018)  Acc@1:  0.0000 ( 1.7920)  Acc@5:  0.0000 ( 4.8766)\n",
            "Test: [ 900/2354]  Time: 0.111 (0.143)  Loss:  5.6250 (5.3886)  Acc@1:  0.0000 ( 1.6926)  Acc@5:  0.0000 ( 5.4661)\n",
            "Test: [ 950/2354]  Time: 0.079 (0.143)  Loss:  4.8984 (5.3885)  Acc@1:  0.0000 ( 1.6036)  Acc@5: 25.0000 ( 5.5205)\n",
            "Test: [1000/2354]  Time: 0.179 (0.142)  Loss:  5.5781 (5.3873)  Acc@1:  0.0000 ( 1.5235)  Acc@5:  0.0000 ( 5.2448)\n",
            "Test: [1050/2354]  Time: 0.161 (0.142)  Loss:  5.1328 (5.3818)  Acc@1:  0.0000 ( 1.7840)  Acc@5:  0.0000 ( 5.9229)\n",
            "Test: [1100/2354]  Time: 0.152 (0.142)  Loss:  5.2539 (5.3740)  Acc@1:  0.0000 ( 1.7030)  Acc@5:  0.0000 ( 5.6540)\n",
            "Test: [1150/2354]  Time: 0.098 (0.142)  Loss:  4.7969 (5.3706)  Acc@1:  0.0000 ( 1.6290)  Acc@5:  0.0000 ( 5.4083)\n",
            "Test: [1200/2354]  Time: 0.148 (0.142)  Loss:  5.3516 (5.3845)  Acc@1:  0.0000 ( 1.5612)  Acc@5:  0.0000 ( 5.1832)\n",
            "Test: [1250/2354]  Time: 0.084 (0.142)  Loss:  5.7617 (5.3939)  Acc@1:  0.0000 ( 1.4988)  Acc@5:  0.0000 ( 4.9760)\n",
            "Test: [1300/2354]  Time: 0.095 (0.141)  Loss:  5.2266 (5.3953)  Acc@1:  0.0000 ( 1.4412)  Acc@5:  0.0000 ( 4.7848)\n",
            "Test: [1350/2354]  Time: 0.148 (0.141)  Loss:  5.5234 (5.4062)  Acc@1:  0.0000 ( 1.3879)  Acc@5:  0.0000 ( 4.6077)\n",
            "Test: [1400/2354]  Time: 0.283 (0.142)  Loss:  5.0430 (5.4092)  Acc@1:  0.0000 ( 1.3383)  Acc@5:  0.0000 ( 4.4433)\n",
            "Test: [1450/2354]  Time: 0.139 (0.143)  Loss:  5.0938 (5.4019)  Acc@1:  0.0000 ( 1.2922)  Acc@5:  0.0000 ( 4.2901)\n",
            "Test: [1500/2354]  Time: 0.187 (0.143)  Loss:  5.2109 (5.4133)  Acc@1:  0.0000 ( 1.2492)  Acc@5:  0.0000 ( 4.1472)\n",
            "Test: [1550/2354]  Time: 0.092 (0.142)  Loss:  5.3711 (5.4170)  Acc@1:  0.0000 ( 1.2089)  Acc@5:  0.0000 ( 4.0135)\n",
            "Test: [1600/2354]  Time: 0.167 (0.142)  Loss:  4.7266 (5.4121)  Acc@1:  0.0000 ( 1.1711)  Acc@5: 75.0000 ( 4.1380)\n",
            "Test: [1650/2354]  Time: 0.164 (0.142)  Loss:  5.3359 (5.4045)  Acc@1:  0.0000 ( 1.1357)  Acc@5:  0.0000 ( 4.8910)\n",
            "Test: [1700/2354]  Time: 0.180 (0.142)  Loss:  5.2070 (5.3948)  Acc@1:  0.0000 ( 1.1023)  Acc@5:  0.0000 ( 5.8054)\n",
            "Test: [1750/2354]  Time: 0.149 (0.142)  Loss:  5.1797 (5.3952)  Acc@1:  0.0000 ( 1.0708)  Acc@5:  0.0000 ( 5.7110)\n",
            "Test: [1800/2354]  Time: 0.077 (0.142)  Loss:  5.2109 (5.3934)  Acc@1:  0.0000 ( 1.0411)  Acc@5:  0.0000 ( 5.5525)\n",
            "Test: [1850/2354]  Time: 0.102 (0.142)  Loss:  5.1641 (5.3860)  Acc@1:  0.0000 ( 1.6478)  Acc@5:  0.0000 ( 6.0373)\n",
            "Test: [1900/2354]  Time: 0.083 (0.142)  Loss:  5.1719 (5.3860)  Acc@1:  0.0000 ( 1.6044)  Acc@5:  0.0000 ( 5.8785)\n",
            "Test: [1950/2354]  Time: 0.177 (0.142)  Loss:  6.6289 (5.3862)  Acc@1:  0.0000 ( 1.5633)  Acc@5:  0.0000 ( 5.8047)\n",
            "Test: [2000/2354]  Time: 0.169 (0.142)  Loss:  4.7500 (5.3918)  Acc@1:  0.0000 ( 1.5242)  Acc@5: 25.0000 ( 5.7971)\n",
            "Test: [2050/2354]  Time: 0.135 (0.142)  Loss:  6.5156 (5.3891)  Acc@1:  0.0000 ( 1.4871)  Acc@5:  0.0000 ( 5.8020)\n",
            "Test: [2100/2354]  Time: 0.136 (0.142)  Loss:  4.6172 (5.3877)  Acc@1:  0.0000 ( 1.4517)  Acc@5: 25.0000 ( 5.6997)\n",
            "Test: [2150/2354]  Time: 0.136 (0.142)  Loss:  4.6719 (5.3806)  Acc@1:  0.0000 ( 1.4179)  Acc@5:  0.0000 ( 5.6950)\n",
            "Test: [2200/2354]  Time: 0.174 (0.142)  Loss:  6.3438 (5.3695)  Acc@1:  0.0000 ( 1.3857)  Acc@5:  0.0000 ( 6.6106)\n",
            "Test: [2250/2354]  Time: 0.157 (0.142)  Loss:  5.5430 (5.3783)  Acc@1:  0.0000 ( 1.3550)  Acc@5:  0.0000 ( 6.4638)\n",
            "Test: [2300/2354]  Time: 0.163 (0.142)  Loss:  5.2617 (5.3881)  Acc@1:  0.0000 ( 1.3255)  Acc@5:  0.0000 ( 6.3233)\n",
            "Test: [2350/2354]  Time: 0.076 (0.141)  Loss:  6.8125 (5.3950)  Acc@1:  0.0000 ( 1.2973)  Acc@5:  0.0000 ( 6.1889)\n",
            "Test: [2354/2354]  Time: 0.216 (0.141)  Loss:  6.4531 (5.3968)  Acc@1:  0.0000 ( 1.2953)  Acc@5:  0.0000 ( 6.1790)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-0.pth.tar', 1.295254273277418)\n",
            "\n",
            "Train: 1 [   0/2354 (  0%)]  Loss: 5.835 (5.84)  Time: 2.178s,    3.67/s  (2.178s,    3.67/s)  LR: 1.000e-04  Data: 0.792 (0.792)\n",
            "Train: 1 [  50/2354 (  2%)]  Loss: 5.592 (5.48)  Time: 0.471s,   16.98/s  (0.521s,   15.36/s)  LR: 1.000e-04  Data: 0.010 (0.023)\n",
            "Train: 1 [ 100/2354 (  4%)]  Loss: 5.967 (5.49)  Time: 0.475s,   16.83/s  (0.495s,   16.17/s)  LR: 1.000e-04  Data: 0.007 (0.015)\n",
            "Train: 1 [ 150/2354 (  6%)]  Loss: 5.138 (5.49)  Time: 0.467s,   17.12/s  (0.486s,   16.47/s)  LR: 1.000e-04  Data: 0.010 (0.013)\n",
            "Train: 1 [ 200/2354 (  8%)]  Loss: 5.665 (5.49)  Time: 0.469s,   17.05/s  (0.482s,   16.61/s)  LR: 1.000e-04  Data: 0.010 (0.012)\n",
            "Train: 1 [ 250/2354 ( 11%)]  Loss: 5.342 (5.48)  Time: 0.465s,   17.22/s  (0.479s,   16.70/s)  LR: 1.000e-04  Data: 0.008 (0.011)\n",
            "Train: 1 [ 300/2354 ( 13%)]  Loss: 5.985 (5.47)  Time: 0.472s,   16.95/s  (0.481s,   16.64/s)  LR: 1.000e-04  Data: 0.007 (0.010)\n",
            "Train: 1 [ 350/2354 ( 15%)]  Loss: 5.453 (5.48)  Time: 0.467s,   17.14/s  (0.479s,   16.70/s)  LR: 1.000e-04  Data: 0.007 (0.010)\n",
            "Train: 1 [ 400/2354 ( 17%)]  Loss: 5.470 (5.47)  Time: 0.472s,   16.96/s  (0.478s,   16.74/s)  LR: 1.000e-04  Data: 0.007 (0.010)\n",
            "Train: 1 [ 450/2354 ( 19%)]  Loss: 5.940 (5.47)  Time: 0.467s,   17.12/s  (0.477s,   16.78/s)  LR: 1.000e-04  Data: 0.007 (0.010)\n",
            "Train: 1 [ 500/2354 ( 21%)]  Loss: 5.115 (5.47)  Time: 0.472s,   16.95/s  (0.476s,   16.81/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 1 [ 550/2354 ( 23%)]  Loss: 4.919 (5.47)  Time: 0.479s,   16.69/s  (0.475s,   16.83/s)  LR: 1.000e-04  Data: 0.010 (0.009)\n",
            "Train: 1 [ 600/2354 ( 25%)]  Loss: 5.792 (5.47)  Time: 0.465s,   17.22/s  (0.476s,   16.80/s)  LR: 1.000e-04  Data: 0.010 (0.009)\n",
            "Train: 1 [ 650/2354 ( 28%)]  Loss: 5.831 (5.47)  Time: 0.463s,   17.28/s  (0.476s,   16.82/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 1 [ 700/2354 ( 30%)]  Loss: 5.574 (5.46)  Time: 0.473s,   16.92/s  (0.475s,   16.84/s)  LR: 1.000e-04  Data: 0.009 (0.009)\n",
            "Train: 1 [ 750/2354 ( 32%)]  Loss: 4.803 (5.46)  Time: 0.465s,   17.19/s  (0.475s,   16.86/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 1 [ 800/2354 ( 34%)]  Loss: 5.526 (5.45)  Time: 0.466s,   17.16/s  (0.474s,   16.87/s)  LR: 1.000e-04  Data: 0.006 (0.009)\n",
            "Train: 1 [ 850/2354 ( 36%)]  Loss: 5.754 (5.45)  Time: 0.467s,   17.12/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 1 [ 900/2354 ( 38%)]  Loss: 5.277 (5.45)  Time: 0.472s,   16.93/s  (0.475s,   16.85/s)  LR: 1.000e-04  Data: 0.008 (0.009)\n",
            "Train: 1 [ 950/2354 ( 40%)]  Loss: 4.951 (5.45)  Time: 0.465s,   17.22/s  (0.474s,   16.86/s)  LR: 1.000e-04  Data: 0.009 (0.009)\n",
            "Train: 1 [1000/2354 ( 42%)]  Loss: 5.681 (5.44)  Time: 0.468s,   17.11/s  (0.474s,   16.87/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 1 [1050/2354 ( 45%)]  Loss: 5.003 (5.44)  Time: 0.469s,   17.05/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.009 (0.009)\n",
            "Train: 1 [1100/2354 ( 47%)]  Loss: 5.625 (5.43)  Time: 0.469s,   17.05/s  (0.474s,   16.89/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 1 [1150/2354 ( 49%)]  Loss: 5.341 (5.43)  Time: 0.551s,   14.52/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 1 [1200/2354 ( 51%)]  Loss: 5.519 (5.42)  Time: 0.462s,   17.32/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.008 (0.009)\n",
            "Train: 1 [1250/2354 ( 53%)]  Loss: 5.965 (5.42)  Time: 0.471s,   16.98/s  (0.474s,   16.89/s)  LR: 1.000e-04  Data: 0.009 (0.008)\n",
            "Train: 1 [1300/2354 ( 55%)]  Loss: 6.154 (5.41)  Time: 0.467s,   17.12/s  (0.474s,   16.89/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 1 [1350/2354 ( 57%)]  Loss: 5.620 (5.41)  Time: 0.462s,   17.31/s  (0.473s,   16.90/s)  LR: 1.000e-04  Data: 0.006 (0.008)\n",
            "Train: 1 [1400/2354 ( 59%)]  Loss: 5.426 (5.40)  Time: 0.475s,   16.85/s  (0.473s,   16.91/s)  LR: 1.000e-04  Data: 0.009 (0.008)\n",
            "Train: 1 [1450/2354 ( 62%)]  Loss: 5.334 (5.40)  Time: 0.466s,   17.16/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 1 [1500/2354 ( 64%)]  Loss: 5.335 (5.39)  Time: 0.472s,   16.96/s  (0.474s,   16.89/s)  LR: 1.000e-04  Data: 0.009 (0.008)\n",
            "Train: 1 [1550/2354 ( 66%)]  Loss: 5.324 (5.39)  Time: 0.475s,   16.85/s  (0.473s,   16.90/s)  LR: 1.000e-04  Data: 0.018 (0.008)\n",
            "Train: 1 [1600/2354 ( 68%)]  Loss: 5.160 (5.38)  Time: 0.470s,   17.01/s  (0.473s,   16.90/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 1 [1650/2354 ( 70%)]  Loss: 5.367 (5.38)  Time: 0.464s,   17.25/s  (0.473s,   16.91/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 1 [1700/2354 ( 72%)]  Loss: 5.396 (5.37)  Time: 0.472s,   16.95/s  (0.473s,   16.91/s)  LR: 1.000e-04  Data: 0.009 (0.008)\n",
            "Train: 1 [1750/2354 ( 74%)]  Loss: 4.998 (5.37)  Time: 0.468s,   17.09/s  (0.473s,   16.90/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 1 [1800/2354 ( 76%)]  Loss: 4.764 (5.36)  Time: 0.468s,   17.08/s  (0.473s,   16.90/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 1 [1850/2354 ( 79%)]  Loss: 4.816 (5.36)  Time: 0.466s,   17.16/s  (0.473s,   16.91/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 1 [1900/2354 ( 81%)]  Loss: 4.418 (5.35)  Time: 0.464s,   17.25/s  (0.473s,   16.91/s)  LR: 1.000e-04  Data: 0.008 (0.008)\n",
            "Train: 1 [1950/2354 ( 83%)]  Loss: 5.709 (5.34)  Time: 0.467s,   17.15/s  (0.473s,   16.92/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 1 [2000/2354 ( 85%)]  Loss: 5.088 (5.34)  Time: 0.472s,   16.96/s  (0.473s,   16.92/s)  LR: 1.000e-04  Data: 0.008 (0.008)\n",
            "Train: 1 [2050/2354 ( 87%)]  Loss: 5.679 (5.34)  Time: 0.463s,   17.28/s  (0.473s,   16.91/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 1 [2100/2354 ( 89%)]  Loss: 4.966 (5.33)  Time: 0.466s,   17.18/s  (0.473s,   16.92/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 1 [2150/2354 ( 91%)]  Loss: 5.361 (5.33)  Time: 0.466s,   17.18/s  (0.473s,   16.92/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 1 [2200/2354 ( 93%)]  Loss: 5.382 (5.32)  Time: 0.467s,   17.12/s  (0.473s,   16.93/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 1 [2250/2354 ( 96%)]  Loss: 4.822 (5.31)  Time: 0.475s,   16.85/s  (0.472s,   16.93/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 1 [2300/2354 ( 98%)]  Loss: 4.589 (5.30)  Time: 0.466s,   17.17/s  (0.472s,   16.93/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 1 [2350/2354 (100%)]  Loss: 5.345 (5.30)  Time: 0.460s,   17.39/s  (0.473s,   16.92/s)  LR: 1.000e-04  Data: 0.006 (0.008)\n",
            "Train: 1 [2353/2354 (100%)]  Loss: 4.496 (5.30)  Time: 0.457s,   17.51/s  (0.473s,   16.92/s)  LR: 1.000e-04  Data: 0.000 (0.008)\n",
            "Test: [   0/2354]  Time: 0.692 (0.692)  Loss:  6.1094 (6.1094)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test: [  50/2354]  Time: 0.184 (0.156)  Loss:  5.6992 (5.0067)  Acc@1:  0.0000 ( 2.9412)  Acc@5:  0.0000 (14.7059)\n",
            "Test: [ 100/2354]  Time: 0.084 (0.148)  Loss:  6.0859 (5.1795)  Acc@1:  0.0000 ( 1.4851)  Acc@5:  0.0000 ( 7.6733)\n",
            "Test: [ 150/2354]  Time: 0.136 (0.146)  Loss:  3.9883 (4.7089)  Acc@1:  0.0000 ( 1.8212)  Acc@5: 50.0000 (29.1391)\n",
            "Test: [ 200/2354]  Time: 0.157 (0.145)  Loss:  6.5273 (4.7627)  Acc@1:  0.0000 ( 1.3682)  Acc@5:  0.0000 (24.5025)\n",
            "Test: [ 250/2354]  Time: 0.091 (0.145)  Loss:  3.9180 (4.8804)  Acc@1:  0.0000 ( 2.3904)  Acc@5: 75.0000 (22.1116)\n",
            "Test: [ 300/2354]  Time: 0.115 (0.145)  Loss:  4.2227 (4.8730)  Acc@1:  0.0000 ( 2.0764)  Acc@5: 25.0000 (19.9336)\n",
            "Test: [ 350/2354]  Time: 0.092 (0.144)  Loss:  5.0977 (4.7926)  Acc@1:  0.0000 ( 2.1368)  Acc@5:  0.0000 (19.4444)\n",
            "Test: [ 400/2354]  Time: 0.144 (0.143)  Loss:  4.3008 (4.7264)  Acc@1:  0.0000 ( 1.9327)  Acc@5:  0.0000 (18.3292)\n",
            "Test: [ 450/2354]  Time: 0.166 (0.143)  Loss:  4.5391 (4.6742)  Acc@1:  0.0000 ( 4.1574)  Acc@5:  0.0000 (19.8448)\n",
            "Test: [ 500/2354]  Time: 0.088 (0.143)  Loss:  3.7344 (4.6359)  Acc@1: 25.0000 ( 4.0419)  Acc@5: 100.0000 (20.8084)\n",
            "Test: [ 550/2354]  Time: 0.158 (0.143)  Loss:  4.6328 (4.5926)  Acc@1:  0.0000 ( 5.8076)  Acc@5:  0.0000 (22.7768)\n",
            "Test: [ 600/2354]  Time: 0.185 (0.142)  Loss:  3.9121 (4.5987)  Acc@1:  0.0000 ( 5.6156)  Acc@5: 75.0000 (23.4609)\n",
            "Test: [ 650/2354]  Time: 0.127 (0.142)  Loss:  4.7969 (4.6129)  Acc@1:  0.0000 ( 5.1843)  Acc@5:  0.0000 (23.0415)\n",
            "Test: [ 700/2354]  Time: 0.155 (0.142)  Loss:  4.7188 (4.6239)  Acc@1:  0.0000 ( 4.9215)  Acc@5:  0.0000 (22.3966)\n",
            "Test: [ 750/2354]  Time: 0.174 (0.142)  Loss:  5.0117 (4.6179)  Acc@1:  0.0000 ( 5.6258)  Acc@5:  0.0000 (23.0027)\n",
            "Test: [ 800/2354]  Time: 0.088 (0.144)  Loss:  4.3125 (4.6444)  Acc@1:  0.0000 ( 5.2747)  Acc@5:  0.0000 (22.0037)\n",
            "Test: [ 850/2354]  Time: 0.096 (0.144)  Loss:  4.2891 (4.6479)  Acc@1:  0.0000 ( 5.2291)  Acc@5: 25.0000 (22.1798)\n",
            "Test: [ 900/2354]  Time: 0.076 (0.143)  Loss:  3.1660 (4.6097)  Acc@1:  0.0000 ( 6.6315)  Acc@5: 75.0000 (24.5560)\n",
            "Test: [ 950/2354]  Time: 0.159 (0.143)  Loss:  4.0312 (4.5862)  Acc@1:  0.0000 ( 6.3354)  Acc@5: 25.0000 (24.5794)\n",
            "Test: [1000/2354]  Time: 0.117 (0.142)  Loss:  4.5547 (4.5973)  Acc@1:  0.0000 ( 6.0190)  Acc@5:  0.0000 (23.3516)\n",
            "Test: [1050/2354]  Time: 0.178 (0.142)  Loss:  4.2031 (4.6084)  Acc@1:  0.0000 ( 5.7326)  Acc@5: 50.0000 (22.4548)\n",
            "Test: [1100/2354]  Time: 0.090 (0.142)  Loss:  4.7539 (4.6022)  Acc@1:  0.0000 ( 6.0173)  Acc@5:  0.0000 (23.0699)\n",
            "Test: [1150/2354]  Time: 0.134 (0.141)  Loss:  4.1953 (4.5751)  Acc@1: 75.0000 ( 7.1460)  Acc@5: 100.0000 (24.3701)\n",
            "Test: [1200/2354]  Time: 0.183 (0.141)  Loss:  3.9570 (4.5715)  Acc@1: 50.0000 ( 7.1399)  Acc@5: 75.0000 (25.4371)\n",
            "Test: [1250/2354]  Time: 0.088 (0.141)  Loss:  4.3789 (4.5763)  Acc@1:  0.0000 ( 7.2342)  Acc@5: 25.0000 (25.7794)\n",
            "Test: [1300/2354]  Time: 0.187 (0.141)  Loss:  3.7090 (4.5804)  Acc@1:  0.0000 ( 6.9562)  Acc@5:  0.0000 (24.9424)\n",
            "Test: [1350/2354]  Time: 0.116 (0.140)  Loss:  4.6016 (4.5830)  Acc@1:  0.0000 ( 6.6987)  Acc@5:  0.0000 (24.4078)\n",
            "Test: [1400/2354]  Time: 0.211 (0.140)  Loss:  3.1934 (4.5833)  Acc@1:  0.0000 ( 6.4597)  Acc@5: 75.0000 (24.0542)\n",
            "Test: [1450/2354]  Time: 0.112 (0.140)  Loss:  3.6367 (4.5486)  Acc@1:  0.0000 ( 6.9090)  Acc@5: 75.0000 (24.9139)\n",
            "Test: [1500/2354]  Time: 0.176 (0.140)  Loss:  4.6406 (4.5566)  Acc@1:  0.0000 ( 6.6789)  Acc@5:  0.0000 (24.2505)\n",
            "Test: [1550/2354]  Time: 0.096 (0.140)  Loss:  3.3984 (4.5647)  Acc@1: 25.0000 ( 6.6086)  Acc@5: 100.0000 (24.0329)\n",
            "Test: [1600/2354]  Time: 0.154 (0.140)  Loss:  4.7227 (4.5624)  Acc@1:  0.0000 ( 6.6052)  Acc@5:  0.0000 (23.9069)\n",
            "Test: [1650/2354]  Time: 0.143 (0.139)  Loss:  4.8359 (4.5616)  Acc@1:  0.0000 ( 6.4809)  Acc@5:  0.0000 (23.9098)\n",
            "Test: [1700/2354]  Time: 0.185 (0.139)  Loss:  4.1211 (4.5458)  Acc@1: 25.0000 ( 6.4815)  Acc@5: 100.0000 (24.5150)\n",
            "Test: [1750/2354]  Time: 0.110 (0.139)  Loss:  5.2188 (4.5503)  Acc@1:  0.0000 ( 6.3107)  Acc@5:  0.0000 (24.0291)\n",
            "Test: [1800/2354]  Time: 0.135 (0.140)  Loss:  5.0312 (4.5575)  Acc@1:  0.0000 ( 6.2188)  Acc@5:  0.0000 (23.8340)\n",
            "Test: [1850/2354]  Time: 0.164 (0.140)  Loss:  5.2148 (4.5597)  Acc@1:  0.0000 ( 6.3074)  Acc@5:  0.0000 (23.6764)\n",
            "Test: [1900/2354]  Time: 0.128 (0.140)  Loss:  4.8945 (4.5531)  Acc@1:  0.0000 ( 6.6675)  Acc@5:  0.0000 (23.9479)\n",
            "Test: [1950/2354]  Time: 0.176 (0.140)  Loss:  5.7695 (4.5648)  Acc@1:  0.0000 ( 6.4967)  Acc@5:  0.0000 (23.3470)\n",
            "Test: [2000/2354]  Time: 0.076 (0.140)  Loss:  4.1719 (4.5768)  Acc@1:  0.0000 ( 6.3343)  Acc@5: 50.0000 (23.2009)\n",
            "Test: [2050/2354]  Time: 0.161 (0.140)  Loss:  5.7773 (4.5798)  Acc@1:  0.0000 ( 6.2409)  Acc@5:  0.0000 (23.1229)\n",
            "Test: [2100/2354]  Time: 0.153 (0.139)  Loss:  4.5508 (4.5710)  Acc@1:  0.0000 ( 6.1280)  Acc@5:  0.0000 (23.0010)\n",
            "Test: [2150/2354]  Time: 0.141 (0.139)  Loss:  2.8809 (4.5644)  Acc@1: 100.0000 ( 6.4389)  Acc@5: 100.0000 (23.1753)\n",
            "Test: [2200/2354]  Time: 0.154 (0.139)  Loss:  5.8359 (4.5574)  Acc@1:  0.0000 ( 6.3721)  Acc@5:  0.0000 (23.0804)\n",
            "Test: [2250/2354]  Time: 0.121 (0.139)  Loss:  4.8867 (4.5685)  Acc@1:  0.0000 ( 6.2306)  Acc@5:  0.0000 (22.7343)\n",
            "Test: [2300/2354]  Time: 0.088 (0.139)  Loss:  4.0234 (4.5851)  Acc@1:  0.0000 ( 6.1060)  Acc@5: 50.0000 (22.3055)\n",
            "Test: [2350/2354]  Time: 0.075 (0.139)  Loss:  6.3828 (4.5935)  Acc@1:  0.0000 ( 6.0187)  Acc@5:  0.0000 (21.9694)\n",
            "Test: [2354/2354]  Time: 0.064 (0.139)  Loss:  5.2656 (4.5952)  Acc@1:  0.0000 ( 6.0091)  Acc@5:  0.0000 (21.9344)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-1.pth.tar', 6.009130480942775)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-0.pth.tar', 1.295254273277418)\n",
            "\n",
            "Train: 2 [   0/2354 (  0%)]  Loss: 5.605 (5.61)  Time: 1.205s,    6.64/s  (1.205s,    6.64/s)  LR: 1.000e-04  Data: 0.447 (0.447)\n",
            "Train: 2 [  50/2354 (  2%)]  Loss: 4.694 (4.93)  Time: 0.474s,   16.87/s  (0.496s,   16.13/s)  LR: 1.000e-04  Data: 0.009 (0.016)\n",
            "Train: 2 [ 100/2354 (  4%)]  Loss: 4.780 (4.93)  Time: 0.468s,   17.08/s  (0.480s,   16.67/s)  LR: 1.000e-04  Data: 0.007 (0.012)\n",
            "Train: 2 [ 150/2354 (  6%)]  Loss: 4.476 (4.95)  Time: 0.467s,   17.13/s  (0.482s,   16.59/s)  LR: 1.000e-04  Data: 0.006 (0.011)\n",
            "Train: 2 [ 200/2354 (  8%)]  Loss: 5.049 (4.97)  Time: 0.472s,   16.95/s  (0.478s,   16.74/s)  LR: 1.000e-04  Data: 0.006 (0.010)\n",
            "Train: 2 [ 250/2354 ( 11%)]  Loss: 4.725 (4.97)  Time: 0.464s,   17.23/s  (0.475s,   16.82/s)  LR: 1.000e-04  Data: 0.007 (0.010)\n",
            "Train: 2 [ 300/2354 ( 13%)]  Loss: 5.252 (4.98)  Time: 0.463s,   17.30/s  (0.474s,   16.88/s)  LR: 1.000e-04  Data: 0.006 (0.009)\n",
            "Train: 2 [ 350/2354 ( 15%)]  Loss: 5.269 (4.97)  Time: 0.471s,   16.98/s  (0.473s,   16.92/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 2 [ 400/2354 ( 17%)]  Loss: 5.214 (4.97)  Time: 0.462s,   17.31/s  (0.474s,   16.86/s)  LR: 1.000e-04  Data: 0.008 (0.009)\n",
            "Train: 2 [ 450/2354 ( 19%)]  Loss: 5.093 (4.96)  Time: 0.464s,   17.22/s  (0.473s,   16.90/s)  LR: 1.000e-04  Data: 0.009 (0.009)\n",
            "Train: 2 [ 500/2354 ( 21%)]  Loss: 4.616 (4.96)  Time: 0.462s,   17.31/s  (0.473s,   16.92/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 2 [ 550/2354 ( 23%)]  Loss: 4.532 (4.94)  Time: 0.470s,   17.03/s  (0.472s,   16.95/s)  LR: 1.000e-04  Data: 0.011 (0.009)\n",
            "Train: 2 [ 600/2354 ( 25%)]  Loss: 5.056 (4.94)  Time: 0.476s,   16.82/s  (0.472s,   16.96/s)  LR: 1.000e-04  Data: 0.009 (0.009)\n",
            "Train: 2 [ 650/2354 ( 28%)]  Loss: 4.910 (4.95)  Time: 0.467s,   17.14/s  (0.471s,   16.97/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 2 [ 700/2354 ( 30%)]  Loss: 4.925 (4.94)  Time: 0.471s,   16.98/s  (0.472s,   16.93/s)  LR: 1.000e-04  Data: 0.009 (0.008)\n",
            "Train: 2 [ 750/2354 ( 32%)]  Loss: 5.811 (4.94)  Time: 0.463s,   17.28/s  (0.472s,   16.95/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 2 [ 800/2354 ( 34%)]  Loss: 5.512 (4.94)  Time: 0.463s,   17.27/s  (0.472s,   16.96/s)  LR: 1.000e-04  Data: 0.010 (0.008)\n",
            "Train: 2 [ 850/2354 ( 36%)]  Loss: 4.694 (4.94)  Time: 0.464s,   17.26/s  (0.471s,   16.97/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 2 [ 900/2354 ( 38%)]  Loss: 4.923 (4.93)  Time: 0.464s,   17.22/s  (0.471s,   16.98/s)  LR: 1.000e-04  Data: 0.010 (0.008)\n",
            "Train: 2 [ 950/2354 ( 40%)]  Loss: 4.726 (4.93)  Time: 0.466s,   17.16/s  (0.471s,   16.99/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 2 [1000/2354 ( 42%)]  Loss: 5.227 (4.93)  Time: 0.467s,   17.14/s  (0.472s,   16.96/s)  LR: 1.000e-04  Data: 0.010 (0.008)\n",
            "Train: 2 [1050/2354 ( 45%)]  Loss: 5.156 (4.93)  Time: 0.477s,   16.78/s  (0.471s,   16.97/s)  LR: 1.000e-04  Data: 0.010 (0.008)\n",
            "Train: 2 [1100/2354 ( 47%)]  Loss: 4.157 (4.92)  Time: 0.463s,   17.26/s  (0.471s,   16.98/s)  LR: 1.000e-04  Data: 0.008 (0.008)\n",
            "Train: 2 [1150/2354 ( 49%)]  Loss: 4.893 (4.91)  Time: 0.461s,   17.37/s  (0.471s,   16.99/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 2 [1200/2354 ( 51%)]  Loss: 5.017 (4.90)  Time: 0.463s,   17.30/s  (0.471s,   16.99/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 2 [1250/2354 ( 53%)]  Loss: 4.063 (4.90)  Time: 0.638s,   12.54/s  (0.471s,   16.98/s)  LR: 1.000e-04  Data: 0.022 (0.008)\n",
            "Train: 2 [1300/2354 ( 55%)]  Loss: 5.546 (4.91)  Time: 0.472s,   16.93/s  (0.471s,   16.98/s)  LR: 1.000e-04  Data: 0.010 (0.008)\n",
            "Train: 2 [1350/2354 ( 57%)]  Loss: 4.915 (4.90)  Time: 0.464s,   17.24/s  (0.471s,   16.99/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 2 [1400/2354 ( 59%)]  Loss: 4.032 (4.90)  Time: 0.466s,   17.18/s  (0.471s,   16.99/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 2 [1450/2354 ( 62%)]  Loss: 4.274 (4.89)  Time: 0.462s,   17.30/s  (0.471s,   17.00/s)  LR: 1.000e-04  Data: 0.006 (0.008)\n",
            "Train: 2 [1500/2354 ( 64%)]  Loss: 4.698 (4.89)  Time: 0.462s,   17.32/s  (0.470s,   17.00/s)  LR: 1.000e-04  Data: 0.009 (0.008)\n",
            "Train: 2 [1550/2354 ( 66%)]  Loss: 4.364 (4.89)  Time: 0.465s,   17.21/s  (0.471s,   16.98/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 2 [1600/2354 ( 68%)]  Loss: 4.843 (4.88)  Time: 0.460s,   17.37/s  (0.471s,   16.99/s)  LR: 1.000e-04  Data: 0.006 (0.008)\n",
            "Train: 2 [1650/2354 ( 70%)]  Loss: 6.043 (4.88)  Time: 0.476s,   16.81/s  (0.471s,   17.00/s)  LR: 1.000e-04  Data: 0.009 (0.008)\n",
            "Train: 2 [1700/2354 ( 72%)]  Loss: 4.471 (4.88)  Time: 0.465s,   17.22/s  (0.471s,   17.00/s)  LR: 1.000e-04  Data: 0.006 (0.008)\n",
            "Train: 2 [1750/2354 ( 74%)]  Loss: 4.487 (4.87)  Time: 0.465s,   17.19/s  (0.470s,   17.01/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 2 [1800/2354 ( 76%)]  Loss: 4.409 (4.87)  Time: 0.464s,   17.23/s  (0.470s,   17.01/s)  LR: 1.000e-04  Data: 0.010 (0.008)\n",
            "Train: 2 [1850/2354 ( 79%)]  Loss: 4.300 (4.86)  Time: 0.473s,   16.90/s  (0.471s,   16.99/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 2 [1900/2354 ( 81%)]  Loss: 4.892 (4.86)  Time: 0.476s,   16.79/s  (0.471s,   17.00/s)  LR: 1.000e-04  Data: 0.009 (0.008)\n",
            "Train: 2 [1950/2354 ( 83%)]  Loss: 3.886 (4.85)  Time: 0.462s,   17.32/s  (0.470s,   17.00/s)  LR: 1.000e-04  Data: 0.006 (0.008)\n",
            "Train: 2 [2000/2354 ( 85%)]  Loss: 5.023 (4.84)  Time: 0.467s,   17.12/s  (0.470s,   17.01/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 2 [2050/2354 ( 87%)]  Loss: 5.334 (4.84)  Time: 0.465s,   17.19/s  (0.470s,   17.01/s)  LR: 1.000e-04  Data: 0.008 (0.008)\n",
            "Train: 2 [2100/2354 ( 89%)]  Loss: 4.425 (4.83)  Time: 0.463s,   17.27/s  (0.471s,   17.00/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 2 [2150/2354 ( 91%)]  Loss: 5.097 (4.83)  Time: 0.465s,   17.22/s  (0.470s,   17.00/s)  LR: 1.000e-04  Data: 0.008 (0.008)\n",
            "Train: 2 [2200/2354 ( 93%)]  Loss: 4.150 (4.83)  Time: 0.461s,   17.35/s  (0.470s,   17.01/s)  LR: 1.000e-04  Data: 0.007 (0.008)\n",
            "Train: 2 [2250/2354 ( 96%)]  Loss: 5.135 (4.82)  Time: 0.472s,   16.95/s  (0.470s,   17.01/s)  LR: 1.000e-04  Data: 0.009 (0.008)\n",
            "Train: 2 [2300/2354 ( 98%)]  Loss: 4.905 (4.82)  Time: 0.464s,   17.23/s  (0.470s,   17.01/s)  LR: 1.000e-04  Data: 0.010 (0.008)\n",
            "Train: 2 [2350/2354 (100%)]  Loss: 4.780 (4.81)  Time: 0.461s,   17.34/s  (0.470s,   17.02/s)  LR: 1.000e-04  Data: 0.006 (0.008)\n",
            "Train: 2 [2353/2354 (100%)]  Loss: 5.143 (4.81)  Time: 0.456s,   17.53/s  (0.470s,   17.02/s)  LR: 1.000e-04  Data: 0.000 (0.008)\n",
            "Test: [   0/2354]  Time: 0.594 (0.594)  Loss:  4.9609 (4.9609)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test: [  50/2354]  Time: 0.112 (0.154)  Loss:  6.3125 (4.1444)  Acc@1:  0.0000 (10.7843)  Acc@5:  0.0000 (35.7843)\n",
            "Test: [ 100/2354]  Time: 0.189 (0.165)  Loss:  5.8203 (4.2521)  Acc@1:  0.0000 (13.6139)  Acc@5:  0.0000 (33.6634)\n",
            "Test: [ 150/2354]  Time: 0.114 (0.156)  Loss:  3.6758 (4.0001)  Acc@1:  0.0000 (18.7086)  Acc@5: 25.0000 (38.7417)\n",
            "Test: [ 200/2354]  Time: 0.095 (0.151)  Loss:  5.4258 (4.0088)  Acc@1:  0.0000 (14.4279)  Acc@5:  0.0000 (35.6965)\n",
            "Test: [ 250/2354]  Time: 0.173 (0.148)  Loss:  3.1914 (4.0814)  Acc@1:  0.0000 (13.2470)  Acc@5: 100.0000 (34.5618)\n",
            "Test: [ 300/2354]  Time: 0.121 (0.147)  Loss:  3.0488 (4.0506)  Acc@1: 100.0000 (12.5415)  Acc@5: 100.0000 (36.9601)\n",
            "Test: [ 350/2354]  Time: 0.156 (0.146)  Loss:  3.9551 (3.9635)  Acc@1:  0.0000 (15.5983)  Acc@5: 25.0000 (41.0256)\n",
            "Test: [ 400/2354]  Time: 0.145 (0.145)  Loss:  3.6172 (3.9127)  Acc@1:  0.0000 (16.7082)  Acc@5:  0.0000 (41.0848)\n",
            "Test: [ 450/2354]  Time: 0.127 (0.144)  Loss:  4.5312 (3.8917)  Acc@1:  0.0000 (16.1863)  Acc@5:  0.0000 (41.3525)\n",
            "Test: [ 500/2354]  Time: 0.117 (0.143)  Loss:  3.3867 (3.8443)  Acc@1:  0.0000 (18.2635)  Acc@5: 100.0000 (43.2136)\n",
            "Test: [ 550/2354]  Time: 0.173 (0.142)  Loss:  4.3906 (3.8375)  Acc@1:  0.0000 (17.6497)  Acc@5:  0.0000 (42.9673)\n",
            "Test: [ 600/2354]  Time: 0.153 (0.142)  Loss:  3.3613 (3.8086)  Acc@1: 50.0000 (18.6356)  Acc@5: 75.0000 (44.8419)\n",
            "Test: [ 650/2354]  Time: 0.166 (0.142)  Loss:  4.7656 (3.7985)  Acc@1:  0.0000 (18.6252)  Acc@5:  0.0000 (43.8556)\n",
            "Test: [ 700/2354]  Time: 0.119 (0.142)  Loss:  4.0000 (3.8367)  Acc@1:  0.0000 (17.4394)  Acc@5: 25.0000 (42.6534)\n",
            "Test: [ 750/2354]  Time: 0.076 (0.142)  Loss:  4.1875 (3.8211)  Acc@1:  0.0000 (17.3103)  Acc@5: 75.0000 (43.8083)\n",
            "Test: [ 800/2354]  Time: 0.115 (0.141)  Loss:  2.2285 (3.8378)  Acc@1: 75.0000 (17.6966)  Acc@5: 100.0000 (43.5081)\n",
            "Test: [ 850/2354]  Time: 0.148 (0.141)  Loss:  1.7930 (3.8024)  Acc@1: 100.0000 (19.4183)  Acc@5: 100.0000 (45.0059)\n",
            "Test: [ 900/2354]  Time: 0.129 (0.141)  Loss:  2.6914 (3.7712)  Acc@1: 75.0000 (21.0599)  Acc@5: 75.0000 (46.3651)\n",
            "Test: [ 950/2354]  Time: 0.082 (0.141)  Loss:  2.8555 (3.7419)  Acc@1: 25.0000 (21.6877)  Acc@5: 100.0000 (47.5026)\n",
            "Test: [1000/2354]  Time: 0.295 (0.142)  Loss:  4.3633 (3.7390)  Acc@1:  0.0000 (21.1039)  Acc@5:  0.0000 (47.9770)\n",
            "Test: [1050/2354]  Time: 0.131 (0.142)  Loss:  3.2402 (3.7458)  Acc@1: 50.0000 (20.8611)  Acc@5: 75.0000 (47.8116)\n",
            "Test: [1100/2354]  Time: 0.202 (0.142)  Loss:  3.8691 (3.7454)  Acc@1:  0.0000 (20.7312)  Acc@5: 50.0000 (47.8656)\n",
            "Test: [1150/2354]  Time: 0.117 (0.142)  Loss:  3.4805 (3.7282)  Acc@1: 25.0000 (20.9383)  Acc@5: 50.0000 (48.3493)\n",
            "Test: [1200/2354]  Time: 0.160 (0.141)  Loss:  4.0000 (3.7304)  Acc@1:  0.0000 (20.2331)  Acc@5: 50.0000 (48.2306)\n",
            "Test: [1250/2354]  Time: 0.106 (0.141)  Loss:  2.1191 (3.7401)  Acc@1: 100.0000 (20.3237)  Acc@5: 100.0000 (47.9616)\n",
            "Test: [1300/2354]  Time: 0.133 (0.141)  Loss:  3.7520 (3.7307)  Acc@1:  0.0000 (20.1576)  Acc@5: 25.0000 (47.7902)\n",
            "Test: [1350/2354]  Time: 0.141 (0.141)  Loss:  3.5508 (3.7454)  Acc@1: 25.0000 (19.5041)  Acc@5: 100.0000 (47.3168)\n",
            "Test: [1400/2354]  Time: 0.167 (0.140)  Loss:  2.6602 (3.7449)  Acc@1: 25.0000 (19.3255)  Acc@5: 100.0000 (47.3769)\n",
            "Test: [1450/2354]  Time: 0.113 (0.140)  Loss:  3.2266 (3.7033)  Acc@1:  0.0000 (19.6072)  Acc@5: 75.0000 (48.5183)\n",
            "Test: [1500/2354]  Time: 0.130 (0.140)  Loss:  3.8730 (3.7157)  Acc@1:  0.0000 (19.0540)  Acc@5: 50.0000 (48.1179)\n",
            "Test: [1550/2354]  Time: 0.127 (0.140)  Loss:  2.5469 (3.7265)  Acc@1: 25.0000 (18.5525)  Acc@5: 100.0000 (47.4210)\n",
            "Test: [1600/2354]  Time: 0.181 (0.140)  Loss:  2.7930 (3.7209)  Acc@1: 100.0000 (18.4572)  Acc@5: 100.0000 (47.5484)\n",
            "Test: [1650/2354]  Time: 0.193 (0.139)  Loss:  3.5898 (3.7059)  Acc@1: 25.0000 (18.8219)  Acc@5: 75.0000 (48.2738)\n",
            "Test: [1700/2354]  Time: 0.162 (0.139)  Loss:  2.2461 (3.6979)  Acc@1: 75.0000 (18.7243)  Acc@5: 100.0000 (48.5156)\n",
            "Test: [1750/2354]  Time: 0.085 (0.139)  Loss:  4.2227 (3.7080)  Acc@1:  0.0000 (18.3467)  Acc@5: 25.0000 (47.7299)\n",
            "Test: [1800/2354]  Time: 0.152 (0.139)  Loss:  4.1094 (3.7198)  Acc@1:  0.0000 (17.8512)  Acc@5: 50.0000 (47.2099)\n",
            "Test: [1850/2354]  Time: 0.163 (0.139)  Loss:  4.3008 (3.7043)  Acc@1:  0.0000 (18.4765)  Acc@5:  0.0000 (47.3663)\n",
            "Test: [1900/2354]  Time: 0.156 (0.139)  Loss:  3.8281 (3.7094)  Acc@1: 25.0000 (18.1089)  Acc@5: 75.0000 (47.2120)\n",
            "Test: [1950/2354]  Time: 0.276 (0.139)  Loss:  4.6406 (3.7116)  Acc@1:  0.0000 (18.1189)  Acc@5:  0.0000 (47.6038)\n",
            "Test: [2000/2354]  Time: 0.112 (0.140)  Loss:  3.3945 (3.7241)  Acc@1: 25.0000 (17.8286)  Acc@5: 75.0000 (47.0140)\n",
            "Test: [2050/2354]  Time: 0.135 (0.139)  Loss:  5.5547 (3.7265)  Acc@1:  0.0000 (17.7353)  Acc@5:  0.0000 (47.2331)\n",
            "Test: [2100/2354]  Time: 0.174 (0.139)  Loss:  3.3145 (3.7191)  Acc@1: 25.0000 (17.5512)  Acc@5: 75.0000 (47.4774)\n",
            "Test: [2150/2354]  Time: 0.079 (0.139)  Loss:  3.2402 (3.7244)  Acc@1:  0.0000 (17.2129)  Acc@5: 75.0000 (47.3152)\n",
            "Test: [2200/2354]  Time: 0.097 (0.139)  Loss:  4.0898 (3.7213)  Acc@1:  0.0000 (17.1059)  Acc@5:  0.0000 (47.2967)\n",
            "Test: [2250/2354]  Time: 0.110 (0.139)  Loss:  4.2461 (3.7266)  Acc@1:  0.0000 (17.0147)  Acc@5:  0.0000 (47.0902)\n",
            "Test: [2300/2354]  Time: 0.099 (0.139)  Loss:  4.1484 (3.7402)  Acc@1:  0.0000 (16.8622)  Acc@5:  0.0000 (46.7840)\n",
            "Test: [2350/2354]  Time: 0.075 (0.139)  Loss:  5.8555 (3.7503)  Acc@1:  0.0000 (16.5036)  Acc@5:  0.0000 (46.2994)\n",
            "Test: [2354/2354]  Time: 0.064 (0.138)  Loss:  4.4414 (3.7520)  Acc@1:  0.0000 (16.4773)  Acc@5:  0.0000 (46.2257)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-2.pth.tar', 16.477333050217645)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-1.pth.tar', 6.009130480942775)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-0.pth.tar', 1.295254273277418)\n",
            "\n",
            "Train: 3 [   0/2354 (  0%)]  Loss: 5.415 (5.42)  Time: 1.103s,    7.25/s  (1.103s,    7.25/s)  LR: 9.966e-05  Data: 0.489 (0.489)\n",
            "Train: 3 [  50/2354 (  2%)]  Loss: 3.668 (4.53)  Time: 0.469s,   17.07/s  (0.494s,   16.19/s)  LR: 9.966e-05  Data: 0.009 (0.017)\n",
            "Train: 3 [ 100/2354 (  4%)]  Loss: 4.952 (4.51)  Time: 0.464s,   17.24/s  (0.480s,   16.66/s)  LR: 9.966e-05  Data: 0.006 (0.012)\n",
            "Train: 3 [ 150/2354 (  6%)]  Loss: 5.318 (4.56)  Time: 0.469s,   17.06/s  (0.476s,   16.82/s)  LR: 9.966e-05  Data: 0.009 (0.011)\n",
            "Train: 3 [ 200/2354 (  8%)]  Loss: 4.719 (4.56)  Time: 0.476s,   16.81/s  (0.478s,   16.75/s)  LR: 9.966e-05  Data: 0.007 (0.010)\n",
            "Train: 3 [ 250/2354 ( 11%)]  Loss: 5.043 (4.54)  Time: 0.476s,   16.80/s  (0.476s,   16.82/s)  LR: 9.966e-05  Data: 0.007 (0.009)\n",
            "Train: 3 [ 300/2354 ( 13%)]  Loss: 4.268 (4.53)  Time: 0.470s,   17.02/s  (0.474s,   16.88/s)  LR: 9.966e-05  Data: 0.009 (0.009)\n",
            "Train: 3 [ 350/2354 ( 15%)]  Loss: 5.225 (4.55)  Time: 0.465s,   17.22/s  (0.473s,   16.91/s)  LR: 9.966e-05  Data: 0.009 (0.009)\n",
            "Train: 3 [ 400/2354 ( 17%)]  Loss: 4.074 (4.55)  Time: 0.470s,   17.01/s  (0.472s,   16.94/s)  LR: 9.966e-05  Data: 0.010 (0.009)\n",
            "Train: 3 [ 450/2354 ( 19%)]  Loss: 4.018 (4.55)  Time: 0.470s,   17.03/s  (0.474s,   16.87/s)  LR: 9.966e-05  Data: 0.007 (0.009)\n",
            "Train: 3 [ 500/2354 ( 21%)]  Loss: 4.695 (4.55)  Time: 0.466s,   17.18/s  (0.474s,   16.89/s)  LR: 9.966e-05  Data: 0.009 (0.009)\n",
            "Train: 3 [ 550/2354 ( 23%)]  Loss: 3.939 (4.54)  Time: 0.464s,   17.25/s  (0.473s,   16.91/s)  LR: 9.966e-05  Data: 0.007 (0.009)\n",
            "Train: 3 [ 600/2354 ( 25%)]  Loss: 3.916 (4.54)  Time: 0.468s,   17.09/s  (0.473s,   16.93/s)  LR: 9.966e-05  Data: 0.007 (0.009)\n",
            "Train: 3 [ 650/2354 ( 28%)]  Loss: 4.032 (4.54)  Time: 0.470s,   17.01/s  (0.472s,   16.94/s)  LR: 9.966e-05  Data: 0.009 (0.009)\n",
            "Train: 3 [ 700/2354 ( 30%)]  Loss: 3.568 (4.53)  Time: 0.460s,   17.40/s  (0.472s,   16.95/s)  LR: 9.966e-05  Data: 0.006 (0.009)\n",
            "Train: 3 [ 750/2354 ( 32%)]  Loss: 4.645 (4.53)  Time: 0.467s,   17.13/s  (0.473s,   16.92/s)  LR: 9.966e-05  Data: 0.006 (0.008)\n",
            "Train: 3 [ 800/2354 ( 34%)]  Loss: 4.523 (4.52)  Time: 0.466s,   17.17/s  (0.473s,   16.93/s)  LR: 9.966e-05  Data: 0.007 (0.008)\n",
            "Train: 3 [ 850/2354 ( 36%)]  Loss: 4.413 (4.52)  Time: 0.467s,   17.11/s  (0.472s,   16.94/s)  LR: 9.966e-05  Data: 0.007 (0.008)\n",
            "Train: 3 [ 900/2354 ( 38%)]  Loss: 4.336 (4.52)  Time: 0.466s,   17.18/s  (0.472s,   16.95/s)  LR: 9.966e-05  Data: 0.008 (0.008)\n",
            "Train: 3 [ 950/2354 ( 40%)]  Loss: 4.522 (4.52)  Time: 0.474s,   16.89/s  (0.472s,   16.96/s)  LR: 9.966e-05  Data: 0.007 (0.008)\n",
            "Train: 3 [1000/2354 ( 42%)]  Loss: 4.023 (4.52)  Time: 0.462s,   17.32/s  (0.472s,   16.93/s)  LR: 9.966e-05  Data: 0.007 (0.008)\n",
            "Train: 3 [1050/2354 ( 45%)]  Loss: 6.124 (4.52)  Time: 0.459s,   17.42/s  (0.472s,   16.94/s)  LR: 9.966e-05  Data: 0.006 (0.008)\n",
            "Train: 3 [1100/2354 ( 47%)]  Loss: 4.719 (4.51)  Time: 0.469s,   17.05/s  (0.472s,   16.95/s)  LR: 9.966e-05  Data: 0.008 (0.008)\n",
            "Train: 3 [1150/2354 ( 49%)]  Loss: 4.275 (4.50)  Time: 0.464s,   17.23/s  (0.472s,   16.96/s)  LR: 9.966e-05  Data: 0.008 (0.008)\n",
            "Train: 3 [1200/2354 ( 51%)]  Loss: 4.310 (4.50)  Time: 0.467s,   17.13/s  (0.472s,   16.96/s)  LR: 9.966e-05  Data: 0.009 (0.008)\n",
            "Train: 3 [1250/2354 ( 53%)]  Loss: 3.943 (4.48)  Time: 0.465s,   17.19/s  (0.471s,   16.97/s)  LR: 9.966e-05  Data: 0.007 (0.008)\n",
            "Train: 3 [1300/2354 ( 55%)]  Loss: 5.735 (4.48)  Time: 0.464s,   17.24/s  (0.472s,   16.94/s)  LR: 9.966e-05  Data: 0.006 (0.008)\n",
            "Train: 3 [1350/2354 ( 57%)]  Loss: 3.331 (4.48)  Time: 0.465s,   17.19/s  (0.472s,   16.95/s)  LR: 9.966e-05  Data: 0.010 (0.008)\n",
            "Train: 3 [1400/2354 ( 59%)]  Loss: 4.647 (4.47)  Time: 0.462s,   17.31/s  (0.472s,   16.96/s)  LR: 9.966e-05  Data: 0.007 (0.008)\n",
            "Train: 3 [1450/2354 ( 62%)]  Loss: 4.991 (4.47)  Time: 0.466s,   17.16/s  (0.472s,   16.96/s)  LR: 9.966e-05  Data: 0.007 (0.008)\n",
            "Train: 3 [1500/2354 ( 64%)]  Loss: 5.028 (4.47)  Time: 0.470s,   17.02/s  (0.471s,   16.97/s)  LR: 9.966e-05  Data: 0.006 (0.008)\n",
            "Train: 3 [1550/2354 ( 66%)]  Loss: 4.411 (4.47)  Time: 0.465s,   17.22/s  (0.472s,   16.95/s)  LR: 9.966e-05  Data: 0.007 (0.008)\n",
            "Train: 3 [1600/2354 ( 68%)]  Loss: 5.479 (4.46)  Time: 0.474s,   16.88/s  (0.472s,   16.96/s)  LR: 9.966e-05  Data: 0.007 (0.008)\n",
            "Train: 3 [1650/2354 ( 70%)]  Loss: 4.309 (4.46)  Time: 0.468s,   17.10/s  (0.472s,   16.96/s)  LR: 9.966e-05  Data: 0.010 (0.008)\n",
            "Train: 3 [1700/2354 ( 72%)]  Loss: 4.718 (4.46)  Time: 0.464s,   17.23/s  (0.472s,   16.97/s)  LR: 9.966e-05  Data: 0.010 (0.008)\n",
            "Train: 3 [1750/2354 ( 74%)]  Loss: 4.813 (4.45)  Time: 0.464s,   17.25/s  (0.471s,   16.97/s)  LR: 9.966e-05  Data: 0.010 (0.008)\n",
            "Train: 3 [1800/2354 ( 76%)]  Loss: 4.018 (4.45)  Time: 0.467s,   17.13/s  (0.471s,   16.98/s)  LR: 9.966e-05  Data: 0.007 (0.008)\n",
            "Train: 3 [1850/2354 ( 79%)]  Loss: 4.299 (4.44)  Time: 0.464s,   17.25/s  (0.472s,   16.96/s)  LR: 9.966e-05  Data: 0.006 (0.008)\n",
            "Train: 3 [1900/2354 ( 81%)]  Loss: 3.446 (4.44)  Time: 0.467s,   17.12/s  (0.472s,   16.97/s)  LR: 9.966e-05  Data: 0.010 (0.008)\n",
            "Train: 3 [1950/2354 ( 83%)]  Loss: 3.912 (4.44)  Time: 0.481s,   16.65/s  (0.471s,   16.97/s)  LR: 9.966e-05  Data: 0.007 (0.008)\n",
            "Train: 3 [2000/2354 ( 85%)]  Loss: 4.311 (4.44)  Time: 0.467s,   17.12/s  (0.471s,   16.98/s)  LR: 9.966e-05  Data: 0.009 (0.008)\n",
            "Train: 3 [2050/2354 ( 87%)]  Loss: 4.697 (4.43)  Time: 0.474s,   16.87/s  (0.471s,   16.98/s)  LR: 9.966e-05  Data: 0.010 (0.008)\n",
            "Train: 3 [2100/2354 ( 89%)]  Loss: 3.068 (4.43)  Time: 0.459s,   17.44/s  (0.472s,   16.96/s)  LR: 9.966e-05  Data: 0.007 (0.008)\n",
            "Train: 3 [2150/2354 ( 91%)]  Loss: 4.624 (4.43)  Time: 0.465s,   17.22/s  (0.471s,   16.97/s)  LR: 9.966e-05  Data: 0.006 (0.008)\n",
            "Train: 3 [2200/2354 ( 93%)]  Loss: 4.064 (4.43)  Time: 0.478s,   16.75/s  (0.471s,   16.97/s)  LR: 9.966e-05  Data: 0.009 (0.008)\n",
            "Train: 3 [2250/2354 ( 96%)]  Loss: 5.402 (4.42)  Time: 0.473s,   16.91/s  (0.471s,   16.97/s)  LR: 9.966e-05  Data: 0.009 (0.008)\n",
            "Train: 3 [2300/2354 ( 98%)]  Loss: 4.760 (4.42)  Time: 0.464s,   17.25/s  (0.471s,   16.98/s)  LR: 9.966e-05  Data: 0.007 (0.008)\n",
            "Train: 3 [2350/2354 (100%)]  Loss: 4.622 (4.42)  Time: 0.461s,   17.36/s  (0.471s,   16.98/s)  LR: 9.966e-05  Data: 0.005 (0.008)\n",
            "Train: 3 [2353/2354 (100%)]  Loss: 4.299 (4.41)  Time: 0.450s,   17.76/s  (0.471s,   16.98/s)  LR: 9.966e-05  Data: 0.000 (0.008)\n",
            "Test: [   0/2354]  Time: 0.608 (0.608)  Loss:  5.2656 (5.2656)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test: [  50/2354]  Time: 0.221 (0.165)  Loss:  5.6016 (3.8663)  Acc@1:  0.0000 (12.2549)  Acc@5:  0.0000 (41.6667)\n",
            "Test: [ 100/2354]  Time: 0.152 (0.167)  Loss:  5.9570 (4.1112)  Acc@1:  0.0000 ( 7.1782)  Acc@5:  0.0000 (31.4356)\n",
            "Test: [ 150/2354]  Time: 0.109 (0.158)  Loss:  2.1914 (3.6585)  Acc@1: 75.0000 (16.5563)  Acc@5: 100.0000 (45.1987)\n",
            "Test: [ 200/2354]  Time: 0.158 (0.151)  Loss:  5.1016 (3.5899)  Acc@1:  0.0000 (17.9104)  Acc@5:  0.0000 (47.5124)\n",
            "Test: [ 250/2354]  Time: 0.158 (0.148)  Loss:  2.7559 (3.7416)  Acc@1:  0.0000 (16.5339)  Acc@5: 75.0000 (43.1275)\n",
            "Test: [ 300/2354]  Time: 0.128 (0.146)  Loss:  2.2383 (3.7027)  Acc@1:  0.0000 (13.9535)  Acc@5: 100.0000 (43.8538)\n",
            "Test: [ 350/2354]  Time: 0.176 (0.144)  Loss:  3.9277 (3.4882)  Acc@1:  0.0000 (19.1595)  Acc@5: 25.0000 (50.7835)\n",
            "Test: [ 400/2354]  Time: 0.104 (0.143)  Loss:  4.3672 (3.4598)  Acc@1:  0.0000 (17.5187)  Acc@5:  0.0000 (52.3691)\n",
            "Test: [ 450/2354]  Time: 0.138 (0.142)  Loss:  3.2305 (3.4202)  Acc@1: 25.0000 (19.0133)  Acc@5: 50.0000 (52.8271)\n",
            "Test: [ 500/2354]  Time: 0.100 (0.142)  Loss:  2.6758 (3.4085)  Acc@1: 50.0000 (19.5110)  Acc@5: 100.0000 (53.3932)\n",
            "Test: [ 550/2354]  Time: 0.180 (0.142)  Loss:  4.0273 (3.3643)  Acc@1:  0.0000 (20.0091)  Acc@5: 50.0000 (53.9020)\n",
            "Test: [ 600/2354]  Time: 0.110 (0.141)  Loss:  2.7344 (3.3473)  Acc@1: 50.0000 (19.8419)  Acc@5: 75.0000 (54.5757)\n",
            "Test: [ 650/2354]  Time: 0.111 (0.141)  Loss:  3.5762 (3.3233)  Acc@1:  0.0000 (20.5069)  Acc@5: 25.0000 (55.1459)\n",
            "Test: [ 700/2354]  Time: 0.120 (0.140)  Loss:  3.0078 (3.3519)  Acc@1: 25.0000 (19.9001)  Acc@5: 75.0000 (54.2439)\n",
            "Test: [ 750/2354]  Time: 0.149 (0.140)  Loss:  4.2812 (3.3489)  Acc@1: 25.0000 (20.1731)  Acc@5: 50.0000 (54.2943)\n",
            "Test: [ 800/2354]  Time: 0.095 (0.140)  Loss:  2.3320 (3.3819)  Acc@1: 50.0000 (19.9126)  Acc@5: 75.0000 (53.4644)\n",
            "Test: [ 850/2354]  Time: 0.131 (0.139)  Loss:  1.6016 (3.3748)  Acc@1: 100.0000 (20.6522)  Acc@5: 100.0000 (53.7015)\n",
            "Test: [ 900/2354]  Time: 0.143 (0.139)  Loss:  2.1309 (3.3322)  Acc@1: 50.0000 (21.8368)  Acc@5: 75.0000 (54.4118)\n",
            "Test: [ 950/2354]  Time: 0.082 (0.139)  Loss:  2.3867 (3.2829)  Acc@1: 50.0000 (22.9758)  Acc@5: 100.0000 (55.9411)\n",
            "Test: [1000/2354]  Time: 0.226 (0.141)  Loss:  1.8311 (3.2796)  Acc@1: 75.0000 (22.2527)  Acc@5: 100.0000 (55.7692)\n",
            "Test: [1050/2354]  Time: 0.100 (0.141)  Loss:  2.7656 (3.2727)  Acc@1:  0.0000 (22.3834)  Acc@5: 75.0000 (55.8040)\n",
            "Test: [1100/2354]  Time: 0.140 (0.141)  Loss:  2.7188 (3.2631)  Acc@1: 25.0000 (22.5250)  Acc@5: 75.0000 (56.4260)\n",
            "Test: [1150/2354]  Time: 0.141 (0.141)  Loss:  2.9492 (3.2398)  Acc@1: 50.0000 (23.3927)  Acc@5: 100.0000 (56.9722)\n",
            "Test: [1200/2354]  Time: 0.118 (0.140)  Loss:  3.0547 (3.2312)  Acc@1: 50.0000 (23.6678)  Acc@5: 75.0000 (57.0774)\n",
            "Test: [1250/2354]  Time: 0.141 (0.140)  Loss:  2.0195 (3.2301)  Acc@1: 50.0000 (24.0008)  Acc@5: 100.0000 (57.2942)\n",
            "Test: [1300/2354]  Time: 0.154 (0.140)  Loss:  3.2949 (3.2165)  Acc@1:  0.0000 (23.8278)  Acc@5: 25.0000 (57.6864)\n",
            "Test: [1350/2354]  Time: 0.110 (0.140)  Loss:  2.0918 (3.2115)  Acc@1: 100.0000 (24.0933)  Acc@5: 100.0000 (57.5500)\n",
            "Test: [1400/2354]  Time: 0.115 (0.139)  Loss:  2.0098 (3.2059)  Acc@1: 25.0000 (24.5717)  Acc@5: 100.0000 (57.9229)\n",
            "Test: [1450/2354]  Time: 0.115 (0.139)  Loss:  2.9121 (3.1814)  Acc@1:  0.0000 (24.9655)  Acc@5: 75.0000 (58.3735)\n",
            "Test: [1500/2354]  Time: 0.149 (0.139)  Loss:  4.1758 (3.2031)  Acc@1:  0.0000 (24.2838)  Acc@5:  0.0000 (57.5949)\n",
            "Test: [1550/2354]  Time: 0.182 (0.139)  Loss:  1.2363 (3.2204)  Acc@1: 100.0000 (23.9523)  Acc@5: 100.0000 (56.5442)\n",
            "Test: [1600/2354]  Time: 0.077 (0.139)  Loss:  2.0957 (3.2084)  Acc@1: 75.0000 (24.1724)  Acc@5: 100.0000 (56.9956)\n",
            "Test: [1650/2354]  Time: 0.177 (0.139)  Loss:  2.8418 (3.1888)  Acc@1: 50.0000 (24.9546)  Acc@5: 75.0000 (57.3289)\n",
            "Test: [1700/2354]  Time: 0.130 (0.139)  Loss:  1.8799 (3.1725)  Acc@1:  0.0000 (24.8089)  Acc@5: 100.0000 (57.9512)\n",
            "Test: [1750/2354]  Time: 0.086 (0.139)  Loss:  3.0410 (3.1725)  Acc@1:  0.0000 (24.3861)  Acc@5: 50.0000 (57.6385)\n",
            "Test: [1800/2354]  Time: 0.097 (0.139)  Loss:  3.1758 (3.1816)  Acc@1: 25.0000 (24.1810)  Acc@5: 75.0000 (57.2599)\n",
            "Test: [1850/2354]  Time: 0.129 (0.139)  Loss:  2.5215 (3.1664)  Acc@1: 50.0000 (24.6759)  Acc@5: 75.0000 (57.5635)\n",
            "Test: [1900/2354]  Time: 0.150 (0.139)  Loss:  2.9668 (3.1800)  Acc@1: 50.0000 (24.4214)  Acc@5: 75.0000 (57.1410)\n",
            "Test: [1950/2354]  Time: 0.096 (0.140)  Loss:  4.6094 (3.1853)  Acc@1:  0.0000 (24.4874)  Acc@5:  0.0000 (57.2142)\n",
            "Test: [2000/2354]  Time: 0.114 (0.140)  Loss:  3.5586 (3.1960)  Acc@1: 25.0000 (24.3753)  Acc@5: 50.0000 (56.8841)\n",
            "Test: [2050/2354]  Time: 0.163 (0.139)  Loss:  5.2852 (3.1996)  Acc@1:  0.0000 (24.3905)  Acc@5:  0.0000 (56.8625)\n",
            "Test: [2100/2354]  Time: 0.150 (0.139)  Loss:  2.8301 (3.1915)  Acc@1:  0.0000 (24.5954)  Acc@5: 75.0000 (57.2465)\n",
            "Test: [2150/2354]  Time: 0.157 (0.139)  Loss:  2.8867 (3.2035)  Acc@1: 25.0000 (24.1283)  Acc@5: 50.0000 (56.6248)\n",
            "Test: [2200/2354]  Time: 0.110 (0.139)  Loss:  4.1016 (3.2037)  Acc@1:  0.0000 (23.9096)  Acc@5:  0.0000 (56.4403)\n",
            "Test: [2250/2354]  Time: 0.139 (0.139)  Loss:  3.2266 (3.2103)  Acc@1:  0.0000 (23.8339)  Acc@5: 75.0000 (56.3083)\n",
            "Test: [2300/2354]  Time: 0.097 (0.139)  Loss:  2.9766 (3.2252)  Acc@1: 25.0000 (23.7505)  Acc@5: 50.0000 (55.9539)\n",
            "Test: [2350/2354]  Time: 0.075 (0.139)  Loss:  5.7930 (3.2460)  Acc@1:  0.0000 (23.3305)  Acc@5:  0.0000 (55.2212)\n",
            "Test: [2354/2354]  Time: 0.063 (0.139)  Loss:  5.1484 (3.2492)  Acc@1:  0.0000 (23.2933)  Acc@5:  0.0000 (55.1332)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-3.pth.tar', 23.29334324238242)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-2.pth.tar', 16.477333050217645)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-1.pth.tar', 6.009130480942775)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-0.pth.tar', 1.295254273277418)\n",
            "\n",
            "Train: 4 [   0/2354 (  0%)]  Loss: 4.701 (4.70)  Time: 1.291s,    6.20/s  (1.291s,    6.20/s)  LR: 9.939e-05  Data: 0.706 (0.706)\n",
            "Train: 4 [  50/2354 (  2%)]  Loss: 3.135 (4.12)  Time: 0.463s,   17.29/s  (0.499s,   16.04/s)  LR: 9.939e-05  Data: 0.006 (0.022)\n",
            "Train: 4 [ 100/2354 (  4%)]  Loss: 4.524 (4.07)  Time: 0.467s,   17.11/s  (0.484s,   16.55/s)  LR: 9.939e-05  Data: 0.009 (0.015)\n",
            "Train: 4 [ 150/2354 (  6%)]  Loss: 3.938 (4.11)  Time: 0.470s,   17.01/s  (0.484s,   16.52/s)  LR: 9.939e-05  Data: 0.007 (0.012)\n",
            "Train: 4 [ 200/2354 (  8%)]  Loss: 5.070 (4.09)  Time: 0.467s,   17.12/s  (0.480s,   16.66/s)  LR: 9.939e-05  Data: 0.010 (0.011)\n",
            "Train: 4 [ 250/2354 ( 11%)]  Loss: 3.257 (4.11)  Time: 0.459s,   17.43/s  (0.478s,   16.75/s)  LR: 9.939e-05  Data: 0.007 (0.011)\n",
            "Train: 4 [ 300/2354 ( 13%)]  Loss: 3.443 (4.16)  Time: 0.471s,   17.00/s  (0.476s,   16.81/s)  LR: 9.939e-05  Data: 0.007 (0.010)\n",
            "Train: 4 [ 350/2354 ( 15%)]  Loss: 3.981 (4.15)  Time: 0.465s,   17.20/s  (0.475s,   16.86/s)  LR: 9.939e-05  Data: 0.008 (0.010)\n",
            "Train: 4 [ 400/2354 ( 17%)]  Loss: 4.491 (4.13)  Time: 0.470s,   17.02/s  (0.474s,   16.89/s)  LR: 9.939e-05  Data: 0.007 (0.010)\n",
            "Train: 4 [ 450/2354 ( 19%)]  Loss: 5.015 (4.13)  Time: 0.463s,   17.27/s  (0.475s,   16.85/s)  LR: 9.939e-05  Data: 0.007 (0.009)\n",
            "Train: 4 [ 500/2354 ( 21%)]  Loss: 3.223 (4.12)  Time: 0.480s,   16.67/s  (0.474s,   16.88/s)  LR: 9.939e-05  Data: 0.010 (0.009)\n",
            "Train: 4 [ 550/2354 ( 23%)]  Loss: 3.565 (4.11)  Time: 0.470s,   17.03/s  (0.473s,   16.91/s)  LR: 9.939e-05  Data: 0.010 (0.009)\n",
            "Train: 4 [ 600/2354 ( 25%)]  Loss: 3.888 (4.11)  Time: 0.459s,   17.44/s  (0.472s,   16.93/s)  LR: 9.939e-05  Data: 0.007 (0.009)\n",
            "Train: 4 [ 650/2354 ( 28%)]  Loss: 4.723 (4.12)  Time: 0.466s,   17.18/s  (0.472s,   16.95/s)  LR: 9.939e-05  Data: 0.006 (0.009)\n",
            "Train: 4 [ 700/2354 ( 30%)]  Loss: 3.479 (4.13)  Time: 0.470s,   17.01/s  (0.473s,   16.92/s)  LR: 9.939e-05  Data: 0.006 (0.009)\n",
            "Train: 4 [ 750/2354 ( 32%)]  Loss: 4.610 (4.12)  Time: 0.463s,   17.27/s  (0.472s,   16.94/s)  LR: 9.939e-05  Data: 0.010 (0.009)\n",
            "Train: 4 [ 800/2354 ( 34%)]  Loss: 3.357 (4.12)  Time: 0.466s,   17.18/s  (0.472s,   16.95/s)  LR: 9.939e-05  Data: 0.007 (0.009)\n",
            "Train: 4 [ 850/2354 ( 36%)]  Loss: 3.374 (4.13)  Time: 0.460s,   17.40/s  (0.472s,   16.97/s)  LR: 9.939e-05  Data: 0.006 (0.009)\n",
            "Train: 4 [ 900/2354 ( 38%)]  Loss: 3.708 (4.12)  Time: 0.475s,   16.83/s  (0.471s,   16.98/s)  LR: 9.939e-05  Data: 0.010 (0.009)\n",
            "Train: 4 [ 950/2354 ( 40%)]  Loss: 3.872 (4.11)  Time: 0.461s,   17.35/s  (0.471s,   16.99/s)  LR: 9.939e-05  Data: 0.009 (0.009)\n",
            "Train: 4 [1000/2354 ( 42%)]  Loss: 4.908 (4.11)  Time: 0.466s,   17.18/s  (0.472s,   16.96/s)  LR: 9.939e-05  Data: 0.007 (0.009)\n",
            "Train: 4 [1050/2354 ( 45%)]  Loss: 4.485 (4.11)  Time: 0.468s,   17.09/s  (0.471s,   16.97/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [1100/2354 ( 47%)]  Loss: 3.568 (4.10)  Time: 0.469s,   17.07/s  (0.471s,   16.98/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [1150/2354 ( 49%)]  Loss: 4.462 (4.11)  Time: 0.470s,   17.01/s  (0.471s,   16.99/s)  LR: 9.939e-05  Data: 0.008 (0.008)\n",
            "Train: 4 [1200/2354 ( 51%)]  Loss: 3.976 (4.11)  Time: 0.477s,   16.76/s  (0.471s,   16.99/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [1250/2354 ( 53%)]  Loss: 5.049 (4.10)  Time: 0.464s,   17.24/s  (0.471s,   16.97/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [1300/2354 ( 55%)]  Loss: 3.491 (4.10)  Time: 0.462s,   17.33/s  (0.471s,   16.98/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [1350/2354 ( 57%)]  Loss: 4.249 (4.09)  Time: 0.473s,   16.92/s  (0.471s,   16.98/s)  LR: 9.939e-05  Data: 0.008 (0.008)\n",
            "Train: 4 [1400/2354 ( 59%)]  Loss: 4.327 (4.09)  Time: 0.470s,   17.01/s  (0.471s,   16.99/s)  LR: 9.939e-05  Data: 0.006 (0.008)\n",
            "Train: 4 [1450/2354 ( 62%)]  Loss: 3.668 (4.10)  Time: 0.467s,   17.11/s  (0.471s,   16.99/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [1500/2354 ( 64%)]  Loss: 3.964 (4.10)  Time: 0.547s,   14.64/s  (0.471s,   16.98/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [1550/2354 ( 66%)]  Loss: 4.144 (4.10)  Time: 0.480s,   16.65/s  (0.471s,   16.98/s)  LR: 9.939e-05  Data: 0.008 (0.008)\n",
            "Train: 4 [1600/2354 ( 68%)]  Loss: 3.845 (4.10)  Time: 0.462s,   17.31/s  (0.471s,   16.99/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [1650/2354 ( 70%)]  Loss: 4.395 (4.10)  Time: 0.458s,   17.45/s  (0.471s,   16.99/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [1700/2354 ( 72%)]  Loss: 3.907 (4.10)  Time: 0.477s,   16.78/s  (0.471s,   16.99/s)  LR: 9.939e-05  Data: 0.014 (0.008)\n",
            "Train: 4 [1750/2354 ( 74%)]  Loss: 4.497 (4.09)  Time: 0.466s,   17.17/s  (0.471s,   17.00/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [1800/2354 ( 76%)]  Loss: 4.201 (4.09)  Time: 0.482s,   16.58/s  (0.471s,   16.98/s)  LR: 9.939e-05  Data: 0.010 (0.008)\n",
            "Train: 4 [1850/2354 ( 79%)]  Loss: 4.161 (4.09)  Time: 0.466s,   17.17/s  (0.471s,   16.98/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [1900/2354 ( 81%)]  Loss: 4.904 (4.09)  Time: 0.461s,   17.35/s  (0.471s,   16.99/s)  LR: 9.939e-05  Data: 0.008 (0.008)\n",
            "Train: 4 [1950/2354 ( 83%)]  Loss: 3.337 (4.08)  Time: 0.464s,   17.25/s  (0.471s,   16.99/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [2000/2354 ( 85%)]  Loss: 3.835 (4.08)  Time: 0.467s,   17.12/s  (0.471s,   17.00/s)  LR: 9.939e-05  Data: 0.009 (0.008)\n",
            "Train: 4 [2050/2354 ( 87%)]  Loss: 3.748 (4.08)  Time: 0.464s,   17.24/s  (0.471s,   16.98/s)  LR: 9.939e-05  Data: 0.006 (0.008)\n",
            "Train: 4 [2100/2354 ( 89%)]  Loss: 4.173 (4.08)  Time: 0.465s,   17.19/s  (0.471s,   16.98/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [2150/2354 ( 91%)]  Loss: 4.212 (4.08)  Time: 0.464s,   17.23/s  (0.471s,   16.99/s)  LR: 9.939e-05  Data: 0.009 (0.008)\n",
            "Train: 4 [2200/2354 ( 93%)]  Loss: 3.112 (4.07)  Time: 0.466s,   17.17/s  (0.471s,   16.99/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [2250/2354 ( 96%)]  Loss: 4.802 (4.07)  Time: 0.481s,   16.64/s  (0.471s,   16.99/s)  LR: 9.939e-05  Data: 0.009 (0.008)\n",
            "Train: 4 [2300/2354 ( 98%)]  Loss: 4.002 (4.07)  Time: 0.474s,   16.86/s  (0.471s,   17.00/s)  LR: 9.939e-05  Data: 0.007 (0.008)\n",
            "Train: 4 [2350/2354 (100%)]  Loss: 3.352 (4.06)  Time: 0.466s,   17.18/s  (0.471s,   16.98/s)  LR: 9.939e-05  Data: 0.006 (0.008)\n",
            "Train: 4 [2353/2354 (100%)]  Loss: 2.870 (4.06)  Time: 0.461s,   17.37/s  (0.471s,   16.98/s)  LR: 9.939e-05  Data: 0.000 (0.008)\n",
            "Test: [   0/2354]  Time: 0.604 (0.604)  Loss:  4.9141 (4.9141)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test: [  50/2354]  Time: 0.157 (0.151)  Loss:  5.4297 (2.7595)  Acc@1:  0.0000 (44.6078)  Acc@5:  0.0000 (63.7255)\n",
            "Test: [ 100/2354]  Time: 0.079 (0.145)  Loss:  4.9102 (3.1064)  Acc@1:  0.0000 (30.1980)  Acc@5:  0.0000 (56.1881)\n",
            "Test: [ 150/2354]  Time: 0.078 (0.143)  Loss:  1.0850 (2.8882)  Acc@1: 100.0000 (31.9536)  Acc@5: 100.0000 (65.2318)\n",
            "Test: [ 200/2354]  Time: 0.086 (0.142)  Loss:  4.3164 (2.7696)  Acc@1:  0.0000 (35.9453)  Acc@5:  0.0000 (69.9005)\n",
            "Test: [ 250/2354]  Time: 0.170 (0.140)  Loss:  1.8105 (2.9113)  Acc@1: 75.0000 (33.6653)  Acc@5: 100.0000 (64.6414)\n",
            "Test: [ 300/2354]  Time: 0.100 (0.139)  Loss:  2.0723 (2.8805)  Acc@1: 75.0000 (34.8837)  Acc@5: 100.0000 (66.0299)\n",
            "Test: [ 350/2354]  Time: 0.134 (0.138)  Loss:  3.1973 (2.7975)  Acc@1:  0.0000 (36.9658)  Acc@5: 50.0000 (69.4444)\n",
            "Test: [ 400/2354]  Time: 0.163 (0.139)  Loss:  3.3203 (2.8005)  Acc@1:  0.0000 (35.7855)  Acc@5: 75.0000 (69.7007)\n",
            "Test: [ 450/2354]  Time: 0.166 (0.138)  Loss:  1.6221 (2.7672)  Acc@1: 75.0000 (37.0288)  Acc@5: 75.0000 (69.6231)\n",
            "Test: [ 500/2354]  Time: 0.161 (0.138)  Loss:  2.2812 (2.7841)  Acc@1: 25.0000 (35.5788)  Acc@5: 100.0000 (69.1118)\n",
            "Test: [ 550/2354]  Time: 0.149 (0.138)  Loss:  2.8184 (2.7530)  Acc@1: 50.0000 (36.4338)  Acc@5: 75.0000 (69.7822)\n",
            "Test: [ 600/2354]  Time: 0.194 (0.138)  Loss:  3.0254 (2.7591)  Acc@1: 25.0000 (36.2313)  Acc@5: 50.0000 (70.3411)\n",
            "Test: [ 650/2354]  Time: 0.094 (0.138)  Loss:  1.4785 (2.7515)  Acc@1: 75.0000 (36.7512)  Acc@5: 100.0000 (70.4685)\n",
            "Test: [ 700/2354]  Time: 0.176 (0.138)  Loss:  3.4180 (2.7439)  Acc@1:  0.0000 (37.1255)  Acc@5: 75.0000 (70.6847)\n",
            "Test: [ 750/2354]  Time: 0.150 (0.141)  Loss:  2.8184 (2.7328)  Acc@1: 75.0000 (37.3835)  Acc@5: 75.0000 (71.0386)\n",
            "Test: [ 800/2354]  Time: 0.085 (0.141)  Loss:  0.9419 (2.7336)  Acc@1: 100.0000 (37.8277)  Acc@5: 100.0000 (71.1923)\n",
            "Test: [ 850/2354]  Time: 0.102 (0.141)  Loss:  0.6831 (2.7041)  Acc@1: 100.0000 (38.8367)  Acc@5: 100.0000 (71.7098)\n",
            "Test: [ 900/2354]  Time: 0.159 (0.141)  Loss:  2.6543 (2.6900)  Acc@1:  0.0000 (39.1787)  Acc@5: 75.0000 (72.2253)\n",
            "Test: [ 950/2354]  Time: 0.175 (0.140)  Loss:  2.1035 (2.6689)  Acc@1: 25.0000 (39.2744)  Acc@5: 75.0000 (72.8707)\n",
            "Test: [1000/2354]  Time: 0.085 (0.140)  Loss:  2.5156 (2.6659)  Acc@1: 25.0000 (38.8112)  Acc@5: 100.0000 (72.8771)\n",
            "Test: [1050/2354]  Time: 0.142 (0.140)  Loss:  2.2637 (2.6889)  Acc@1: 50.0000 (37.9163)  Acc@5: 75.0000 (72.2883)\n",
            "Test: [1100/2354]  Time: 0.079 (0.140)  Loss:  2.1133 (2.6732)  Acc@1: 75.0000 (39.0100)  Acc@5: 100.0000 (72.9564)\n",
            "Test: [1150/2354]  Time: 0.130 (0.140)  Loss:  3.3047 (2.6588)  Acc@1: 50.0000 (39.9652)  Acc@5: 100.0000 (73.1321)\n",
            "Test: [1200/2354]  Time: 0.175 (0.140)  Loss:  2.3613 (2.6468)  Acc@1: 75.0000 (40.6953)  Acc@5: 75.0000 (73.5221)\n",
            "Test: [1250/2354]  Time: 0.097 (0.140)  Loss:  0.9595 (2.6397)  Acc@1: 100.0000 (41.3469)  Acc@5: 100.0000 (73.7010)\n",
            "Test: [1300/2354]  Time: 0.167 (0.139)  Loss:  3.2031 (2.6470)  Acc@1:  0.0000 (40.6610)  Acc@5: 50.0000 (73.2706)\n",
            "Test: [1350/2354]  Time: 0.141 (0.139)  Loss:  3.2500 (2.6654)  Acc@1: 25.0000 (40.0814)  Acc@5: 50.0000 (72.7979)\n",
            "Test: [1400/2354]  Time: 0.076 (0.139)  Loss:  1.5488 (2.6750)  Acc@1: 75.0000 (39.7395)  Acc@5: 100.0000 (72.8587)\n",
            "Test: [1450/2354]  Time: 0.109 (0.139)  Loss:  3.3535 (2.6450)  Acc@1:  0.0000 (39.8691)  Acc@5: 50.0000 (73.5355)\n",
            "Test: [1500/2354]  Time: 0.108 (0.139)  Loss:  2.8535 (2.6451)  Acc@1:  0.0000 (39.8401)  Acc@5: 75.0000 (73.3344)\n",
            "Test: [1550/2354]  Time: 0.153 (0.139)  Loss:  1.1543 (2.6469)  Acc@1: 100.0000 (39.7002)  Acc@5: 100.0000 (73.1464)\n",
            "Test: [1600/2354]  Time: 0.154 (0.139)  Loss:  1.6289 (2.6418)  Acc@1: 100.0000 (39.6939)  Acc@5: 100.0000 (73.2511)\n",
            "Test: [1650/2354]  Time: 0.087 (0.140)  Loss:  2.1445 (2.6410)  Acc@1: 50.0000 (39.6729)  Acc@5: 100.0000 (72.8498)\n",
            "Test: [1700/2354]  Time: 0.149 (0.140)  Loss:  0.8545 (2.6303)  Acc@1: 100.0000 (39.6091)  Acc@5: 100.0000 (73.2657)\n",
            "Test: [1750/2354]  Time: 0.180 (0.140)  Loss:  2.8887 (2.6263)  Acc@1:  0.0000 (39.4917)  Acc@5: 75.0000 (73.3152)\n",
            "Test: [1800/2354]  Time: 0.076 (0.140)  Loss:  2.7266 (2.6197)  Acc@1: 50.0000 (39.8806)  Acc@5: 75.0000 (73.4870)\n",
            "Test: [1850/2354]  Time: 0.182 (0.140)  Loss:  2.3359 (2.6123)  Acc@1: 50.0000 (40.2620)  Acc@5: 100.0000 (73.5008)\n",
            "Test: [1900/2354]  Time: 0.101 (0.139)  Loss:  3.0430 (2.6212)  Acc@1: 50.0000 (40.0973)  Acc@5: 75.0000 (73.4613)\n",
            "Test: [1950/2354]  Time: 0.101 (0.139)  Loss:  3.8203 (2.6244)  Acc@1:  0.0000 (40.1205)  Acc@5: 50.0000 (73.5136)\n",
            "Test: [2000/2354]  Time: 0.089 (0.139)  Loss:  2.1133 (2.6324)  Acc@1: 75.0000 (39.9175)  Acc@5: 100.0000 (73.2259)\n",
            "Test: [2050/2354]  Time: 0.216 (0.139)  Loss:  4.3359 (2.6410)  Acc@1:  0.0000 (39.9074)  Acc@5:  0.0000 (72.9157)\n",
            "Test: [2100/2354]  Time: 0.187 (0.139)  Loss:  3.1836 (2.6319)  Acc@1:  0.0000 (40.1951)  Acc@5: 50.0000 (73.1913)\n",
            "Test: [2150/2354]  Time: 0.204 (0.139)  Loss:  2.5312 (2.6388)  Acc@1: 75.0000 (40.0163)  Acc@5: 75.0000 (73.0939)\n",
            "Test: [2200/2354]  Time: 0.118 (0.139)  Loss:  3.5703 (2.6504)  Acc@1:  0.0000 (39.4139)  Acc@5:  0.0000 (72.7056)\n",
            "Test: [2250/2354]  Time: 0.155 (0.139)  Loss:  1.9453 (2.6571)  Acc@1: 75.0000 (39.3714)  Acc@5: 75.0000 (72.4789)\n",
            "Test: [2300/2354]  Time: 0.130 (0.139)  Loss:  1.4512 (2.6602)  Acc@1: 75.0000 (39.3959)  Acc@5: 100.0000 (72.3272)\n",
            "Test: [2350/2354]  Time: 0.076 (0.139)  Loss:  4.9023 (2.6746)  Acc@1:  0.0000 (39.1429)  Acc@5:  0.0000 (71.9268)\n",
            "Test: [2354/2354]  Time: 0.064 (0.139)  Loss:  3.2188 (2.6762)  Acc@1:  0.0000 (39.0806)  Acc@5: 33.3333 (71.8654)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-4.pth.tar', 39.08058180273915)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-3.pth.tar', 23.29334324238242)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-2.pth.tar', 16.477333050217645)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-1.pth.tar', 6.009130480942775)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-0.pth.tar', 1.295254273277418)\n",
            "\n",
            "Train: 5 [   0/2354 (  0%)]  Loss: 3.949 (3.95)  Time: 1.302s,    6.15/s  (1.302s,    6.15/s)  LR: 9.905e-05  Data: 0.700 (0.700)\n",
            "Train: 5 [  50/2354 (  2%)]  Loss: 3.280 (3.81)  Time: 0.617s,   12.97/s  (0.518s,   15.43/s)  LR: 9.905e-05  Data: 0.018 (0.021)\n",
            "Train: 5 [ 100/2354 (  4%)]  Loss: 3.580 (3.80)  Time: 0.467s,   17.14/s  (0.494s,   16.18/s)  LR: 9.905e-05  Data: 0.009 (0.015)\n",
            "Train: 5 [ 150/2354 (  6%)]  Loss: 4.955 (3.85)  Time: 0.464s,   17.23/s  (0.485s,   16.48/s)  LR: 9.905e-05  Data: 0.006 (0.013)\n",
            "Train: 5 [ 200/2354 (  8%)]  Loss: 3.869 (3.82)  Time: 0.462s,   17.32/s  (0.480s,   16.65/s)  LR: 9.905e-05  Data: 0.009 (0.011)\n",
            "Train: 5 [ 250/2354 ( 11%)]  Loss: 4.227 (3.80)  Time: 0.466s,   17.15/s  (0.478s,   16.75/s)  LR: 9.905e-05  Data: 0.007 (0.011)\n",
            "Train: 5 [ 300/2354 ( 13%)]  Loss: 4.231 (3.81)  Time: 0.459s,   17.43/s  (0.476s,   16.82/s)  LR: 9.905e-05  Data: 0.006 (0.010)\n",
            "Train: 5 [ 350/2354 ( 15%)]  Loss: 3.333 (3.82)  Time: 0.469s,   17.05/s  (0.477s,   16.75/s)  LR: 9.905e-05  Data: 0.006 (0.010)\n",
            "Train: 5 [ 400/2354 ( 17%)]  Loss: 4.073 (3.82)  Time: 0.482s,   16.61/s  (0.476s,   16.80/s)  LR: 9.905e-05  Data: 0.007 (0.010)\n",
            "Train: 5 [ 450/2354 ( 19%)]  Loss: 4.718 (3.83)  Time: 0.464s,   17.24/s  (0.475s,   16.84/s)  LR: 9.905e-05  Data: 0.010 (0.010)\n",
            "Train: 5 [ 500/2354 ( 21%)]  Loss: 4.644 (3.83)  Time: 0.463s,   17.27/s  (0.474s,   16.88/s)  LR: 9.905e-05  Data: 0.007 (0.009)\n",
            "Train: 5 [ 550/2354 ( 23%)]  Loss: 5.438 (3.84)  Time: 0.463s,   17.27/s  (0.473s,   16.90/s)  LR: 9.905e-05  Data: 0.008 (0.009)\n",
            "Train: 5 [ 600/2354 ( 25%)]  Loss: 4.310 (3.85)  Time: 0.479s,   16.71/s  (0.474s,   16.86/s)  LR: 9.905e-05  Data: 0.010 (0.009)\n",
            "Train: 5 [ 650/2354 ( 28%)]  Loss: 3.569 (3.84)  Time: 0.465s,   17.21/s  (0.474s,   16.88/s)  LR: 9.905e-05  Data: 0.007 (0.009)\n",
            "Train: 5 [ 700/2354 ( 30%)]  Loss: 2.718 (3.83)  Time: 0.463s,   17.29/s  (0.473s,   16.90/s)  LR: 9.905e-05  Data: 0.007 (0.009)\n",
            "Train: 5 [ 750/2354 ( 32%)]  Loss: 4.481 (3.84)  Time: 0.469s,   17.04/s  (0.473s,   16.92/s)  LR: 9.905e-05  Data: 0.009 (0.009)\n",
            "Train: 5 [ 800/2354 ( 34%)]  Loss: 4.595 (3.84)  Time: 0.471s,   17.00/s  (0.472s,   16.93/s)  LR: 9.905e-05  Data: 0.007 (0.009)\n",
            "Train: 5 [ 850/2354 ( 36%)]  Loss: 4.139 (3.84)  Time: 0.460s,   17.37/s  (0.473s,   16.91/s)  LR: 9.905e-05  Data: 0.007 (0.009)\n",
            "Train: 5 [ 900/2354 ( 38%)]  Loss: 3.962 (3.84)  Time: 0.464s,   17.25/s  (0.473s,   16.92/s)  LR: 9.905e-05  Data: 0.008 (0.009)\n",
            "Train: 5 [ 950/2354 ( 40%)]  Loss: 2.847 (3.83)  Time: 0.466s,   17.17/s  (0.473s,   16.93/s)  LR: 9.905e-05  Data: 0.006 (0.009)\n",
            "Train: 5 [1000/2354 ( 42%)]  Loss: 3.751 (3.83)  Time: 0.459s,   17.42/s  (0.472s,   16.94/s)  LR: 9.905e-05  Data: 0.006 (0.009)\n",
            "Train: 5 [1050/2354 ( 45%)]  Loss: 4.905 (3.82)  Time: 0.466s,   17.17/s  (0.472s,   16.95/s)  LR: 9.905e-05  Data: 0.007 (0.009)\n",
            "Train: 5 [1100/2354 ( 47%)]  Loss: 3.021 (3.82)  Time: 0.472s,   16.97/s  (0.472s,   16.96/s)  LR: 9.905e-05  Data: 0.010 (0.009)\n",
            "Train: 5 [1150/2354 ( 49%)]  Loss: 3.610 (3.82)  Time: 0.462s,   17.33/s  (0.472s,   16.94/s)  LR: 9.905e-05  Data: 0.007 (0.008)\n",
            "Train: 5 [1200/2354 ( 51%)]  Loss: 2.892 (3.82)  Time: 0.465s,   17.21/s  (0.472s,   16.95/s)  LR: 9.905e-05  Data: 0.007 (0.008)\n",
            "Train: 5 [1250/2354 ( 53%)]  Loss: 4.250 (3.82)  Time: 0.475s,   16.85/s  (0.472s,   16.96/s)  LR: 9.905e-05  Data: 0.006 (0.008)\n",
            "Train: 5 [1300/2354 ( 55%)]  Loss: 4.399 (3.82)  Time: 0.468s,   17.10/s  (0.472s,   16.97/s)  LR: 9.905e-05  Data: 0.007 (0.008)\n",
            "Train: 5 [1350/2354 ( 57%)]  Loss: 3.478 (3.82)  Time: 0.461s,   17.34/s  (0.471s,   16.97/s)  LR: 9.905e-05  Data: 0.006 (0.008)\n",
            "Train: 5 [1400/2354 ( 59%)]  Loss: 3.037 (3.82)  Time: 0.465s,   17.20/s  (0.472s,   16.96/s)  LR: 9.905e-05  Data: 0.007 (0.008)\n",
            "Train: 5 [1450/2354 ( 62%)]  Loss: 3.808 (3.82)  Time: 0.465s,   17.20/s  (0.472s,   16.96/s)  LR: 9.905e-05  Data: 0.007 (0.008)\n",
            "Train: 5 [1500/2354 ( 64%)]  Loss: 3.186 (3.81)  Time: 0.461s,   17.34/s  (0.471s,   16.97/s)  LR: 9.905e-05  Data: 0.006 (0.008)\n",
            "Train: 5 [1550/2354 ( 66%)]  Loss: 3.344 (3.80)  Time: 0.460s,   17.38/s  (0.471s,   16.98/s)  LR: 9.905e-05  Data: 0.006 (0.008)\n",
            "Train: 5 [1600/2354 ( 68%)]  Loss: 3.319 (3.80)  Time: 0.466s,   17.16/s  (0.471s,   16.98/s)  LR: 9.905e-05  Data: 0.007 (0.008)\n",
            "Train: 5 [1650/2354 ( 70%)]  Loss: 4.007 (3.79)  Time: 0.482s,   16.60/s  (0.472s,   16.96/s)  LR: 9.905e-05  Data: 0.007 (0.008)\n",
            "Train: 5 [1700/2354 ( 72%)]  Loss: 2.723 (3.79)  Time: 0.466s,   17.17/s  (0.471s,   16.97/s)  LR: 9.905e-05  Data: 0.007 (0.008)\n",
            "Train: 5 [1750/2354 ( 74%)]  Loss: 2.358 (3.78)  Time: 0.462s,   17.32/s  (0.471s,   16.98/s)  LR: 9.905e-05  Data: 0.006 (0.008)\n",
            "Train: 5 [1800/2354 ( 76%)]  Loss: 6.098 (3.78)  Time: 0.466s,   17.18/s  (0.471s,   16.98/s)  LR: 9.905e-05  Data: 0.006 (0.008)\n",
            "Train: 5 [1850/2354 ( 79%)]  Loss: 3.017 (3.77)  Time: 0.461s,   17.36/s  (0.471s,   16.99/s)  LR: 9.905e-05  Data: 0.007 (0.008)\n",
            "Train: 5 [1900/2354 ( 81%)]  Loss: 2.217 (3.77)  Time: 0.514s,   15.57/s  (0.471s,   16.98/s)  LR: 9.905e-05  Data: 0.007 (0.008)\n",
            "Train: 5 [1950/2354 ( 83%)]  Loss: 4.966 (3.77)  Time: 0.461s,   17.34/s  (0.471s,   16.98/s)  LR: 9.905e-05  Data: 0.007 (0.008)\n",
            "Train: 5 [2000/2354 ( 85%)]  Loss: 3.948 (3.77)  Time: 0.475s,   16.83/s  (0.471s,   16.98/s)  LR: 9.905e-05  Data: 0.006 (0.008)\n",
            "Train: 5 [2050/2354 ( 87%)]  Loss: 4.432 (3.77)  Time: 0.461s,   17.37/s  (0.471s,   16.99/s)  LR: 9.905e-05  Data: 0.007 (0.008)\n",
            "Train: 5 [2100/2354 ( 89%)]  Loss: 3.537 (3.77)  Time: 0.465s,   17.19/s  (0.471s,   16.99/s)  LR: 9.905e-05  Data: 0.007 (0.008)\n",
            "Train: 5 [2150/2354 ( 91%)]  Loss: 3.926 (3.76)  Time: 0.475s,   16.85/s  (0.471s,   16.99/s)  LR: 9.905e-05  Data: 0.006 (0.008)\n",
            "Train: 5 [2200/2354 ( 93%)]  Loss: 3.549 (3.76)  Time: 0.465s,   17.22/s  (0.471s,   16.99/s)  LR: 9.905e-05  Data: 0.009 (0.008)\n",
            "Train: 5 [2250/2354 ( 96%)]  Loss: 4.426 (3.76)  Time: 0.459s,   17.42/s  (0.471s,   16.99/s)  LR: 9.905e-05  Data: 0.006 (0.008)\n",
            "Train: 5 [2300/2354 ( 98%)]  Loss: 2.954 (3.76)  Time: 0.461s,   17.34/s  (0.471s,   16.99/s)  LR: 9.905e-05  Data: 0.006 (0.008)\n",
            "Train: 5 [2350/2354 (100%)]  Loss: 2.692 (3.76)  Time: 0.460s,   17.41/s  (0.471s,   17.00/s)  LR: 9.905e-05  Data: 0.005 (0.008)\n",
            "Train: 5 [2353/2354 (100%)]  Loss: 2.718 (3.76)  Time: 0.455s,   17.60/s  (0.471s,   17.00/s)  LR: 9.905e-05  Data: 0.000 (0.008)\n",
            "Test: [   0/2354]  Time: 0.651 (0.651)  Loss:  3.7324 (3.7324)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test: [  50/2354]  Time: 0.123 (0.149)  Loss:  5.0195 (2.8594)  Acc@1:  0.0000 (40.6863)  Acc@5:  0.0000 (66.1765)\n",
            "Test: [ 100/2354]  Time: 0.145 (0.141)  Loss:  4.3672 (3.1504)  Acc@1:  0.0000 (27.4752)  Acc@5:  0.0000 (59.4059)\n",
            "Test: [ 150/2354]  Time: 0.076 (0.138)  Loss:  2.1191 (2.7617)  Acc@1: 50.0000 (34.4371)  Acc@5: 75.0000 (69.5364)\n",
            "Test: [ 200/2354]  Time: 0.176 (0.137)  Loss:  4.3438 (2.7335)  Acc@1:  0.0000 (33.8308)  Acc@5:  0.0000 (70.6468)\n",
            "Test: [ 250/2354]  Time: 0.278 (0.137)  Loss:  2.2617 (2.8340)  Acc@1: 50.0000 (31.8725)  Acc@5: 75.0000 (66.9323)\n",
            "Test: [ 300/2354]  Time: 0.122 (0.142)  Loss:  2.9551 (2.7628)  Acc@1: 25.0000 (35.2990)  Acc@5: 75.0000 (69.2691)\n",
            "Test: [ 350/2354]  Time: 0.089 (0.140)  Loss:  3.5234 (2.7155)  Acc@1:  0.0000 (35.6125)  Acc@5: 50.0000 (70.5128)\n",
            "Test: [ 400/2354]  Time: 0.152 (0.139)  Loss:  2.5156 (2.6816)  Acc@1:  0.0000 (37.0324)  Acc@5: 75.0000 (71.9451)\n",
            "Test: [ 450/2354]  Time: 0.161 (0.138)  Loss:  1.0508 (2.6226)  Acc@1: 100.0000 (38.8027)  Acc@5: 100.0000 (72.4501)\n",
            "Test: [ 500/2354]  Time: 0.086 (0.137)  Loss:  1.1201 (2.5561)  Acc@1: 75.0000 (40.6687)  Acc@5: 100.0000 (74.2515)\n",
            "Test: [ 550/2354]  Time: 0.168 (0.137)  Loss:  2.5625 (2.5271)  Acc@1: 75.0000 (40.4719)  Acc@5: 75.0000 (74.8185)\n",
            "Test: [ 600/2354]  Time: 0.140 (0.137)  Loss:  1.4883 (2.4937)  Acc@1: 100.0000 (41.6389)  Acc@5: 100.0000 (75.8319)\n",
            "Test: [ 650/2354]  Time: 0.087 (0.136)  Loss:  1.8174 (2.4681)  Acc@1: 50.0000 (42.6267)  Acc@5: 100.0000 (76.1905)\n",
            "Test: [ 700/2354]  Time: 0.151 (0.136)  Loss:  2.5977 (2.4414)  Acc@1: 25.0000 (43.5449)  Acc@5: 100.0000 (76.7475)\n",
            "Test: [ 750/2354]  Time: 0.166 (0.136)  Loss:  2.5000 (2.4099)  Acc@1: 50.0000 (44.8735)  Acc@5: 75.0000 (77.5632)\n",
            "Test: [ 800/2354]  Time: 0.126 (0.136)  Loss:  0.3477 (2.4344)  Acc@1: 100.0000 (43.8826)  Acc@5: 100.0000 (76.7166)\n",
            "Test: [ 850/2354]  Time: 0.159 (0.136)  Loss:  1.0732 (2.4005)  Acc@1: 100.0000 (44.8002)  Acc@5: 100.0000 (77.3208)\n",
            "Test: [ 900/2354]  Time: 0.124 (0.136)  Loss:  2.9492 (2.3844)  Acc@1: 50.0000 (45.5050)  Acc@5: 75.0000 (77.6915)\n",
            "Test: [ 950/2354]  Time: 0.189 (0.135)  Loss:  1.6494 (2.3668)  Acc@1: 50.0000 (45.8991)  Acc@5: 100.0000 (78.1809)\n",
            "Test: [1000/2354]  Time: 0.075 (0.135)  Loss:  2.3770 (2.3596)  Acc@1: 25.0000 (45.8541)  Acc@5: 100.0000 (77.8971)\n",
            "Test: [1050/2354]  Time: 0.085 (0.135)  Loss:  2.3516 (2.3562)  Acc@1: 25.0000 (45.6470)  Acc@5: 100.0000 (77.9258)\n",
            "Test: [1100/2354]  Time: 0.102 (0.135)  Loss:  1.9697 (2.3707)  Acc@1: 75.0000 (45.1408)  Acc@5: 100.0000 (77.7929)\n",
            "Test: [1150/2354]  Time: 0.076 (0.135)  Loss:  2.4688 (2.3492)  Acc@1: 50.0000 (45.9818)  Acc@5: 100.0000 (78.0843)\n",
            "Test: [1200/2354]  Time: 0.107 (0.136)  Loss:  1.9893 (2.3340)  Acc@1: 75.0000 (46.4821)  Acc@5: 75.0000 (78.1848)\n",
            "Test: [1250/2354]  Time: 0.179 (0.136)  Loss:  1.5879 (2.3224)  Acc@1: 25.0000 (46.8225)  Acc@5: 100.0000 (78.3573)\n",
            "Test: [1300/2354]  Time: 0.174 (0.136)  Loss:  1.9805 (2.3064)  Acc@1: 50.0000 (46.9062)  Acc@5: 100.0000 (78.7471)\n",
            "Test: [1350/2354]  Time: 0.088 (0.136)  Loss:  1.6748 (2.3077)  Acc@1: 50.0000 (46.8542)  Acc@5: 100.0000 (78.6640)\n",
            "Test: [1400/2354]  Time: 0.184 (0.136)  Loss:  1.3408 (2.3032)  Acc@1: 100.0000 (46.7523)  Acc@5: 100.0000 (78.9079)\n",
            "Test: [1450/2354]  Time: 0.137 (0.135)  Loss:  2.3613 (2.2895)  Acc@1: 50.0000 (47.1054)  Acc@5: 75.0000 (79.2557)\n",
            "Test: [1500/2354]  Time: 0.087 (0.135)  Loss:  2.6055 (2.3040)  Acc@1:  0.0000 (46.6023)  Acc@5: 75.0000 (79.0306)\n",
            "Test: [1550/2354]  Time: 0.091 (0.135)  Loss:  2.0195 (2.3272)  Acc@1: 50.0000 (45.8736)  Acc@5: 100.0000 (78.5139)\n",
            "Test: [1600/2354]  Time: 0.077 (0.135)  Loss:  2.2422 (2.3446)  Acc@1: 75.0000 (45.1749)  Acc@5: 100.0000 (78.0762)\n",
            "Test: [1650/2354]  Time: 0.184 (0.135)  Loss:  2.8477 (2.3433)  Acc@1: 25.0000 (45.1999)  Acc@5: 75.0000 (77.9376)\n",
            "Test: [1700/2354]  Time: 0.151 (0.135)  Loss:  1.5439 (2.3415)  Acc@1: 75.0000 (45.0470)  Acc@5: 100.0000 (78.0276)\n",
            "Test: [1750/2354]  Time: 0.162 (0.135)  Loss:  3.4492 (2.3480)  Acc@1:  0.0000 (44.7173)  Acc@5: 50.0000 (77.8841)\n",
            "Test: [1800/2354]  Time: 0.076 (0.135)  Loss:  2.3359 (2.3480)  Acc@1: 50.0000 (44.6557)  Acc@5: 50.0000 (77.6097)\n",
            "Test: [1850/2354]  Time: 0.103 (0.135)  Loss:  1.9580 (2.3292)  Acc@1: 75.0000 (45.2323)  Acc@5: 100.0000 (77.7283)\n",
            "Test: [1900/2354]  Time: 0.103 (0.135)  Loss:  2.4785 (2.3349)  Acc@1: 50.0000 (45.1867)  Acc@5: 75.0000 (77.7223)\n",
            "Test: [1950/2354]  Time: 0.143 (0.135)  Loss:  3.2500 (2.3321)  Acc@1:  0.0000 (45.3998)  Acc@5: 25.0000 (77.8575)\n",
            "Test: [2000/2354]  Time: 0.145 (0.134)  Loss:  1.4023 (2.3328)  Acc@1: 100.0000 (45.4398)  Acc@5: 100.0000 (77.7236)\n",
            "Test: [2050/2354]  Time: 0.160 (0.135)  Loss:  4.8906 (2.3395)  Acc@1:  0.0000 (45.5022)  Acc@5:  0.0000 (77.4744)\n",
            "Test: [2100/2354]  Time: 0.211 (0.135)  Loss:  3.0664 (2.3337)  Acc@1:  0.0000 (45.5497)  Acc@5: 25.0000 (77.6178)\n",
            "Test: [2150/2354]  Time: 0.104 (0.135)  Loss:  3.1016 (2.3500)  Acc@1:  0.0000 (45.0139)  Acc@5: 75.0000 (77.3477)\n",
            "Test: [2200/2354]  Time: 0.083 (0.135)  Loss:  3.0938 (2.3593)  Acc@1:  0.0000 (44.5593)  Acc@5: 75.0000 (77.1240)\n",
            "Test: [2250/2354]  Time: 0.188 (0.135)  Loss:  2.3477 (2.3637)  Acc@1: 50.0000 (44.6024)  Acc@5: 75.0000 (77.0880)\n",
            "Test: [2300/2354]  Time: 0.124 (0.135)  Loss:  2.4453 (2.3683)  Acc@1: 25.0000 (44.7414)  Acc@5: 100.0000 (77.0100)\n",
            "Test: [2350/2354]  Time: 0.075 (0.135)  Loss:  4.1016 (2.3736)  Acc@1:  0.0000 (44.6831)  Acc@5: 25.0000 (76.9885)\n",
            "Test: [2354/2354]  Time: 0.064 (0.135)  Loss:  3.4199 (2.3756)  Acc@1:  0.0000 (44.6332)  Acc@5: 66.6667 (76.9296)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-5.pth.tar', 44.633188236543155)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-4.pth.tar', 39.08058180273915)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-3.pth.tar', 23.29334324238242)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-2.pth.tar', 16.477333050217645)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-1.pth.tar', 6.009130480942775)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-0.pth.tar', 1.295254273277418)\n",
            "\n",
            "Train: 6 [   0/2354 (  0%)]  Loss: 2.433 (2.43)  Time: 1.182s,    6.77/s  (1.182s,    6.77/s)  LR: 9.863e-05  Data: 0.551 (0.551)\n",
            "Train: 6 [  50/2354 (  2%)]  Loss: 3.316 (3.54)  Time: 0.456s,   17.56/s  (0.495s,   16.17/s)  LR: 9.863e-05  Data: 0.006 (0.019)\n",
            "Train: 6 [ 100/2354 (  4%)]  Loss: 3.287 (3.48)  Time: 0.463s,   17.28/s  (0.480s,   16.67/s)  LR: 9.863e-05  Data: 0.009 (0.013)\n",
            "Train: 6 [ 150/2354 (  6%)]  Loss: 3.194 (3.50)  Time: 0.474s,   16.87/s  (0.475s,   16.84/s)  LR: 9.863e-05  Data: 0.006 (0.012)\n",
            "Train: 6 [ 200/2354 (  8%)]  Loss: 4.194 (3.48)  Time: 0.470s,   17.01/s  (0.477s,   16.77/s)  LR: 9.863e-05  Data: 0.015 (0.011)\n",
            "Train: 6 [ 250/2354 ( 11%)]  Loss: 2.846 (3.51)  Time: 0.470s,   17.03/s  (0.475s,   16.85/s)  LR: 9.863e-05  Data: 0.007 (0.010)\n",
            "Train: 6 [ 300/2354 ( 13%)]  Loss: 2.711 (3.54)  Time: 0.465s,   17.21/s  (0.473s,   16.91/s)  LR: 9.863e-05  Data: 0.006 (0.010)\n",
            "Train: 6 [ 350/2354 ( 15%)]  Loss: 3.788 (3.56)  Time: 0.467s,   17.12/s  (0.472s,   16.96/s)  LR: 9.863e-05  Data: 0.007 (0.009)\n",
            "Train: 6 [ 400/2354 ( 17%)]  Loss: 4.601 (3.56)  Time: 0.462s,   17.32/s  (0.471s,   16.99/s)  LR: 9.863e-05  Data: 0.007 (0.009)\n",
            "Train: 6 [ 450/2354 ( 19%)]  Loss: 3.392 (3.56)  Time: 0.460s,   17.37/s  (0.473s,   16.92/s)  LR: 9.863e-05  Data: 0.007 (0.009)\n",
            "Train: 6 [ 500/2354 ( 21%)]  Loss: 2.278 (3.53)  Time: 0.469s,   17.06/s  (0.472s,   16.95/s)  LR: 9.863e-05  Data: 0.009 (0.009)\n",
            "Train: 6 [ 550/2354 ( 23%)]  Loss: 2.434 (3.51)  Time: 0.476s,   16.81/s  (0.472s,   16.97/s)  LR: 9.863e-05  Data: 0.009 (0.009)\n",
            "Train: 6 [ 600/2354 ( 25%)]  Loss: 3.905 (3.50)  Time: 0.469s,   17.08/s  (0.471s,   16.98/s)  LR: 9.863e-05  Data: 0.009 (0.009)\n",
            "Train: 6 [ 650/2354 ( 28%)]  Loss: 3.685 (3.50)  Time: 0.467s,   17.11/s  (0.471s,   16.99/s)  LR: 9.863e-05  Data: 0.007 (0.009)\n",
            "Train: 6 [ 700/2354 ( 30%)]  Loss: 3.379 (3.50)  Time: 0.461s,   17.36/s  (0.470s,   17.00/s)  LR: 9.863e-05  Data: 0.007 (0.009)\n",
            "Train: 6 [ 750/2354 ( 32%)]  Loss: 3.150 (3.52)  Time: 0.465s,   17.19/s  (0.471s,   16.97/s)  LR: 9.863e-05  Data: 0.011 (0.009)\n",
            "Train: 6 [ 800/2354 ( 34%)]  Loss: 3.238 (3.51)  Time: 0.463s,   17.27/s  (0.471s,   16.98/s)  LR: 9.863e-05  Data: 0.009 (0.009)\n",
            "Train: 6 [ 850/2354 ( 36%)]  Loss: 2.699 (3.51)  Time: 0.474s,   16.86/s  (0.471s,   16.99/s)  LR: 9.863e-05  Data: 0.009 (0.009)\n",
            "Train: 6 [ 900/2354 ( 38%)]  Loss: 4.418 (3.51)  Time: 0.461s,   17.37/s  (0.470s,   17.00/s)  LR: 9.863e-05  Data: 0.006 (0.008)\n",
            "Train: 6 [ 950/2354 ( 40%)]  Loss: 3.805 (3.51)  Time: 0.464s,   17.26/s  (0.470s,   17.01/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [1000/2354 ( 42%)]  Loss: 3.699 (3.51)  Time: 0.470s,   17.02/s  (0.471s,   16.98/s)  LR: 9.863e-05  Data: 0.009 (0.008)\n",
            "Train: 6 [1050/2354 ( 45%)]  Loss: 2.530 (3.52)  Time: 0.468s,   17.09/s  (0.471s,   16.99/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [1100/2354 ( 47%)]  Loss: 2.584 (3.52)  Time: 0.458s,   17.47/s  (0.471s,   17.00/s)  LR: 9.863e-05  Data: 0.006 (0.008)\n",
            "Train: 6 [1150/2354 ( 49%)]  Loss: 4.955 (3.51)  Time: 0.462s,   17.32/s  (0.470s,   17.01/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [1200/2354 ( 51%)]  Loss: 3.382 (3.51)  Time: 0.475s,   16.84/s  (0.470s,   17.01/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [1250/2354 ( 53%)]  Loss: 2.710 (3.51)  Time: 0.476s,   16.79/s  (0.471s,   16.99/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [1300/2354 ( 55%)]  Loss: 4.148 (3.51)  Time: 0.473s,   16.91/s  (0.471s,   17.00/s)  LR: 9.863e-05  Data: 0.010 (0.008)\n",
            "Train: 6 [1350/2354 ( 57%)]  Loss: 4.518 (3.50)  Time: 0.467s,   17.15/s  (0.471s,   17.00/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [1400/2354 ( 59%)]  Loss: 3.791 (3.51)  Time: 0.462s,   17.31/s  (0.470s,   17.01/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [1450/2354 ( 62%)]  Loss: 4.230 (3.51)  Time: 0.462s,   17.33/s  (0.470s,   17.01/s)  LR: 9.863e-05  Data: 0.010 (0.008)\n",
            "Train: 6 [1500/2354 ( 64%)]  Loss: 4.386 (3.51)  Time: 0.466s,   17.17/s  (0.471s,   16.99/s)  LR: 9.863e-05  Data: 0.010 (0.008)\n",
            "Train: 6 [1550/2354 ( 66%)]  Loss: 3.428 (3.51)  Time: 0.478s,   16.74/s  (0.471s,   16.99/s)  LR: 9.863e-05  Data: 0.008 (0.008)\n",
            "Train: 6 [1600/2354 ( 68%)]  Loss: 4.132 (3.51)  Time: 0.463s,   17.28/s  (0.471s,   17.00/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [1650/2354 ( 70%)]  Loss: 2.535 (3.51)  Time: 0.465s,   17.22/s  (0.471s,   17.00/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [1700/2354 ( 72%)]  Loss: 4.429 (3.51)  Time: 0.466s,   17.17/s  (0.470s,   17.01/s)  LR: 9.863e-05  Data: 0.009 (0.008)\n",
            "Train: 6 [1750/2354 ( 74%)]  Loss: 3.994 (3.51)  Time: 0.473s,   16.91/s  (0.471s,   16.99/s)  LR: 9.863e-05  Data: 0.006 (0.008)\n",
            "Train: 6 [1800/2354 ( 76%)]  Loss: 3.321 (3.51)  Time: 0.461s,   17.37/s  (0.471s,   16.99/s)  LR: 9.863e-05  Data: 0.010 (0.008)\n",
            "Train: 6 [1850/2354 ( 79%)]  Loss: 3.650 (3.51)  Time: 0.461s,   17.34/s  (0.471s,   17.00/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [1900/2354 ( 81%)]  Loss: 3.250 (3.51)  Time: 0.467s,   17.12/s  (0.471s,   17.00/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [1950/2354 ( 83%)]  Loss: 3.624 (3.51)  Time: 0.464s,   17.25/s  (0.471s,   17.00/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [2000/2354 ( 85%)]  Loss: 3.614 (3.51)  Time: 0.617s,   12.96/s  (0.471s,   17.00/s)  LR: 9.863e-05  Data: 0.023 (0.008)\n",
            "Train: 6 [2050/2354 ( 87%)]  Loss: 4.281 (3.51)  Time: 0.478s,   16.75/s  (0.471s,   16.99/s)  LR: 9.863e-05  Data: 0.006 (0.008)\n",
            "Train: 6 [2100/2354 ( 89%)]  Loss: 3.044 (3.51)  Time: 0.462s,   17.32/s  (0.471s,   16.99/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [2150/2354 ( 91%)]  Loss: 3.570 (3.51)  Time: 0.459s,   17.44/s  (0.471s,   17.00/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [2200/2354 ( 93%)]  Loss: 3.093 (3.51)  Time: 0.465s,   17.21/s  (0.471s,   17.00/s)  LR: 9.863e-05  Data: 0.010 (0.008)\n",
            "Train: 6 [2250/2354 ( 96%)]  Loss: 2.569 (3.51)  Time: 0.471s,   16.98/s  (0.471s,   17.00/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [2300/2354 ( 98%)]  Loss: 3.599 (3.50)  Time: 0.470s,   17.03/s  (0.471s,   16.99/s)  LR: 9.863e-05  Data: 0.007 (0.008)\n",
            "Train: 6 [2350/2354 (100%)]  Loss: 5.167 (3.50)  Time: 0.465s,   17.19/s  (0.471s,   16.99/s)  LR: 9.863e-05  Data: 0.005 (0.008)\n",
            "Train: 6 [2353/2354 (100%)]  Loss: 2.958 (3.50)  Time: 0.453s,   17.65/s  (0.471s,   16.99/s)  LR: 9.863e-05  Data: 0.000 (0.008)\n",
            "Test: [   0/2354]  Time: 0.643 (0.643)  Loss:  2.9492 (2.9492)  Acc@1:  0.0000 ( 0.0000)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [  50/2354]  Time: 0.175 (0.150)  Loss:  4.5117 (1.8624)  Acc@1:  0.0000 (64.2157)  Acc@5:  0.0000 (80.8824)\n",
            "Test: [ 100/2354]  Time: 0.227 (0.141)  Loss:  4.3867 (2.2369)  Acc@1:  0.0000 (49.2574)  Acc@5:  0.0000 (76.7327)\n",
            "Test: [ 150/2354]  Time: 0.140 (0.140)  Loss:  0.9072 (1.9917)  Acc@1: 75.0000 (53.4768)  Acc@5: 100.0000 (81.9536)\n",
            "Test: [ 200/2354]  Time: 0.127 (0.139)  Loss:  4.1016 (1.9048)  Acc@1:  0.0000 (56.8408)  Acc@5: 25.0000 (83.8308)\n",
            "Test: [ 250/2354]  Time: 0.099 (0.139)  Loss:  0.4521 (2.1440)  Acc@1: 100.0000 (51.7928)  Acc@5: 100.0000 (78.6853)\n",
            "Test: [ 300/2354]  Time: 0.183 (0.138)  Loss:  1.7803 (2.1076)  Acc@1: 100.0000 (52.6578)  Acc@5: 100.0000 (79.9834)\n",
            "Test: [ 350/2354]  Time: 0.158 (0.138)  Loss:  3.1660 (2.0833)  Acc@1:  0.0000 (52.5641)  Acc@5: 50.0000 (81.6239)\n",
            "Test: [ 400/2354]  Time: 0.112 (0.137)  Loss:  1.7861 (2.0861)  Acc@1: 75.0000 (52.4938)  Acc@5: 75.0000 (82.1072)\n",
            "Test: [ 450/2354]  Time: 0.129 (0.138)  Loss:  1.3311 (2.0750)  Acc@1: 75.0000 (51.9401)  Acc@5: 100.0000 (82.2616)\n",
            "Test: [ 500/2354]  Time: 0.133 (0.138)  Loss:  1.2520 (2.0203)  Acc@1: 75.0000 (53.2934)  Acc@5: 100.0000 (83.0838)\n",
            "Test: [ 550/2354]  Time: 0.293 (0.140)  Loss:  2.4980 (1.9932)  Acc@1: 50.0000 (53.6298)  Acc@5: 75.0000 (83.5753)\n",
            "Test: [ 600/2354]  Time: 0.138 (0.141)  Loss:  2.2598 (2.0044)  Acc@1:  0.0000 (53.4526)  Acc@5: 100.0000 (83.9434)\n",
            "Test: [ 650/2354]  Time: 0.215 (0.141)  Loss:  1.0391 (1.9933)  Acc@1: 50.0000 (53.5714)  Acc@5: 100.0000 (84.1782)\n",
            "Test: [ 700/2354]  Time: 0.136 (0.141)  Loss:  2.1602 (1.9636)  Acc@1: 75.0000 (54.7076)  Acc@5: 100.0000 (84.2011)\n",
            "Test: [ 750/2354]  Time: 0.123 (0.140)  Loss:  1.5537 (1.9698)  Acc@1: 75.0000 (54.4940)  Acc@5: 100.0000 (83.7217)\n",
            "Test: [ 800/2354]  Time: 0.157 (0.140)  Loss:  0.7773 (1.9810)  Acc@1: 100.0000 (53.6205)  Acc@5: 100.0000 (83.6142)\n",
            "Test: [ 850/2354]  Time: 0.143 (0.140)  Loss:  0.5771 (1.9598)  Acc@1: 100.0000 (54.0541)  Acc@5: 100.0000 (83.8132)\n",
            "Test: [ 900/2354]  Time: 0.173 (0.139)  Loss:  1.6406 (1.9268)  Acc@1: 75.0000 (55.1054)  Acc@5: 75.0000 (84.1565)\n",
            "Test: [ 950/2354]  Time: 0.090 (0.139)  Loss:  1.4199 (1.9230)  Acc@1: 75.0000 (54.4690)  Acc@5: 100.0000 (84.5426)\n",
            "Test: [1000/2354]  Time: 0.113 (0.139)  Loss:  1.5791 (1.9115)  Acc@1:  0.0000 (54.9451)  Acc@5: 100.0000 (84.7153)\n",
            "Test: [1050/2354]  Time: 0.179 (0.139)  Loss:  2.2012 (1.9310)  Acc@1: 25.0000 (54.1627)  Acc@5: 75.0000 (84.3482)\n",
            "Test: [1100/2354]  Time: 0.115 (0.138)  Loss:  1.4893 (1.9327)  Acc@1: 75.0000 (54.3143)  Acc@5: 100.0000 (84.3551)\n",
            "Test: [1150/2354]  Time: 0.076 (0.138)  Loss:  2.5566 (1.9154)  Acc@1: 50.0000 (55.1260)  Acc@5: 100.0000 (84.5569)\n",
            "Test: [1200/2354]  Time: 0.134 (0.138)  Loss:  2.0645 (1.9167)  Acc@1: 50.0000 (54.8918)  Acc@5: 75.0000 (84.7419)\n",
            "Test: [1250/2354]  Time: 0.078 (0.138)  Loss:  0.3989 (1.9027)  Acc@1: 100.0000 (55.2958)  Acc@5: 100.0000 (84.8122)\n",
            "Test: [1300/2354]  Time: 0.161 (0.137)  Loss:  1.7383 (1.8869)  Acc@1: 50.0000 (55.4189)  Acc@5: 100.0000 (85.0884)\n",
            "Test: [1350/2354]  Time: 0.110 (0.137)  Loss:  0.8726 (1.8854)  Acc@1: 100.0000 (55.4959)  Acc@5: 100.0000 (85.0481)\n",
            "Test: [1400/2354]  Time: 0.078 (0.137)  Loss:  1.5391 (1.8861)  Acc@1: 75.0000 (55.6210)  Acc@5: 100.0000 (85.0642)\n",
            "Test: [1450/2354]  Time: 0.172 (0.138)  Loss:  1.2881 (1.8695)  Acc@1: 75.0000 (56.1165)  Acc@5: 100.0000 (85.4411)\n",
            "Test: [1500/2354]  Time: 0.138 (0.138)  Loss:  0.7480 (1.8706)  Acc@1: 100.0000 (55.8794)  Acc@5: 100.0000 (85.4930)\n",
            "Test: [1550/2354]  Time: 0.110 (0.138)  Loss:  0.3567 (1.8860)  Acc@1: 100.0000 (55.2708)  Acc@5: 100.0000 (84.9613)\n",
            "Test: [1600/2354]  Time: 0.111 (0.138)  Loss:  2.7773 (1.8936)  Acc@1: 50.0000 (55.2155)  Acc@5: 50.0000 (84.8376)\n",
            "Test: [1650/2354]  Time: 0.076 (0.137)  Loss:  1.2061 (1.8915)  Acc@1: 75.0000 (55.1787)  Acc@5: 100.0000 (84.8728)\n",
            "Test: [1700/2354]  Time: 0.114 (0.137)  Loss:  1.7285 (1.8833)  Acc@1: 50.0000 (55.3939)  Acc@5: 100.0000 (85.1558)\n",
            "Test: [1750/2354]  Time: 0.194 (0.137)  Loss:  2.0859 (1.8992)  Acc@1: 25.0000 (54.5117)  Acc@5: 100.0000 (84.7801)\n",
            "Test: [1800/2354]  Time: 0.117 (0.137)  Loss:  2.1230 (1.8973)  Acc@1: 50.0000 (54.6502)  Acc@5: 75.0000 (84.8418)\n",
            "Test: [1850/2354]  Time: 0.099 (0.137)  Loss:  1.9014 (1.8903)  Acc@1: 75.0000 (54.8757)  Acc@5: 100.0000 (84.9406)\n",
            "Test: [1900/2354]  Time: 0.151 (0.137)  Loss:  1.7773 (1.8894)  Acc@1: 75.0000 (55.0368)  Acc@5: 75.0000 (85.0079)\n",
            "Test: [1950/2354]  Time: 0.121 (0.137)  Loss:  2.5586 (1.8895)  Acc@1:  0.0000 (55.0999)  Acc@5: 75.0000 (84.9949)\n",
            "Test: [2000/2354]  Time: 0.121 (0.137)  Loss:  2.2852 (1.8910)  Acc@1: 25.0000 (55.0100)  Acc@5: 75.0000 (85.1199)\n",
            "Test: [2050/2354]  Time: 0.102 (0.137)  Loss:  3.3750 (1.8995)  Acc@1:  0.0000 (54.8391)  Acc@5: 50.0000 (84.9464)\n",
            "Test: [2100/2354]  Time: 0.194 (0.136)  Loss:  2.0859 (1.8919)  Acc@1: 50.0000 (55.1642)  Acc@5: 100.0000 (85.1380)\n",
            "Test: [2150/2354]  Time: 0.155 (0.136)  Loss:  2.8555 (1.9132)  Acc@1:  0.0000 (54.3701)  Acc@5: 50.0000 (84.6583)\n",
            "Test: [2200/2354]  Time: 0.099 (0.136)  Loss:  4.2500 (1.9173)  Acc@1:  0.0000 (54.1686)  Acc@5: 25.0000 (84.6206)\n",
            "Test: [2250/2354]  Time: 0.117 (0.136)  Loss:  1.0439 (1.9169)  Acc@1: 75.0000 (54.2537)  Acc@5: 100.0000 (84.6291)\n",
            "Test: [2300/2354]  Time: 0.083 (0.136)  Loss:  3.0625 (1.9172)  Acc@1: 25.0000 (54.2482)  Acc@5: 50.0000 (84.5937)\n",
            "Test: [2350/2354]  Time: 0.077 (0.137)  Loss:  4.0742 (1.9366)  Acc@1:  0.0000 (53.7325)  Acc@5: 25.0000 (84.1663)\n",
            "Test: [2354/2354]  Time: 0.064 (0.137)  Loss:  2.8867 (1.9383)  Acc@1: 33.3333 (53.6787)  Acc@5: 33.3333 (84.1172)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-6.pth.tar', 53.67873447368398)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-5.pth.tar', 44.633188236543155)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-4.pth.tar', 39.08058180273915)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-3.pth.tar', 23.29334324238242)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-2.pth.tar', 16.477333050217645)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-1.pth.tar', 6.009130480942775)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-0.pth.tar', 1.295254273277418)\n",
            "\n",
            "Train: 7 [   0/2354 (  0%)]  Loss: 2.643 (2.64)  Time: 1.279s,    6.25/s  (1.279s,    6.25/s)  LR: 9.814e-05  Data: 0.590 (0.590)\n",
            "Train: 7 [  50/2354 (  2%)]  Loss: 3.708 (3.39)  Time: 0.466s,   17.18/s  (0.497s,   16.11/s)  LR: 9.814e-05  Data: 0.007 (0.019)\n",
            "Train: 7 [ 100/2354 (  4%)]  Loss: 3.388 (3.34)  Time: 0.459s,   17.45/s  (0.481s,   16.64/s)  LR: 9.814e-05  Data: 0.007 (0.013)\n",
            "Train: 7 [ 150/2354 (  6%)]  Loss: 2.969 (3.27)  Time: 0.469s,   17.05/s  (0.475s,   16.83/s)  LR: 9.814e-05  Data: 0.010 (0.011)\n",
            "Train: 7 [ 200/2354 (  8%)]  Loss: 2.922 (3.28)  Time: 0.466s,   17.18/s  (0.473s,   16.92/s)  LR: 9.814e-05  Data: 0.006 (0.011)\n",
            "Train: 7 [ 250/2354 ( 11%)]  Loss: 2.487 (3.31)  Time: 0.463s,   17.27/s  (0.475s,   16.85/s)  LR: 9.814e-05  Data: 0.009 (0.010)\n",
            "Train: 7 [ 300/2354 ( 13%)]  Loss: 3.090 (3.32)  Time: 0.463s,   17.26/s  (0.473s,   16.91/s)  LR: 9.814e-05  Data: 0.007 (0.010)\n",
            "Train: 7 [ 350/2354 ( 15%)]  Loss: 2.358 (3.33)  Time: 0.461s,   17.35/s  (0.472s,   16.96/s)  LR: 9.814e-05  Data: 0.007 (0.009)\n",
            "Train: 7 [ 400/2354 ( 17%)]  Loss: 3.811 (3.33)  Time: 0.466s,   17.16/s  (0.471s,   16.99/s)  LR: 9.814e-05  Data: 0.007 (0.009)\n",
            "Train: 7 [ 450/2354 ( 19%)]  Loss: 3.156 (3.33)  Time: 0.474s,   16.88/s  (0.470s,   17.01/s)  LR: 9.814e-05  Data: 0.010 (0.009)\n",
            "Train: 7 [ 500/2354 ( 21%)]  Loss: 2.535 (3.35)  Time: 0.459s,   17.44/s  (0.472s,   16.96/s)  LR: 9.814e-05  Data: 0.006 (0.009)\n",
            "Train: 7 [ 550/2354 ( 23%)]  Loss: 4.727 (3.33)  Time: 0.461s,   17.36/s  (0.471s,   16.98/s)  LR: 9.814e-05  Data: 0.006 (0.009)\n",
            "Train: 7 [ 600/2354 ( 25%)]  Loss: 2.273 (3.34)  Time: 0.471s,   16.98/s  (0.471s,   17.00/s)  LR: 9.814e-05  Data: 0.009 (0.009)\n",
            "Train: 7 [ 650/2354 ( 28%)]  Loss: 3.605 (3.33)  Time: 0.460s,   17.38/s  (0.470s,   17.02/s)  LR: 9.814e-05  Data: 0.009 (0.009)\n",
            "Train: 7 [ 700/2354 ( 30%)]  Loss: 1.860 (3.33)  Time: 0.465s,   17.19/s  (0.470s,   17.03/s)  LR: 9.814e-05  Data: 0.011 (0.009)\n",
            "Train: 7 [ 750/2354 ( 32%)]  Loss: 3.985 (3.32)  Time: 0.463s,   17.27/s  (0.471s,   17.00/s)  LR: 9.814e-05  Data: 0.006 (0.009)\n",
            "Train: 7 [ 800/2354 ( 34%)]  Loss: 3.797 (3.33)  Time: 0.469s,   17.07/s  (0.470s,   17.02/s)  LR: 9.814e-05  Data: 0.010 (0.009)\n",
            "Train: 7 [ 850/2354 ( 36%)]  Loss: 3.322 (3.34)  Time: 0.460s,   17.39/s  (0.470s,   17.03/s)  LR: 9.814e-05  Data: 0.006 (0.009)\n",
            "Train: 7 [ 900/2354 ( 38%)]  Loss: 2.988 (3.34)  Time: 0.463s,   17.27/s  (0.469s,   17.04/s)  LR: 9.814e-05  Data: 0.006 (0.008)\n",
            "Train: 7 [ 950/2354 ( 40%)]  Loss: 3.206 (3.33)  Time: 0.460s,   17.40/s  (0.469s,   17.05/s)  LR: 9.814e-05  Data: 0.009 (0.008)\n",
            "Train: 7 [1000/2354 ( 42%)]  Loss: 2.780 (3.33)  Time: 0.570s,   14.03/s  (0.470s,   17.03/s)  LR: 9.814e-05  Data: 0.012 (0.008)\n",
            "Train: 7 [1050/2354 ( 45%)]  Loss: 2.180 (3.32)  Time: 0.461s,   17.36/s  (0.470s,   17.04/s)  LR: 9.814e-05  Data: 0.006 (0.008)\n",
            "Train: 7 [1100/2354 ( 47%)]  Loss: 3.091 (3.32)  Time: 0.464s,   17.23/s  (0.469s,   17.05/s)  LR: 9.814e-05  Data: 0.006 (0.008)\n",
            "Train: 7 [1150/2354 ( 49%)]  Loss: 3.249 (3.31)  Time: 0.465s,   17.21/s  (0.469s,   17.05/s)  LR: 9.814e-05  Data: 0.006 (0.008)\n",
            "Train: 7 [1200/2354 ( 51%)]  Loss: 2.790 (3.32)  Time: 0.462s,   17.33/s  (0.469s,   17.06/s)  LR: 9.814e-05  Data: 0.007 (0.008)\n",
            "Train: 7 [1250/2354 ( 53%)]  Loss: 4.415 (3.32)  Time: 0.561s,   14.26/s  (0.469s,   17.06/s)  LR: 9.814e-05  Data: 0.013 (0.008)\n",
            "Train: 7 [1300/2354 ( 55%)]  Loss: 3.423 (3.32)  Time: 0.462s,   17.30/s  (0.469s,   17.05/s)  LR: 9.814e-05  Data: 0.006 (0.008)\n",
            "Train: 7 [1350/2354 ( 57%)]  Loss: 3.902 (3.32)  Time: 0.473s,   16.91/s  (0.469s,   17.05/s)  LR: 9.814e-05  Data: 0.008 (0.008)\n",
            "Train: 7 [1400/2354 ( 59%)]  Loss: 3.623 (3.32)  Time: 0.463s,   17.29/s  (0.469s,   17.06/s)  LR: 9.814e-05  Data: 0.006 (0.008)\n",
            "Train: 7 [1450/2354 ( 62%)]  Loss: 3.005 (3.31)  Time: 0.464s,   17.24/s  (0.469s,   17.06/s)  LR: 9.814e-05  Data: 0.007 (0.008)\n",
            "Train: 7 [1500/2354 ( 64%)]  Loss: 1.927 (3.31)  Time: 0.592s,   13.51/s  (0.469s,   17.06/s)  LR: 9.814e-05  Data: 0.018 (0.008)\n",
            "Train: 7 [1550/2354 ( 66%)]  Loss: 5.100 (3.31)  Time: 0.461s,   17.36/s  (0.469s,   17.05/s)  LR: 9.814e-05  Data: 0.007 (0.008)\n",
            "Train: 7 [1600/2354 ( 68%)]  Loss: 3.867 (3.31)  Time: 0.463s,   17.28/s  (0.469s,   17.06/s)  LR: 9.814e-05  Data: 0.008 (0.008)\n",
            "Train: 7 [1650/2354 ( 70%)]  Loss: 3.695 (3.31)  Time: 0.461s,   17.34/s  (0.469s,   17.07/s)  LR: 9.814e-05  Data: 0.007 (0.008)\n",
            "Train: 7 [1700/2354 ( 72%)]  Loss: 2.351 (3.31)  Time: 0.456s,   17.56/s  (0.469s,   17.07/s)  LR: 9.814e-05  Data: 0.007 (0.008)\n",
            "Train: 7 [1750/2354 ( 74%)]  Loss: 2.660 (3.31)  Time: 0.507s,   15.77/s  (0.469s,   17.07/s)  LR: 9.814e-05  Data: 0.007 (0.008)\n",
            "Train: 7 [1800/2354 ( 76%)]  Loss: 2.895 (3.31)  Time: 0.464s,   17.25/s  (0.469s,   17.06/s)  LR: 9.814e-05  Data: 0.006 (0.008)\n",
            "Train: 7 [1850/2354 ( 79%)]  Loss: 2.536 (3.30)  Time: 0.468s,   17.09/s  (0.469s,   17.07/s)  LR: 9.814e-05  Data: 0.009 (0.008)\n",
            "Train: 7 [1900/2354 ( 81%)]  Loss: 2.567 (3.30)  Time: 0.464s,   17.25/s  (0.469s,   17.07/s)  LR: 9.814e-05  Data: 0.006 (0.008)\n",
            "Train: 7 [1950/2354 ( 83%)]  Loss: 4.194 (3.30)  Time: 0.468s,   17.11/s  (0.469s,   17.08/s)  LR: 9.814e-05  Data: 0.007 (0.008)\n",
            "Train: 7 [2000/2354 ( 85%)]  Loss: 3.514 (3.30)  Time: 0.469s,   17.07/s  (0.468s,   17.08/s)  LR: 9.814e-05  Data: 0.007 (0.008)\n",
            "Train: 7 [2050/2354 ( 87%)]  Loss: 2.270 (3.30)  Time: 0.467s,   17.13/s  (0.469s,   17.06/s)  LR: 9.814e-05  Data: 0.009 (0.008)\n",
            "Train: 7 [2100/2354 ( 89%)]  Loss: 4.008 (3.30)  Time: 0.464s,   17.26/s  (0.469s,   17.07/s)  LR: 9.814e-05  Data: 0.010 (0.008)\n",
            "Train: 7 [2150/2354 ( 91%)]  Loss: 3.786 (3.30)  Time: 0.463s,   17.28/s  (0.469s,   17.07/s)  LR: 9.814e-05  Data: 0.007 (0.008)\n",
            "Train: 7 [2200/2354 ( 93%)]  Loss: 2.514 (3.30)  Time: 0.464s,   17.23/s  (0.469s,   17.08/s)  LR: 9.814e-05  Data: 0.007 (0.008)\n",
            "Train: 7 [2250/2354 ( 96%)]  Loss: 3.194 (3.31)  Time: 0.461s,   17.35/s  (0.468s,   17.08/s)  LR: 9.814e-05  Data: 0.006 (0.008)\n",
            "Train: 7 [2300/2354 ( 98%)]  Loss: 4.240 (3.31)  Time: 0.463s,   17.27/s  (0.469s,   17.07/s)  LR: 9.814e-05  Data: 0.009 (0.008)\n",
            "Train: 7 [2350/2354 (100%)]  Loss: 3.151 (3.31)  Time: 0.454s,   17.62/s  (0.469s,   17.07/s)  LR: 9.814e-05  Data: 0.005 (0.008)\n",
            "Train: 7 [2353/2354 (100%)]  Loss: 2.817 (3.31)  Time: 0.461s,   17.35/s  (0.469s,   17.07/s)  LR: 9.814e-05  Data: 0.000 (0.008)\n",
            "Test: [   0/2354]  Time: 0.637 (0.637)  Loss:  3.7949 (3.7949)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test: [  50/2354]  Time: 0.119 (0.146)  Loss:  4.6016 (2.0657)  Acc@1:  0.0000 (54.9020)  Acc@5: 25.0000 (79.9020)\n",
            "Test: [ 100/2354]  Time: 0.152 (0.137)  Loss:  4.2461 (2.3129)  Acc@1:  0.0000 (46.2871)  Acc@5:  0.0000 (77.7228)\n",
            "Test: [ 150/2354]  Time: 0.167 (0.135)  Loss:  1.0586 (2.0622)  Acc@1: 75.0000 (50.9934)  Acc@5: 100.0000 (83.1126)\n",
            "Test: [ 200/2354]  Time: 0.100 (0.134)  Loss:  4.1797 (2.0208)  Acc@1:  0.0000 (53.2338)  Acc@5:  0.0000 (83.7065)\n",
            "Test: [ 250/2354]  Time: 0.158 (0.133)  Loss:  0.6035 (2.2034)  Acc@1: 100.0000 (48.0080)  Acc@5: 100.0000 (80.4781)\n",
            "Test: [ 300/2354]  Time: 0.151 (0.133)  Loss:  2.3652 (2.1799)  Acc@1:  0.0000 (49.0033)  Acc@5: 75.0000 (81.3953)\n",
            "Test: [ 350/2354]  Time: 0.109 (0.133)  Loss:  2.8496 (2.1187)  Acc@1:  0.0000 (48.5043)  Acc@5: 75.0000 (83.0484)\n",
            "Test: [ 400/2354]  Time: 0.096 (0.133)  Loss:  1.2578 (2.0319)  Acc@1: 75.0000 (51.4339)  Acc@5: 100.0000 (84.1646)\n",
            "Test: [ 450/2354]  Time: 0.154 (0.133)  Loss:  0.8716 (1.9423)  Acc@1: 100.0000 (54.6563)  Acc@5: 100.0000 (85.2550)\n",
            "Test: [ 500/2354]  Time: 0.162 (0.133)  Loss:  1.1719 (1.9115)  Acc@1: 75.0000 (54.8403)  Acc@5: 100.0000 (86.0279)\n",
            "Test: [ 550/2354]  Time: 0.130 (0.137)  Loss:  2.5684 (1.8853)  Acc@1: 50.0000 (54.4011)  Acc@5: 75.0000 (86.5699)\n",
            "Test: [ 600/2354]  Time: 0.134 (0.137)  Loss:  1.9414 (1.8759)  Acc@1: 50.0000 (54.9917)  Acc@5: 100.0000 (87.0632)\n",
            "Test: [ 650/2354]  Time: 0.099 (0.136)  Loss:  1.4404 (1.8562)  Acc@1: 50.0000 (55.7220)  Acc@5: 100.0000 (87.3656)\n",
            "Test: [ 700/2354]  Time: 0.105 (0.136)  Loss:  1.2432 (1.8488)  Acc@1: 75.0000 (56.3481)  Acc@5: 100.0000 (87.6605)\n",
            "Test: [ 750/2354]  Time: 0.137 (0.136)  Loss:  1.6553 (1.8311)  Acc@1: 75.0000 (57.2903)  Acc@5: 75.0000 (88.0160)\n",
            "Test: [ 800/2354]  Time: 0.111 (0.135)  Loss:  0.4844 (1.8417)  Acc@1: 100.0000 (56.9913)  Acc@5: 100.0000 (87.8901)\n",
            "Test: [ 850/2354]  Time: 0.171 (0.135)  Loss:  0.7529 (1.8252)  Acc@1: 100.0000 (57.1680)  Acc@5: 100.0000 (88.1904)\n",
            "Test: [ 900/2354]  Time: 0.113 (0.135)  Loss:  1.5947 (1.8060)  Acc@1: 75.0000 (57.6304)  Acc@5: 75.0000 (88.4573)\n",
            "Test: [ 950/2354]  Time: 0.168 (0.135)  Loss:  0.9648 (1.7710)  Acc@1: 50.0000 (58.4385)  Acc@5: 100.0000 (88.7487)\n",
            "Test: [1000/2354]  Time: 0.099 (0.135)  Loss:  1.7744 (1.7740)  Acc@1: 25.0000 (58.2168)  Acc@5: 100.0000 (88.6613)\n",
            "Test: [1050/2354]  Time: 0.155 (0.135)  Loss:  1.8301 (1.7839)  Acc@1: 25.0000 (57.9448)  Acc@5: 100.0000 (88.4872)\n",
            "Test: [1100/2354]  Time: 0.152 (0.135)  Loss:  2.2383 (1.7845)  Acc@1: 50.0000 (58.3333)  Acc@5: 75.0000 (88.6694)\n",
            "Test: [1150/2354]  Time: 0.075 (0.135)  Loss:  2.5625 (1.7679)  Acc@1: 100.0000 (59.1008)  Acc@5: 100.0000 (88.7924)\n",
            "Test: [1200/2354]  Time: 0.125 (0.135)  Loss:  1.7646 (1.7675)  Acc@1: 50.0000 (59.2839)  Acc@5: 75.0000 (88.7386)\n",
            "Test: [1250/2354]  Time: 0.120 (0.135)  Loss:  1.0254 (1.7542)  Acc@1: 100.0000 (59.6723)  Acc@5: 100.0000 (88.6491)\n",
            "Test: [1300/2354]  Time: 0.169 (0.134)  Loss:  1.1758 (1.7382)  Acc@1: 100.0000 (59.8770)  Acc@5: 100.0000 (88.8547)\n",
            "Test: [1350/2354]  Time: 0.102 (0.134)  Loss:  0.8638 (1.7498)  Acc@1: 100.0000 (59.6410)  Acc@5: 100.0000 (88.6936)\n",
            "Test: [1400/2354]  Time: 0.148 (0.136)  Loss:  1.5176 (1.7517)  Acc@1: 50.0000 (59.4040)  Acc@5: 100.0000 (88.8116)\n",
            "Test: [1450/2354]  Time: 0.164 (0.136)  Loss:  1.2979 (1.7392)  Acc@1: 75.0000 (59.3901)  Acc@5: 100.0000 (88.9904)\n",
            "Test: [1500/2354]  Time: 0.150 (0.135)  Loss:  2.5156 (1.7424)  Acc@1: 25.0000 (59.4604)  Acc@5: 100.0000 (88.9241)\n",
            "Test: [1550/2354]  Time: 0.160 (0.135)  Loss:  0.9600 (1.7598)  Acc@1: 100.0000 (58.5912)  Acc@5: 100.0000 (88.7814)\n",
            "Test: [1600/2354]  Time: 0.122 (0.135)  Loss:  1.2764 (1.7608)  Acc@1: 100.0000 (58.1199)  Acc@5: 100.0000 (88.9288)\n",
            "Test: [1650/2354]  Time: 0.126 (0.135)  Loss:  2.2012 (1.7582)  Acc@1: 50.0000 (58.2374)  Acc@5: 75.0000 (88.8704)\n",
            "Test: [1700/2354]  Time: 0.172 (0.135)  Loss:  0.5093 (1.7554)  Acc@1: 100.0000 (58.2158)  Acc@5: 100.0000 (88.7860)\n",
            "Test: [1750/2354]  Time: 0.157 (0.135)  Loss:  2.2949 (1.7544)  Acc@1: 25.0000 (58.1239)  Acc@5: 75.0000 (88.9206)\n",
            "Test: [1800/2354]  Time: 0.126 (0.135)  Loss:  2.0020 (1.7576)  Acc@1: 75.0000 (58.1621)  Acc@5: 75.0000 (88.8118)\n",
            "Test: [1850/2354]  Time: 0.152 (0.135)  Loss:  1.1289 (1.7474)  Acc@1: 75.0000 (58.5224)  Acc@5: 100.0000 (88.8574)\n",
            "Test: [1900/2354]  Time: 0.094 (0.135)  Loss:  2.5918 (1.7500)  Acc@1: 50.0000 (58.6796)  Acc@5: 75.0000 (88.7822)\n",
            "Test: [1950/2354]  Time: 0.150 (0.135)  Loss:  2.1797 (1.7614)  Acc@1: 25.0000 (58.6110)  Acc@5: 100.0000 (88.5828)\n",
            "Test: [2000/2354]  Time: 0.136 (0.135)  Loss:  0.4712 (1.7478)  Acc@1: 100.0000 (59.0955)  Acc@5: 100.0000 (88.6807)\n",
            "Test: [2050/2354]  Time: 0.090 (0.135)  Loss:  3.8516 (1.7528)  Acc@1:  0.0000 (58.9590)  Acc@5:  0.0000 (88.4081)\n",
            "Test: [2100/2354]  Time: 0.169 (0.135)  Loss:  1.0117 (1.7482)  Acc@1: 100.0000 (59.0552)  Acc@5: 100.0000 (88.4579)\n",
            "Test: [2150/2354]  Time: 0.100 (0.135)  Loss:  1.9980 (1.7582)  Acc@1:  0.0000 (58.7401)  Acc@5: 100.0000 (88.3659)\n",
            "Test: [2200/2354]  Time: 0.113 (0.135)  Loss:  4.1094 (1.7591)  Acc@1:  0.0000 (58.5756)  Acc@5:  0.0000 (88.3348)\n",
            "Test: [2250/2354]  Time: 0.137 (0.135)  Loss:  1.0684 (1.7636)  Acc@1: 75.0000 (58.6073)  Acc@5: 100.0000 (88.2163)\n",
            "Test: [2300/2354]  Time: 0.167 (0.135)  Loss:  1.0293 (1.7656)  Acc@1: 75.0000 (58.6375)  Acc@5: 100.0000 (88.1139)\n",
            "Test: [2350/2354]  Time: 0.075 (0.135)  Loss:  4.1992 (1.7713)  Acc@1:  0.0000 (58.4964)  Acc@5:  0.0000 (88.0583)\n",
            "Test: [2354/2354]  Time: 0.064 (0.135)  Loss:  2.8809 (1.7735)  Acc@1: 33.3333 (58.4245)  Acc@5: 100.0000 (88.0136)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-7.pth.tar', 58.424461196265995)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-6.pth.tar', 53.67873447368398)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-5.pth.tar', 44.633188236543155)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-4.pth.tar', 39.08058180273915)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-3.pth.tar', 23.29334324238242)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-2.pth.tar', 16.477333050217645)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-1.pth.tar', 6.009130480942775)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-0.pth.tar', 1.295254273277418)\n",
            "\n",
            "Train: 8 [   0/2354 (  0%)]  Loss: 3.902 (3.90)  Time: 1.307s,    6.12/s  (1.307s,    6.12/s)  LR: 9.758e-05  Data: 0.685 (0.685)\n",
            "Train: 8 [  50/2354 (  2%)]  Loss: 2.252 (3.07)  Time: 0.463s,   17.28/s  (0.501s,   15.98/s)  LR: 9.758e-05  Data: 0.006 (0.021)\n",
            "Train: 8 [ 100/2354 (  4%)]  Loss: 3.231 (3.11)  Time: 0.464s,   17.23/s  (0.483s,   16.57/s)  LR: 9.758e-05  Data: 0.007 (0.015)\n",
            "Train: 8 [ 150/2354 (  6%)]  Loss: 3.097 (3.12)  Time: 0.459s,   17.42/s  (0.477s,   16.78/s)  LR: 9.758e-05  Data: 0.007 (0.012)\n",
            "Train: 8 [ 200/2354 (  8%)]  Loss: 3.461 (3.10)  Time: 0.639s,   12.52/s  (0.477s,   16.76/s)  LR: 9.758e-05  Data: 0.020 (0.011)\n",
            "Train: 8 [ 250/2354 ( 11%)]  Loss: 2.698 (3.13)  Time: 0.467s,   17.14/s  (0.476s,   16.81/s)  LR: 9.758e-05  Data: 0.013 (0.011)\n",
            "Train: 8 [ 300/2354 ( 13%)]  Loss: 3.279 (3.15)  Time: 0.463s,   17.27/s  (0.474s,   16.88/s)  LR: 9.758e-05  Data: 0.009 (0.010)\n",
            "Train: 8 [ 350/2354 ( 15%)]  Loss: 3.738 (3.16)  Time: 0.464s,   17.24/s  (0.473s,   16.92/s)  LR: 9.758e-05  Data: 0.008 (0.010)\n",
            "Train: 8 [ 400/2354 ( 17%)]  Loss: 2.542 (3.14)  Time: 0.462s,   17.33/s  (0.472s,   16.96/s)  LR: 9.758e-05  Data: 0.009 (0.009)\n",
            "Train: 8 [ 450/2354 ( 19%)]  Loss: 2.450 (3.14)  Time: 0.470s,   17.02/s  (0.473s,   16.90/s)  LR: 9.758e-05  Data: 0.009 (0.009)\n",
            "Train: 8 [ 500/2354 ( 21%)]  Loss: 2.111 (3.14)  Time: 0.465s,   17.21/s  (0.472s,   16.93/s)  LR: 9.758e-05  Data: 0.007 (0.009)\n",
            "Train: 8 [ 550/2354 ( 23%)]  Loss: 3.617 (3.14)  Time: 0.469s,   17.08/s  (0.472s,   16.96/s)  LR: 9.758e-05  Data: 0.007 (0.009)\n",
            "Train: 8 [ 600/2354 ( 25%)]  Loss: 3.167 (3.15)  Time: 0.464s,   17.24/s  (0.471s,   16.98/s)  LR: 9.758e-05  Data: 0.009 (0.009)\n",
            "Train: 8 [ 650/2354 ( 28%)]  Loss: 2.564 (3.14)  Time: 0.462s,   17.30/s  (0.471s,   17.00/s)  LR: 9.758e-05  Data: 0.007 (0.009)\n",
            "Train: 8 [ 700/2354 ( 30%)]  Loss: 2.685 (3.15)  Time: 0.461s,   17.36/s  (0.471s,   16.97/s)  LR: 9.758e-05  Data: 0.007 (0.009)\n",
            "Train: 8 [ 750/2354 ( 32%)]  Loss: 3.883 (3.15)  Time: 0.463s,   17.28/s  (0.471s,   16.99/s)  LR: 9.758e-05  Data: 0.008 (0.009)\n",
            "Train: 8 [ 800/2354 ( 34%)]  Loss: 2.959 (3.15)  Time: 0.459s,   17.45/s  (0.470s,   17.00/s)  LR: 9.758e-05  Data: 0.007 (0.009)\n",
            "Train: 8 [ 850/2354 ( 36%)]  Loss: 2.249 (3.15)  Time: 0.459s,   17.43/s  (0.470s,   17.02/s)  LR: 9.758e-05  Data: 0.007 (0.009)\n",
            "Train: 8 [ 900/2354 ( 38%)]  Loss: 3.301 (3.16)  Time: 0.460s,   17.41/s  (0.470s,   17.03/s)  LR: 9.758e-05  Data: 0.006 (0.009)\n",
            "Train: 8 [ 950/2354 ( 40%)]  Loss: 3.431 (3.16)  Time: 0.466s,   17.18/s  (0.470s,   17.01/s)  LR: 9.758e-05  Data: 0.009 (0.009)\n",
            "Train: 8 [1000/2354 ( 42%)]  Loss: 2.143 (3.15)  Time: 0.471s,   17.00/s  (0.470s,   17.02/s)  LR: 9.758e-05  Data: 0.009 (0.009)\n",
            "Train: 8 [1050/2354 ( 45%)]  Loss: 2.975 (3.16)  Time: 0.461s,   17.37/s  (0.470s,   17.03/s)  LR: 9.758e-05  Data: 0.007 (0.009)\n",
            "Train: 8 [1100/2354 ( 47%)]  Loss: 3.601 (3.16)  Time: 0.463s,   17.30/s  (0.470s,   17.04/s)  LR: 9.758e-05  Data: 0.010 (0.008)\n",
            "Train: 8 [1150/2354 ( 49%)]  Loss: 3.946 (3.16)  Time: 0.459s,   17.42/s  (0.469s,   17.05/s)  LR: 9.758e-05  Data: 0.007 (0.008)\n",
            "Train: 8 [1200/2354 ( 51%)]  Loss: 2.285 (3.16)  Time: 0.461s,   17.37/s  (0.470s,   17.03/s)  LR: 9.758e-05  Data: 0.007 (0.008)\n",
            "Train: 8 [1250/2354 ( 53%)]  Loss: 2.900 (3.16)  Time: 0.461s,   17.36/s  (0.470s,   17.04/s)  LR: 9.758e-05  Data: 0.008 (0.008)\n",
            "Train: 8 [1300/2354 ( 55%)]  Loss: 2.380 (3.17)  Time: 0.465s,   17.20/s  (0.469s,   17.04/s)  LR: 9.758e-05  Data: 0.010 (0.008)\n",
            "Train: 8 [1350/2354 ( 57%)]  Loss: 3.886 (3.16)  Time: 0.459s,   17.42/s  (0.469s,   17.05/s)  LR: 9.758e-05  Data: 0.006 (0.008)\n",
            "Train: 8 [1400/2354 ( 59%)]  Loss: 4.376 (3.17)  Time: 0.464s,   17.25/s  (0.469s,   17.06/s)  LR: 9.758e-05  Data: 0.007 (0.008)\n",
            "Train: 8 [1450/2354 ( 62%)]  Loss: 2.755 (3.16)  Time: 0.460s,   17.40/s  (0.469s,   17.04/s)  LR: 9.758e-05  Data: 0.007 (0.008)\n",
            "Train: 8 [1500/2354 ( 64%)]  Loss: 2.442 (3.16)  Time: 0.468s,   17.11/s  (0.469s,   17.05/s)  LR: 9.758e-05  Data: 0.006 (0.008)\n",
            "Train: 8 [1550/2354 ( 66%)]  Loss: 4.118 (3.15)  Time: 0.469s,   17.05/s  (0.469s,   17.05/s)  LR: 9.758e-05  Data: 0.010 (0.008)\n",
            "Train: 8 [1600/2354 ( 68%)]  Loss: 2.635 (3.15)  Time: 0.468s,   17.08/s  (0.469s,   17.06/s)  LR: 9.758e-05  Data: 0.009 (0.008)\n",
            "Train: 8 [1650/2354 ( 70%)]  Loss: 3.232 (3.15)  Time: 0.463s,   17.27/s  (0.469s,   17.06/s)  LR: 9.758e-05  Data: 0.007 (0.008)\n",
            "Train: 8 [1700/2354 ( 72%)]  Loss: 3.187 (3.15)  Time: 0.479s,   16.72/s  (0.469s,   17.04/s)  LR: 9.758e-05  Data: 0.009 (0.008)\n",
            "Train: 8 [1750/2354 ( 74%)]  Loss: 2.115 (3.15)  Time: 0.469s,   17.05/s  (0.469s,   17.05/s)  LR: 9.758e-05  Data: 0.006 (0.008)\n",
            "Train: 8 [1800/2354 ( 76%)]  Loss: 3.333 (3.14)  Time: 0.464s,   17.26/s  (0.469s,   17.05/s)  LR: 9.758e-05  Data: 0.007 (0.008)\n",
            "Train: 8 [1850/2354 ( 79%)]  Loss: 3.342 (3.14)  Time: 0.459s,   17.42/s  (0.469s,   17.06/s)  LR: 9.758e-05  Data: 0.008 (0.008)\n",
            "Train: 8 [1900/2354 ( 81%)]  Loss: 3.606 (3.14)  Time: 0.608s,   13.17/s  (0.469s,   17.05/s)  LR: 9.758e-05  Data: 0.016 (0.008)\n",
            "Train: 8 [1950/2354 ( 83%)]  Loss: 3.107 (3.14)  Time: 0.462s,   17.33/s  (0.469s,   17.05/s)  LR: 9.758e-05  Data: 0.006 (0.008)\n",
            "Train: 8 [2000/2354 ( 85%)]  Loss: 3.722 (3.14)  Time: 0.459s,   17.42/s  (0.469s,   17.05/s)  LR: 9.758e-05  Data: 0.007 (0.008)\n",
            "Train: 8 [2050/2354 ( 87%)]  Loss: 3.829 (3.14)  Time: 0.464s,   17.26/s  (0.469s,   17.05/s)  LR: 9.758e-05  Data: 0.006 (0.008)\n",
            "Train: 8 [2100/2354 ( 89%)]  Loss: 3.675 (3.14)  Time: 0.463s,   17.28/s  (0.469s,   17.06/s)  LR: 9.758e-05  Data: 0.006 (0.008)\n",
            "Train: 8 [2150/2354 ( 91%)]  Loss: 4.143 (3.14)  Time: 0.464s,   17.24/s  (0.469s,   17.04/s)  LR: 9.758e-05  Data: 0.007 (0.008)\n",
            "Train: 8 [2200/2354 ( 93%)]  Loss: 3.175 (3.14)  Time: 0.459s,   17.43/s  (0.469s,   17.05/s)  LR: 9.758e-05  Data: 0.007 (0.008)\n",
            "Train: 8 [2250/2354 ( 96%)]  Loss: 2.884 (3.14)  Time: 0.460s,   17.38/s  (0.469s,   17.05/s)  LR: 9.758e-05  Data: 0.007 (0.008)\n",
            "Train: 8 [2300/2354 ( 98%)]  Loss: 4.158 (3.14)  Time: 0.463s,   17.29/s  (0.469s,   17.06/s)  LR: 9.758e-05  Data: 0.007 (0.008)\n",
            "Train: 8 [2350/2354 (100%)]  Loss: 2.793 (3.14)  Time: 0.455s,   17.59/s  (0.469s,   17.06/s)  LR: 9.758e-05  Data: 0.005 (0.008)\n",
            "Train: 8 [2353/2354 (100%)]  Loss: 3.661 (3.14)  Time: 0.453s,   17.68/s  (0.469s,   17.06/s)  LR: 9.758e-05  Data: 0.000 (0.008)\n",
            "Test: [   0/2354]  Time: 0.623 (0.623)  Loss:  2.3652 (2.3652)  Acc@1:  0.0000 ( 0.0000)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [  50/2354]  Time: 0.148 (0.147)  Loss:  4.4570 (1.6827)  Acc@1:  0.0000 (68.1373)  Acc@5: 25.0000 (86.2745)\n",
            "Test: [ 100/2354]  Time: 0.187 (0.162)  Loss:  3.4922 (1.8955)  Acc@1: 25.0000 (63.3663)  Acc@5: 75.0000 (86.3861)\n",
            "Test: [ 150/2354]  Time: 0.207 (0.152)  Loss:  1.4922 (1.7020)  Acc@1: 75.0000 (65.2318)  Acc@5: 75.0000 (88.9073)\n",
            "Test: [ 200/2354]  Time: 0.089 (0.148)  Loss:  3.8340 (1.6658)  Acc@1:  0.0000 (64.8010)  Acc@5: 25.0000 (89.5522)\n",
            "Test: [ 250/2354]  Time: 0.137 (0.146)  Loss:  0.7700 (1.7859)  Acc@1: 100.0000 (61.7530)  Acc@5: 100.0000 (86.9522)\n",
            "Test: [ 300/2354]  Time: 0.175 (0.144)  Loss:  1.3867 (1.7462)  Acc@1: 50.0000 (63.2060)  Acc@5: 100.0000 (87.8738)\n",
            "Test: [ 350/2354]  Time: 0.096 (0.143)  Loss:  1.5469 (1.6780)  Acc@1: 75.0000 (62.5356)  Acc@5: 100.0000 (89.3162)\n",
            "Test: [ 400/2354]  Time: 0.208 (0.142)  Loss:  0.9570 (1.6299)  Acc@1: 75.0000 (63.0299)  Acc@5: 100.0000 (89.8379)\n",
            "Test: [ 450/2354]  Time: 0.153 (0.141)  Loss:  0.2681 (1.5766)  Acc@1: 100.0000 (64.2461)  Acc@5: 100.0000 (90.1885)\n",
            "Test: [ 500/2354]  Time: 0.184 (0.140)  Loss:  1.9258 (1.5343)  Acc@1: 50.0000 (65.2196)  Acc@5: 100.0000 (90.5190)\n",
            "Test: [ 550/2354]  Time: 0.144 (0.139)  Loss:  3.4062 (1.5423)  Acc@1:  0.0000 (64.2015)  Acc@5: 75.0000 (90.5626)\n",
            "Test: [ 600/2354]  Time: 0.178 (0.139)  Loss:  1.6055 (1.5535)  Acc@1: 75.0000 (63.4775)  Acc@5: 100.0000 (90.8070)\n",
            "Test: [ 650/2354]  Time: 0.102 (0.139)  Loss:  0.8794 (1.5299)  Acc@1: 75.0000 (64.1321)  Acc@5: 100.0000 (91.0522)\n",
            "Test: [ 700/2354]  Time: 0.175 (0.139)  Loss:  0.2810 (1.5132)  Acc@1: 100.0000 (65.1213)  Acc@5: 100.0000 (91.1198)\n",
            "Test: [ 750/2354]  Time: 0.163 (0.138)  Loss:  2.4023 (1.5218)  Acc@1: 50.0000 (64.8136)  Acc@5: 75.0000 (91.1784)\n",
            "Test: [ 800/2354]  Time: 0.176 (0.138)  Loss:  0.9600 (1.5329)  Acc@1: 75.0000 (64.4507)  Acc@5: 100.0000 (90.8864)\n",
            "Test: [ 850/2354]  Time: 0.162 (0.138)  Loss:  0.2159 (1.5356)  Acc@1: 100.0000 (64.3067)  Acc@5: 100.0000 (90.7756)\n",
            "Test: [ 900/2354]  Time: 0.235 (0.138)  Loss:  1.3975 (1.5239)  Acc@1: 75.0000 (64.5117)  Acc@5: 75.0000 (91.0655)\n",
            "Test: [ 950/2354]  Time: 0.125 (0.140)  Loss:  1.2109 (1.4989)  Acc@1: 100.0000 (65.1682)  Acc@5: 100.0000 (91.3512)\n",
            "Test: [1000/2354]  Time: 0.150 (0.139)  Loss:  0.6318 (1.4911)  Acc@1: 75.0000 (65.5095)  Acc@5: 100.0000 (91.4086)\n",
            "Test: [1050/2354]  Time: 0.157 (0.139)  Loss:  0.7832 (1.4854)  Acc@1: 75.0000 (65.6755)  Acc@5: 100.0000 (91.4605)\n",
            "Test: [1100/2354]  Time: 0.076 (0.139)  Loss:  1.7764 (1.4822)  Acc@1: 50.0000 (65.8265)  Acc@5: 100.0000 (91.5531)\n",
            "Test: [1150/2354]  Time: 0.165 (0.139)  Loss:  1.6807 (1.4692)  Acc@1: 75.0000 (66.3119)  Acc@5: 100.0000 (91.6377)\n",
            "Test: [1200/2354]  Time: 0.083 (0.138)  Loss:  1.4668 (1.4598)  Acc@1: 75.0000 (66.5695)  Acc@5: 75.0000 (91.6528)\n",
            "Test: [1250/2354]  Time: 0.169 (0.138)  Loss:  0.4370 (1.4439)  Acc@1: 100.0000 (67.1063)  Acc@5: 100.0000 (91.6667)\n",
            "Test: [1300/2354]  Time: 0.076 (0.138)  Loss:  1.2139 (1.4372)  Acc@1: 100.0000 (67.1983)  Acc@5: 100.0000 (91.7756)\n",
            "Test: [1350/2354]  Time: 0.082 (0.138)  Loss:  0.6055 (1.4556)  Acc@1: 100.0000 (66.2472)  Acc@5: 100.0000 (91.6173)\n",
            "Test: [1400/2354]  Time: 0.126 (0.138)  Loss:  0.4753 (1.4485)  Acc@1: 100.0000 (66.4168)  Acc@5: 100.0000 (91.5774)\n",
            "Test: [1450/2354]  Time: 0.076 (0.138)  Loss:  2.0312 (1.4316)  Acc@1: 25.0000 (66.4542)  Acc@5: 100.0000 (91.7815)\n",
            "Test: [1500/2354]  Time: 0.126 (0.138)  Loss:  1.5859 (1.4342)  Acc@1: 75.0000 (66.2558)  Acc@5: 100.0000 (91.8221)\n",
            "Test: [1550/2354]  Time: 0.208 (0.138)  Loss:  0.5498 (1.4530)  Acc@1: 100.0000 (65.6190)  Acc@5: 100.0000 (91.5377)\n",
            "Test: [1600/2354]  Time: 0.110 (0.137)  Loss:  1.0742 (1.4564)  Acc@1: 100.0000 (65.6152)  Acc@5: 100.0000 (91.5834)\n",
            "Test: [1650/2354]  Time: 0.192 (0.137)  Loss:  1.0898 (1.4600)  Acc@1: 75.0000 (65.5360)  Acc@5: 100.0000 (91.4597)\n",
            "Test: [1700/2354]  Time: 0.111 (0.137)  Loss:  0.7505 (1.4511)  Acc@1: 100.0000 (65.8877)  Acc@5: 100.0000 (91.5932)\n",
            "Test: [1750/2354]  Time: 0.129 (0.138)  Loss:  2.9062 (1.4566)  Acc@1:  0.0000 (65.5340)  Acc@5: 50.0000 (91.5762)\n",
            "Test: [1800/2354]  Time: 0.153 (0.138)  Loss:  1.9297 (1.4611)  Acc@1: 75.0000 (65.2415)  Acc@5: 75.0000 (91.6158)\n",
            "Test: [1850/2354]  Time: 0.153 (0.138)  Loss:  0.5278 (1.4537)  Acc@1: 100.0000 (65.3160)  Acc@5: 100.0000 (91.7072)\n",
            "Test: [1900/2354]  Time: 0.124 (0.138)  Loss:  1.7412 (1.4534)  Acc@1: 75.0000 (65.3866)  Acc@5: 75.0000 (91.6754)\n",
            "Test: [1950/2354]  Time: 0.150 (0.138)  Loss:  2.2559 (1.4590)  Acc@1: 50.0000 (65.3511)  Acc@5: 75.0000 (91.6325)\n",
            "Test: [2000/2354]  Time: 0.152 (0.138)  Loss:  1.7119 (1.4595)  Acc@1: 75.0000 (65.4173)  Acc@5: 100.0000 (91.6167)\n",
            "Test: [2050/2354]  Time: 0.140 (0.138)  Loss:  3.3828 (1.4725)  Acc@1:  0.0000 (64.9196)  Acc@5: 25.0000 (91.4188)\n",
            "Test: [2100/2354]  Time: 0.191 (0.138)  Loss:  0.9390 (1.4789)  Acc@1: 100.0000 (64.6121)  Acc@5: 100.0000 (91.4564)\n",
            "Test: [2150/2354]  Time: 0.163 (0.137)  Loss:  2.3105 (1.4925)  Acc@1: 25.0000 (64.3422)  Acc@5: 75.0000 (91.4226)\n",
            "Test: [2200/2354]  Time: 0.174 (0.137)  Loss:  2.1543 (1.4984)  Acc@1: 50.0000 (64.1981)  Acc@5: 100.0000 (91.4357)\n",
            "Test: [2250/2354]  Time: 0.202 (0.137)  Loss:  1.2480 (1.4974)  Acc@1: 75.0000 (64.3159)  Acc@5: 100.0000 (91.3927)\n",
            "Test: [2300/2354]  Time: 0.157 (0.137)  Loss:  0.6816 (1.4995)  Acc@1: 75.0000 (64.3742)  Acc@5: 100.0000 (91.3842)\n",
            "Test: [2350/2354]  Time: 0.074 (0.137)  Loss:  3.3457 (1.5038)  Acc@1:  0.0000 (64.2493)  Acc@5: 75.0000 (91.3016)\n",
            "Test: [2354/2354]  Time: 0.064 (0.137)  Loss:  1.3477 (1.5043)  Acc@1: 100.0000 (64.2319)  Acc@5: 100.0000 (91.3154)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-8.pth.tar', 64.23187174859326)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-7.pth.tar', 58.424461196265995)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-6.pth.tar', 53.67873447368398)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-5.pth.tar', 44.633188236543155)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-4.pth.tar', 39.08058180273915)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-3.pth.tar', 23.29334324238242)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-2.pth.tar', 16.477333050217645)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-1.pth.tar', 6.009130480942775)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-0.pth.tar', 1.295254273277418)\n",
            "\n",
            "Train: 9 [   0/2354 (  0%)]  Loss: 4.666 (4.67)  Time: 1.286s,    6.22/s  (1.286s,    6.22/s)  LR: 9.694e-05  Data: 0.519 (0.519)\n",
            "Train: 9 [  50/2354 (  2%)]  Loss: 3.096 (2.96)  Time: 0.468s,   17.09/s  (0.518s,   15.43/s)  LR: 9.694e-05  Data: 0.007 (0.018)\n",
            "Train: 9 [ 100/2354 (  4%)]  Loss: 3.182 (2.92)  Time: 0.466s,   17.18/s  (0.492s,   16.25/s)  LR: 9.694e-05  Data: 0.007 (0.013)\n",
            "Train: 9 [ 150/2354 (  6%)]  Loss: 2.220 (2.92)  Time: 0.468s,   17.09/s  (0.483s,   16.56/s)  LR: 9.694e-05  Data: 0.009 (0.011)\n",
            "Train: 9 [ 200/2354 (  8%)]  Loss: 3.242 (2.97)  Time: 0.465s,   17.20/s  (0.478s,   16.72/s)  LR: 9.694e-05  Data: 0.006 (0.010)\n",
            "Train: 9 [ 250/2354 ( 11%)]  Loss: 2.949 (2.99)  Time: 0.480s,   16.65/s  (0.475s,   16.83/s)  LR: 9.694e-05  Data: 0.012 (0.010)\n",
            "Train: 9 [ 300/2354 ( 13%)]  Loss: 2.883 (2.99)  Time: 0.468s,   17.10/s  (0.476s,   16.79/s)  LR: 9.694e-05  Data: 0.009 (0.010)\n",
            "Train: 9 [ 350/2354 ( 15%)]  Loss: 2.048 (2.97)  Time: 0.462s,   17.32/s  (0.475s,   16.86/s)  LR: 9.694e-05  Data: 0.006 (0.010)\n",
            "Train: 9 [ 400/2354 ( 17%)]  Loss: 3.089 (2.96)  Time: 0.462s,   17.33/s  (0.473s,   16.90/s)  LR: 9.694e-05  Data: 0.007 (0.009)\n",
            "Train: 9 [ 450/2354 ( 19%)]  Loss: 3.794 (2.95)  Time: 0.460s,   17.38/s  (0.472s,   16.94/s)  LR: 9.694e-05  Data: 0.007 (0.009)\n",
            "Train: 9 [ 500/2354 ( 21%)]  Loss: 2.489 (2.96)  Time: 0.464s,   17.26/s  (0.472s,   16.97/s)  LR: 9.694e-05  Data: 0.007 (0.009)\n",
            "Train: 9 [ 550/2354 ( 23%)]  Loss: 4.920 (2.96)  Time: 0.468s,   17.08/s  (0.472s,   16.94/s)  LR: 9.694e-05  Data: 0.007 (0.009)\n",
            "Train: 9 [ 600/2354 ( 25%)]  Loss: 3.568 (2.96)  Time: 0.474s,   16.89/s  (0.472s,   16.96/s)  LR: 9.694e-05  Data: 0.009 (0.009)\n",
            "Train: 9 [ 650/2354 ( 28%)]  Loss: 2.971 (2.98)  Time: 0.465s,   17.19/s  (0.471s,   16.98/s)  LR: 9.694e-05  Data: 0.009 (0.009)\n",
            "Train: 9 [ 700/2354 ( 30%)]  Loss: 3.260 (2.98)  Time: 0.463s,   17.26/s  (0.471s,   17.00/s)  LR: 9.694e-05  Data: 0.008 (0.009)\n",
            "Train: 9 [ 750/2354 ( 32%)]  Loss: 3.025 (2.99)  Time: 0.463s,   17.27/s  (0.470s,   17.02/s)  LR: 9.694e-05  Data: 0.007 (0.009)\n",
            "Train: 9 [ 800/2354 ( 34%)]  Loss: 3.074 (2.98)  Time: 0.463s,   17.27/s  (0.471s,   16.99/s)  LR: 9.694e-05  Data: 0.008 (0.009)\n",
            "Train: 9 [ 850/2354 ( 36%)]  Loss: 3.274 (2.98)  Time: 0.461s,   17.34/s  (0.470s,   17.01/s)  LR: 9.694e-05  Data: 0.007 (0.008)\n",
            "Train: 9 [ 900/2354 ( 38%)]  Loss: 3.122 (2.98)  Time: 0.464s,   17.23/s  (0.470s,   17.02/s)  LR: 9.694e-05  Data: 0.009 (0.008)\n",
            "Train: 9 [ 950/2354 ( 40%)]  Loss: 2.617 (2.98)  Time: 0.465s,   17.22/s  (0.470s,   17.03/s)  LR: 9.694e-05  Data: 0.010 (0.008)\n",
            "Train: 9 [1000/2354 ( 42%)]  Loss: 2.326 (2.98)  Time: 0.620s,   12.91/s  (0.470s,   17.02/s)  LR: 9.694e-05  Data: 0.019 (0.008)\n",
            "Train: 9 [1050/2354 ( 45%)]  Loss: 3.515 (2.98)  Time: 0.461s,   17.35/s  (0.470s,   17.02/s)  LR: 9.694e-05  Data: 0.010 (0.008)\n",
            "Train: 9 [1100/2354 ( 47%)]  Loss: 2.624 (2.98)  Time: 0.470s,   17.01/s  (0.470s,   17.03/s)  LR: 9.694e-05  Data: 0.007 (0.008)\n",
            "Train: 9 [1150/2354 ( 49%)]  Loss: 3.553 (2.98)  Time: 0.465s,   17.20/s  (0.470s,   17.04/s)  LR: 9.694e-05  Data: 0.009 (0.008)\n",
            "Train: 9 [1200/2354 ( 51%)]  Loss: 4.569 (2.98)  Time: 0.466s,   17.15/s  (0.469s,   17.05/s)  LR: 9.694e-05  Data: 0.007 (0.008)\n",
            "Train: 9 [1250/2354 ( 53%)]  Loss: 2.660 (2.97)  Time: 0.465s,   17.20/s  (0.470s,   17.03/s)  LR: 9.694e-05  Data: 0.007 (0.008)\n",
            "Train: 9 [1300/2354 ( 55%)]  Loss: 2.897 (2.97)  Time: 0.461s,   17.35/s  (0.470s,   17.04/s)  LR: 9.694e-05  Data: 0.006 (0.008)\n",
            "Train: 9 [1350/2354 ( 57%)]  Loss: 2.271 (2.97)  Time: 0.463s,   17.29/s  (0.469s,   17.04/s)  LR: 9.694e-05  Data: 0.010 (0.008)\n",
            "Train: 9 [1400/2354 ( 59%)]  Loss: 1.958 (2.97)  Time: 0.462s,   17.31/s  (0.469s,   17.05/s)  LR: 9.694e-05  Data: 0.010 (0.008)\n",
            "Train: 9 [1450/2354 ( 62%)]  Loss: 3.928 (2.98)  Time: 0.463s,   17.27/s  (0.469s,   17.06/s)  LR: 9.694e-05  Data: 0.006 (0.008)\n",
            "Train: 9 [1500/2354 ( 64%)]  Loss: 4.147 (2.98)  Time: 0.473s,   16.92/s  (0.469s,   17.04/s)  LR: 9.694e-05  Data: 0.009 (0.008)\n",
            "Train: 9 [1550/2354 ( 66%)]  Loss: 2.607 (2.98)  Time: 0.467s,   17.12/s  (0.469s,   17.05/s)  LR: 9.694e-05  Data: 0.007 (0.008)\n",
            "Train: 9 [1600/2354 ( 68%)]  Loss: 3.174 (2.98)  Time: 0.466s,   17.15/s  (0.469s,   17.05/s)  LR: 9.694e-05  Data: 0.008 (0.008)\n",
            "Train: 9 [1650/2354 ( 70%)]  Loss: 3.114 (2.98)  Time: 0.461s,   17.35/s  (0.469s,   17.06/s)  LR: 9.694e-05  Data: 0.006 (0.008)\n",
            "Train: 9 [1700/2354 ( 72%)]  Loss: 3.044 (2.98)  Time: 0.462s,   17.33/s  (0.469s,   17.06/s)  LR: 9.694e-05  Data: 0.008 (0.008)\n",
            "Train: 9 [1750/2354 ( 74%)]  Loss: 3.834 (2.98)  Time: 0.461s,   17.34/s  (0.469s,   17.04/s)  LR: 9.694e-05  Data: 0.007 (0.008)\n",
            "Train: 9 [1800/2354 ( 76%)]  Loss: 3.845 (2.97)  Time: 0.461s,   17.35/s  (0.469s,   17.05/s)  LR: 9.694e-05  Data: 0.007 (0.008)\n",
            "Train: 9 [1850/2354 ( 79%)]  Loss: 3.512 (2.98)  Time: 0.468s,   17.09/s  (0.469s,   17.05/s)  LR: 9.694e-05  Data: 0.007 (0.008)\n",
            "Train: 9 [1900/2354 ( 81%)]  Loss: 2.538 (2.98)  Time: 0.463s,   17.29/s  (0.469s,   17.06/s)  LR: 9.694e-05  Data: 0.007 (0.008)\n",
            "Train: 9 [1950/2354 ( 83%)]  Loss: 2.341 (2.97)  Time: 0.475s,   16.84/s  (0.469s,   17.05/s)  LR: 9.694e-05  Data: 0.007 (0.008)\n",
            "Train: 9 [2000/2354 ( 85%)]  Loss: 3.582 (2.98)  Time: 0.461s,   17.35/s  (0.469s,   17.05/s)  LR: 9.694e-05  Data: 0.006 (0.008)\n",
            "Train: 9 [2050/2354 ( 87%)]  Loss: 4.379 (2.97)  Time: 0.460s,   17.38/s  (0.469s,   17.05/s)  LR: 9.694e-05  Data: 0.007 (0.008)\n",
            "Train: 9 [2100/2354 ( 89%)]  Loss: 3.432 (2.97)  Time: 0.460s,   17.39/s  (0.469s,   17.06/s)  LR: 9.694e-05  Data: 0.008 (0.008)\n",
            "Train: 9 [2150/2354 ( 91%)]  Loss: 2.835 (2.98)  Time: 0.462s,   17.32/s  (0.469s,   17.06/s)  LR: 9.694e-05  Data: 0.007 (0.008)\n",
            "Train: 9 [2200/2354 ( 93%)]  Loss: 2.672 (2.98)  Time: 0.464s,   17.24/s  (0.469s,   17.05/s)  LR: 9.694e-05  Data: 0.007 (0.008)\n",
            "Train: 9 [2250/2354 ( 96%)]  Loss: 3.683 (2.98)  Time: 0.462s,   17.30/s  (0.469s,   17.06/s)  LR: 9.694e-05  Data: 0.007 (0.008)\n",
            "Train: 9 [2300/2354 ( 98%)]  Loss: 2.750 (2.98)  Time: 0.468s,   17.11/s  (0.469s,   17.06/s)  LR: 9.694e-05  Data: 0.009 (0.008)\n",
            "Train: 9 [2350/2354 (100%)]  Loss: 2.101 (2.98)  Time: 0.458s,   17.46/s  (0.469s,   17.06/s)  LR: 9.694e-05  Data: 0.005 (0.008)\n",
            "Train: 9 [2353/2354 (100%)]  Loss: 2.661 (2.98)  Time: 0.452s,   17.70/s  (0.469s,   17.07/s)  LR: 9.694e-05  Data: 0.000 (0.008)\n",
            "Test: [   0/2354]  Time: 0.613 (0.613)  Loss:  2.2324 (2.2324)  Acc@1: 50.0000 (50.0000)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [  50/2354]  Time: 0.145 (0.148)  Loss:  4.6094 (1.4125)  Acc@1:  0.0000 (68.1373)  Acc@5: 25.0000 (88.7255)\n",
            "Test: [ 100/2354]  Time: 0.126 (0.140)  Loss:  4.5273 (1.7181)  Acc@1:  0.0000 (59.9010)  Acc@5: 25.0000 (86.8812)\n",
            "Test: [ 150/2354]  Time: 0.083 (0.139)  Loss:  0.8838 (1.5440)  Acc@1: 75.0000 (63.2450)  Acc@5: 100.0000 (89.9007)\n",
            "Test: [ 200/2354]  Time: 0.101 (0.147)  Loss:  2.7402 (1.4389)  Acc@1: 25.0000 (65.7960)  Acc@5: 100.0000 (90.9204)\n",
            "Test: [ 250/2354]  Time: 0.169 (0.145)  Loss:  0.1719 (1.5604)  Acc@1: 100.0000 (62.2510)  Acc@5: 100.0000 (89.2430)\n",
            "Test: [ 300/2354]  Time: 0.082 (0.143)  Loss:  0.8833 (1.5296)  Acc@1: 100.0000 (64.2857)  Acc@5: 100.0000 (89.3688)\n",
            "Test: [ 350/2354]  Time: 0.146 (0.142)  Loss:  1.8115 (1.5036)  Acc@1: 50.0000 (63.8889)  Acc@5: 100.0000 (90.2422)\n",
            "Test: [ 400/2354]  Time: 0.158 (0.141)  Loss:  1.1582 (1.4847)  Acc@1: 75.0000 (64.5262)  Acc@5: 100.0000 (90.4613)\n",
            "Test: [ 450/2354]  Time: 0.158 (0.140)  Loss:  0.6401 (1.4728)  Acc@1: 100.0000 (64.5787)  Acc@5: 100.0000 (90.3548)\n",
            "Test: [ 500/2354]  Time: 0.108 (0.139)  Loss:  1.3057 (1.4193)  Acc@1: 25.0000 (65.4192)  Acc@5: 100.0000 (91.0180)\n",
            "Test: [ 550/2354]  Time: 0.170 (0.139)  Loss:  2.1641 (1.4179)  Acc@1: 50.0000 (65.0635)  Acc@5: 75.0000 (91.0163)\n",
            "Test: [ 600/2354]  Time: 0.097 (0.139)  Loss:  1.3945 (1.3995)  Acc@1: 75.0000 (66.3894)  Acc@5: 100.0000 (91.5141)\n",
            "Test: [ 650/2354]  Time: 0.106 (0.138)  Loss:  0.3840 (1.3657)  Acc@1: 100.0000 (67.3579)  Acc@5: 100.0000 (91.7051)\n",
            "Test: [ 700/2354]  Time: 0.111 (0.138)  Loss:  0.2073 (1.3378)  Acc@1: 100.0000 (68.2953)  Acc@5: 100.0000 (91.9757)\n",
            "Test: [ 750/2354]  Time: 0.147 (0.138)  Loss:  1.6260 (1.3305)  Acc@1: 75.0000 (68.4088)  Acc@5: 75.0000 (92.0772)\n",
            "Test: [ 800/2354]  Time: 0.076 (0.137)  Loss:  0.2603 (1.3384)  Acc@1: 100.0000 (68.1960)  Acc@5: 100.0000 (91.9476)\n",
            "Test: [ 850/2354]  Time: 0.123 (0.137)  Loss:  0.1322 (1.3284)  Acc@1: 100.0000 (68.7133)  Acc@5: 100.0000 (91.8919)\n",
            "Test: [ 900/2354]  Time: 0.146 (0.137)  Loss:  1.1494 (1.2992)  Acc@1: 75.0000 (69.7558)  Acc@5: 75.0000 (92.2863)\n",
            "Test: [ 950/2354]  Time: 0.119 (0.137)  Loss:  0.7656 (1.2708)  Acc@1: 100.0000 (70.7413)  Acc@5: 100.0000 (92.5868)\n",
            "Test: [1000/2354]  Time: 0.173 (0.139)  Loss:  0.7271 (1.2670)  Acc@1: 100.0000 (70.7792)  Acc@5: 100.0000 (92.7323)\n",
            "Test: [1050/2354]  Time: 0.130 (0.138)  Loss:  1.3418 (1.2662)  Acc@1: 75.0000 (70.9324)  Acc@5: 100.0000 (92.8164)\n",
            "Test: [1100/2354]  Time: 0.155 (0.138)  Loss:  1.9375 (1.2829)  Acc@1: 50.0000 (70.1635)  Acc@5: 100.0000 (92.7793)\n",
            "Test: [1150/2354]  Time: 0.125 (0.138)  Loss:  2.3242 (1.2637)  Acc@1: 100.0000 (70.7211)  Acc@5: 100.0000 (92.8975)\n",
            "Test: [1200/2354]  Time: 0.124 (0.138)  Loss:  1.3691 (1.2622)  Acc@1: 75.0000 (70.6911)  Acc@5: 75.0000 (92.8601)\n",
            "Test: [1250/2354]  Time: 0.130 (0.138)  Loss:  0.1851 (1.2501)  Acc@1: 100.0000 (71.1231)  Acc@5: 100.0000 (92.8257)\n",
            "Test: [1300/2354]  Time: 0.141 (0.138)  Loss:  1.0615 (1.2374)  Acc@1: 75.0000 (71.3874)  Acc@5: 100.0000 (92.9093)\n",
            "Test: [1350/2354]  Time: 0.163 (0.137)  Loss:  0.6348 (1.2565)  Acc@1: 75.0000 (70.6329)  Acc@5: 100.0000 (92.8201)\n",
            "Test: [1400/2354]  Time: 0.116 (0.137)  Loss:  1.1445 (1.2581)  Acc@1: 25.0000 (70.5389)  Acc@5: 100.0000 (92.8979)\n",
            "Test: [1450/2354]  Time: 0.077 (0.137)  Loss:  1.5840 (1.2448)  Acc@1: 25.0000 (70.5720)  Acc@5: 100.0000 (93.0910)\n",
            "Test: [1500/2354]  Time: 0.132 (0.137)  Loss:  0.9507 (1.2579)  Acc@1: 100.0000 (70.3198)  Acc@5: 100.0000 (92.9547)\n",
            "Test: [1550/2354]  Time: 0.162 (0.137)  Loss:  0.6318 (1.2724)  Acc@1: 100.0000 (69.6003)  Acc@5: 100.0000 (92.7305)\n",
            "Test: [1600/2354]  Time: 0.189 (0.137)  Loss:  0.6348 (1.2755)  Acc@1: 100.0000 (69.4722)  Acc@5: 100.0000 (92.6921)\n",
            "Test: [1650/2354]  Time: 0.141 (0.137)  Loss:  1.1387 (1.2807)  Acc@1: 75.0000 (69.0491)  Acc@5: 100.0000 (92.6257)\n",
            "Test: [1700/2354]  Time: 0.124 (0.137)  Loss:  0.2441 (1.2688)  Acc@1: 100.0000 (69.3857)  Acc@5: 100.0000 (92.6955)\n",
            "Test: [1750/2354]  Time: 0.093 (0.137)  Loss:  1.6250 (1.2804)  Acc@1: 75.0000 (68.9463)  Acc@5: 100.0000 (92.6899)\n",
            "Test: [1800/2354]  Time: 0.104 (0.138)  Loss:  1.1484 (1.2766)  Acc@1: 75.0000 (69.2810)  Acc@5: 100.0000 (92.7540)\n",
            "Test: [1850/2354]  Time: 0.150 (0.138)  Loss:  0.6797 (1.2684)  Acc@1: 100.0000 (69.4489)  Acc@5: 100.0000 (92.7877)\n",
            "Test: [1900/2354]  Time: 0.179 (0.137)  Loss:  1.5986 (1.2685)  Acc@1: 75.0000 (69.5818)  Acc@5: 75.0000 (92.7144)\n",
            "Test: [1950/2354]  Time: 0.157 (0.137)  Loss:  1.0840 (1.2697)  Acc@1: 75.0000 (69.7078)  Acc@5: 100.0000 (92.6961)\n",
            "Test: [2000/2354]  Time: 0.141 (0.137)  Loss:  0.7856 (1.2711)  Acc@1: 75.0000 (69.6527)  Acc@5: 100.0000 (92.7411)\n",
            "Test: [2050/2354]  Time: 0.143 (0.137)  Loss:  2.4043 (1.2783)  Acc@1: 25.0000 (69.2711)  Acc@5: 75.0000 (92.6255)\n",
            "Test: [2100/2354]  Time: 0.165 (0.137)  Loss:  1.3184 (1.2797)  Acc@1: 75.0000 (69.2646)  Acc@5: 100.0000 (92.5988)\n",
            "Test: [2150/2354]  Time: 0.179 (0.137)  Loss:  1.7900 (1.2923)  Acc@1: 25.0000 (68.7820)  Acc@5: 100.0000 (92.5151)\n",
            "Test: [2200/2354]  Time: 0.181 (0.137)  Loss:  2.8086 (1.2968)  Acc@1: 25.0000 (68.5143)  Acc@5: 75.0000 (92.5488)\n",
            "Test: [2250/2354]  Time: 0.099 (0.137)  Loss:  0.3074 (1.3016)  Acc@1: 100.0000 (68.4474)  Acc@5: 100.0000 (92.4922)\n",
            "Test: [2300/2354]  Time: 0.098 (0.137)  Loss:  0.3816 (1.3042)  Acc@1: 100.0000 (68.4702)  Acc@5: 100.0000 (92.3946)\n",
            "Test: [2350/2354]  Time: 0.075 (0.137)  Loss:  2.8711 (1.3078)  Acc@1: 25.0000 (68.3220)  Acc@5: 75.0000 (92.3650)\n",
            "Test: [2354/2354]  Time: 0.064 (0.137)  Loss:  0.9985 (1.3080)  Acc@1: 100.0000 (68.3406)  Acc@5: 100.0000 (92.3771)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-9.pth.tar', 68.34058817284213)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-8.pth.tar', 64.23187174859326)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-7.pth.tar', 58.424461196265995)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-6.pth.tar', 53.67873447368398)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-5.pth.tar', 44.633188236543155)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-4.pth.tar', 39.08058180273915)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-3.pth.tar', 23.29334324238242)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-2.pth.tar', 16.477333050217645)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-1.pth.tar', 6.009130480942775)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-0.pth.tar', 1.295254273277418)\n",
            "\n",
            "Train: 10 [   0/2354 (  0%)]  Loss: 2.629 (2.63)  Time: 1.255s,    6.37/s  (1.255s,    6.37/s)  LR: 9.623e-05  Data: 0.415 (0.415)\n",
            "Train: 10 [  50/2354 (  2%)]  Loss: 2.263 (2.87)  Time: 0.491s,   16.29/s  (0.497s,   16.11/s)  LR: 9.623e-05  Data: 0.010 (0.016)\n",
            "Train: 10 [ 100/2354 (  4%)]  Loss: 3.048 (2.83)  Time: 0.461s,   17.35/s  (0.489s,   16.36/s)  LR: 9.623e-05  Data: 0.007 (0.012)\n",
            "Train: 10 [ 150/2354 (  6%)]  Loss: 1.664 (2.85)  Time: 0.469s,   17.06/s  (0.481s,   16.64/s)  LR: 9.623e-05  Data: 0.009 (0.011)\n",
            "Train: 10 [ 200/2354 (  8%)]  Loss: 1.758 (2.84)  Time: 0.461s,   17.37/s  (0.477s,   16.78/s)  LR: 9.623e-05  Data: 0.009 (0.010)\n",
            "Train: 10 [ 250/2354 ( 11%)]  Loss: 3.755 (2.87)  Time: 0.463s,   17.29/s  (0.474s,   16.86/s)  LR: 9.623e-05  Data: 0.007 (0.010)\n",
            "Train: 10 [ 300/2354 ( 13%)]  Loss: 3.873 (2.84)  Time: 0.471s,   16.98/s  (0.476s,   16.82/s)  LR: 9.623e-05  Data: 0.006 (0.009)\n",
            "Train: 10 [ 350/2354 ( 15%)]  Loss: 2.129 (2.84)  Time: 0.464s,   17.25/s  (0.474s,   16.88/s)  LR: 9.623e-05  Data: 0.006 (0.009)\n",
            "Train: 10 [ 400/2354 ( 17%)]  Loss: 3.272 (2.83)  Time: 0.464s,   17.25/s  (0.473s,   16.92/s)  LR: 9.623e-05  Data: 0.007 (0.009)\n",
            "Train: 10 [ 450/2354 ( 19%)]  Loss: 2.200 (2.83)  Time: 0.466s,   17.18/s  (0.472s,   16.95/s)  LR: 9.623e-05  Data: 0.009 (0.009)\n",
            "Train: 10 [ 500/2354 ( 21%)]  Loss: 4.682 (2.85)  Time: 0.463s,   17.28/s  (0.471s,   16.98/s)  LR: 9.623e-05  Data: 0.008 (0.009)\n",
            "Train: 10 [ 550/2354 ( 23%)]  Loss: 3.283 (2.84)  Time: 0.461s,   17.34/s  (0.472s,   16.95/s)  LR: 9.623e-05  Data: 0.009 (0.009)\n",
            "Train: 10 [ 600/2354 ( 25%)]  Loss: 3.238 (2.82)  Time: 0.469s,   17.06/s  (0.471s,   16.97/s)  LR: 9.623e-05  Data: 0.010 (0.009)\n",
            "Train: 10 [ 650/2354 ( 28%)]  Loss: 2.794 (2.83)  Time: 0.464s,   17.24/s  (0.471s,   16.99/s)  LR: 9.623e-05  Data: 0.006 (0.009)\n",
            "Train: 10 [ 700/2354 ( 30%)]  Loss: 2.478 (2.82)  Time: 0.461s,   17.36/s  (0.471s,   17.00/s)  LR: 9.623e-05  Data: 0.006 (0.008)\n",
            "Train: 10 [ 750/2354 ( 32%)]  Loss: 2.736 (2.82)  Time: 0.677s,   11.82/s  (0.471s,   16.99/s)  LR: 9.623e-05  Data: 0.018 (0.008)\n",
            "Train: 10 [ 800/2354 ( 34%)]  Loss: 2.501 (2.82)  Time: 0.463s,   17.29/s  (0.471s,   16.98/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [ 850/2354 ( 36%)]  Loss: 1.756 (2.82)  Time: 0.462s,   17.31/s  (0.471s,   17.00/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [ 900/2354 ( 38%)]  Loss: 4.183 (2.83)  Time: 0.471s,   16.97/s  (0.470s,   17.01/s)  LR: 9.623e-05  Data: 0.009 (0.008)\n",
            "Train: 10 [ 950/2354 ( 40%)]  Loss: 2.674 (2.83)  Time: 0.468s,   17.11/s  (0.470s,   17.02/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [1000/2354 ( 42%)]  Loss: 1.940 (2.83)  Time: 0.473s,   16.90/s  (0.471s,   17.00/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [1050/2354 ( 45%)]  Loss: 3.101 (2.84)  Time: 0.467s,   17.13/s  (0.470s,   17.01/s)  LR: 9.623e-05  Data: 0.009 (0.008)\n",
            "Train: 10 [1100/2354 ( 47%)]  Loss: 3.386 (2.84)  Time: 0.468s,   17.11/s  (0.470s,   17.02/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [1150/2354 ( 49%)]  Loss: 2.053 (2.85)  Time: 0.461s,   17.35/s  (0.470s,   17.03/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [1200/2354 ( 51%)]  Loss: 2.708 (2.86)  Time: 0.462s,   17.30/s  (0.470s,   17.04/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [1250/2354 ( 53%)]  Loss: 2.193 (2.86)  Time: 0.471s,   16.99/s  (0.470s,   17.02/s)  LR: 9.623e-05  Data: 0.006 (0.008)\n",
            "Train: 10 [1300/2354 ( 55%)]  Loss: 2.714 (2.86)  Time: 0.468s,   17.10/s  (0.470s,   17.03/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [1350/2354 ( 57%)]  Loss: 2.350 (2.86)  Time: 0.465s,   17.20/s  (0.470s,   17.03/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [1400/2354 ( 59%)]  Loss: 1.624 (2.86)  Time: 0.466s,   17.16/s  (0.469s,   17.04/s)  LR: 9.623e-05  Data: 0.010 (0.008)\n",
            "Train: 10 [1450/2354 ( 62%)]  Loss: 3.434 (2.86)  Time: 0.566s,   14.13/s  (0.470s,   17.03/s)  LR: 9.623e-05  Data: 0.019 (0.008)\n",
            "Train: 10 [1500/2354 ( 64%)]  Loss: 1.812 (2.86)  Time: 0.460s,   17.39/s  (0.470s,   17.04/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [1550/2354 ( 66%)]  Loss: 1.865 (2.86)  Time: 0.474s,   16.88/s  (0.469s,   17.04/s)  LR: 9.623e-05  Data: 0.009 (0.008)\n",
            "Train: 10 [1600/2354 ( 68%)]  Loss: 3.742 (2.86)  Time: 0.466s,   17.17/s  (0.469s,   17.05/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [1650/2354 ( 70%)]  Loss: 4.148 (2.86)  Time: 0.465s,   17.22/s  (0.469s,   17.05/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [1700/2354 ( 72%)]  Loss: 2.196 (2.87)  Time: 0.473s,   16.91/s  (0.469s,   17.04/s)  LR: 9.623e-05  Data: 0.009 (0.008)\n",
            "Train: 10 [1750/2354 ( 74%)]  Loss: 3.032 (2.86)  Time: 0.462s,   17.31/s  (0.469s,   17.04/s)  LR: 9.623e-05  Data: 0.008 (0.008)\n",
            "Train: 10 [1800/2354 ( 76%)]  Loss: 2.602 (2.86)  Time: 0.468s,   17.09/s  (0.469s,   17.05/s)  LR: 9.623e-05  Data: 0.009 (0.008)\n",
            "Train: 10 [1850/2354 ( 79%)]  Loss: 3.402 (2.87)  Time: 0.461s,   17.37/s  (0.469s,   17.05/s)  LR: 9.623e-05  Data: 0.006 (0.008)\n",
            "Train: 10 [1900/2354 ( 81%)]  Loss: 2.783 (2.87)  Time: 0.461s,   17.34/s  (0.469s,   17.06/s)  LR: 9.623e-05  Data: 0.006 (0.008)\n",
            "Train: 10 [1950/2354 ( 83%)]  Loss: 3.020 (2.87)  Time: 0.460s,   17.38/s  (0.469s,   17.04/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [2000/2354 ( 85%)]  Loss: 2.796 (2.87)  Time: 0.460s,   17.38/s  (0.469s,   17.05/s)  LR: 9.623e-05  Data: 0.006 (0.008)\n",
            "Train: 10 [2050/2354 ( 87%)]  Loss: 2.339 (2.87)  Time: 0.464s,   17.22/s  (0.469s,   17.05/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [2100/2354 ( 89%)]  Loss: 2.000 (2.87)  Time: 0.470s,   17.00/s  (0.469s,   17.05/s)  LR: 9.623e-05  Data: 0.008 (0.008)\n",
            "Train: 10 [2150/2354 ( 91%)]  Loss: 2.069 (2.87)  Time: 0.464s,   17.25/s  (0.469s,   17.04/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [2200/2354 ( 93%)]  Loss: 4.167 (2.87)  Time: 0.466s,   17.16/s  (0.469s,   17.05/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [2250/2354 ( 96%)]  Loss: 2.525 (2.86)  Time: 0.462s,   17.32/s  (0.469s,   17.05/s)  LR: 9.623e-05  Data: 0.007 (0.008)\n",
            "Train: 10 [2300/2354 ( 98%)]  Loss: 3.564 (2.87)  Time: 0.459s,   17.41/s  (0.469s,   17.05/s)  LR: 9.623e-05  Data: 0.008 (0.008)\n",
            "Train: 10 [2350/2354 (100%)]  Loss: 3.008 (2.87)  Time: 0.455s,   17.57/s  (0.469s,   17.06/s)  LR: 9.623e-05  Data: 0.005 (0.008)\n",
            "Train: 10 [2353/2354 (100%)]  Loss: 2.407 (2.87)  Time: 0.454s,   17.63/s  (0.469s,   17.06/s)  LR: 9.623e-05  Data: 0.000 (0.008)\n",
            "Test: [   0/2354]  Time: 0.633 (0.633)  Loss:  1.5273 (1.5273)  Acc@1: 50.0000 (50.0000)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [  50/2354]  Time: 0.093 (0.154)  Loss:  3.5625 (1.2866)  Acc@1: 25.0000 (75.9804)  Acc@5: 50.0000 (93.6275)\n",
            "Test: [ 100/2354]  Time: 0.101 (0.161)  Loss:  2.7773 (1.4878)  Acc@1: 25.0000 (64.6040)  Acc@5: 50.0000 (92.5743)\n",
            "Test: [ 150/2354]  Time: 0.079 (0.152)  Loss:  0.8086 (1.3737)  Acc@1: 75.0000 (69.0397)  Acc@5: 100.0000 (94.7020)\n",
            "Test: [ 200/2354]  Time: 0.179 (0.147)  Loss:  2.0391 (1.3135)  Acc@1: 50.0000 (71.6418)  Acc@5: 100.0000 (94.7761)\n",
            "Test: [ 250/2354]  Time: 0.171 (0.144)  Loss:  0.2421 (1.4274)  Acc@1: 100.0000 (67.2311)  Acc@5: 100.0000 (93.0279)\n",
            "Test: [ 300/2354]  Time: 0.105 (0.143)  Loss:  1.6025 (1.3972)  Acc@1: 100.0000 (69.3522)  Acc@5: 100.0000 (93.1063)\n",
            "Test: [ 350/2354]  Time: 0.183 (0.143)  Loss:  1.6221 (1.4016)  Acc@1: 25.0000 (68.9459)  Acc@5: 100.0000 (93.3761)\n",
            "Test: [ 400/2354]  Time: 0.123 (0.142)  Loss:  0.8979 (1.3854)  Acc@1: 75.0000 (69.2020)  Acc@5: 100.0000 (93.3915)\n",
            "Test: [ 450/2354]  Time: 0.181 (0.141)  Loss:  0.4800 (1.3649)  Acc@1: 75.0000 (69.6231)  Acc@5: 100.0000 (93.2373)\n",
            "Test: [ 500/2354]  Time: 0.168 (0.140)  Loss:  1.6006 (1.3295)  Acc@1: 50.0000 (69.8603)  Acc@5: 100.0000 (93.4631)\n",
            "Test: [ 550/2354]  Time: 0.123 (0.139)  Loss:  1.7754 (1.3431)  Acc@1: 75.0000 (68.1034)  Acc@5: 75.0000 (93.5118)\n",
            "Test: [ 600/2354]  Time: 0.118 (0.139)  Loss:  1.3262 (1.3243)  Acc@1: 75.0000 (69.2180)  Acc@5: 100.0000 (93.7188)\n",
            "Test: [ 650/2354]  Time: 0.142 (0.139)  Loss:  0.4587 (1.3067)  Acc@1: 100.0000 (69.8541)  Acc@5: 100.0000 (93.9708)\n",
            "Test: [ 700/2354]  Time: 0.136 (0.139)  Loss:  0.3228 (1.2904)  Acc@1: 100.0000 (70.6847)  Acc@5: 100.0000 (94.0442)\n",
            "Test: [ 750/2354]  Time: 0.149 (0.139)  Loss:  1.3672 (1.2986)  Acc@1: 75.0000 (70.7723)  Acc@5: 100.0000 (94.0413)\n",
            "Test: [ 800/2354]  Time: 0.150 (0.138)  Loss:  1.1973 (1.3192)  Acc@1: 50.0000 (69.9126)  Acc@5: 100.0000 (93.9451)\n",
            "Test: [ 850/2354]  Time: 0.148 (0.139)  Loss:  0.9302 (1.3249)  Acc@1: 100.0000 (69.8296)  Acc@5: 100.0000 (93.8014)\n",
            "Test: [ 900/2354]  Time: 0.106 (0.140)  Loss:  1.3027 (1.3059)  Acc@1: 75.0000 (70.5882)  Acc@5: 75.0000 (94.0622)\n",
            "Test: [ 950/2354]  Time: 0.095 (0.139)  Loss:  0.6865 (1.2834)  Acc@1: 100.0000 (71.2145)  Acc@5: 100.0000 (94.2692)\n",
            "Test: [1000/2354]  Time: 0.087 (0.139)  Loss:  1.7080 (1.2895)  Acc@1:  0.0000 (70.4545)  Acc@5: 100.0000 (94.2807)\n",
            "Test: [1050/2354]  Time: 0.158 (0.139)  Loss:  0.9253 (1.2983)  Acc@1: 75.0000 (70.0523)  Acc@5: 100.0000 (94.1484)\n",
            "Test: [1100/2354]  Time: 0.128 (0.139)  Loss:  1.0684 (1.2877)  Acc@1: 100.0000 (70.5041)  Acc@5: 100.0000 (94.3006)\n",
            "Test: [1150/2354]  Time: 0.101 (0.139)  Loss:  2.3633 (1.2725)  Acc@1: 25.0000 (70.9383)  Acc@5: 100.0000 (94.3527)\n",
            "Test: [1200/2354]  Time: 0.141 (0.139)  Loss:  1.1074 (1.2668)  Acc@1: 75.0000 (71.0450)  Acc@5: 75.0000 (94.4421)\n",
            "Test: [1250/2354]  Time: 0.195 (0.139)  Loss:  0.3523 (1.2543)  Acc@1: 100.0000 (71.6627)  Acc@5: 100.0000 (94.5444)\n",
            "Test: [1300/2354]  Time: 0.135 (0.138)  Loss:  0.6826 (1.2525)  Acc@1: 100.0000 (71.5603)  Acc@5: 100.0000 (94.4850)\n",
            "Test: [1350/2354]  Time: 0.118 (0.138)  Loss:  0.9707 (1.2612)  Acc@1: 100.0000 (71.4101)  Acc@5: 100.0000 (94.3745)\n",
            "Test: [1400/2354]  Time: 0.105 (0.138)  Loss:  0.9004 (1.2584)  Acc@1: 75.0000 (71.6631)  Acc@5: 100.0000 (94.3612)\n",
            "Test: [1450/2354]  Time: 0.091 (0.138)  Loss:  0.9404 (1.2422)  Acc@1: 100.0000 (72.2261)  Acc@5: 100.0000 (94.5210)\n",
            "Test: [1500/2354]  Time: 0.182 (0.138)  Loss:  1.0381 (1.2463)  Acc@1: 75.0000 (72.0520)  Acc@5: 100.0000 (94.4037)\n",
            "Test: [1550/2354]  Time: 0.190 (0.138)  Loss:  0.4382 (1.2545)  Acc@1: 100.0000 (71.6151)  Acc@5: 100.0000 (94.3746)\n",
            "Test: [1600/2354]  Time: 0.209 (0.138)  Loss:  0.9438 (1.2548)  Acc@1: 100.0000 (71.6583)  Acc@5: 100.0000 (94.4410)\n",
            "Test: [1650/2354]  Time: 0.076 (0.139)  Loss:  1.1025 (1.2550)  Acc@1: 75.0000 (71.8353)  Acc@5: 100.0000 (94.3973)\n",
            "Test: [1700/2354]  Time: 0.114 (0.139)  Loss:  0.7139 (1.2512)  Acc@1: 100.0000 (72.1193)  Acc@5: 100.0000 (94.4444)\n",
            "Test: [1750/2354]  Time: 0.140 (0.138)  Loss:  1.4736 (1.2624)  Acc@1: 75.0000 (71.5877)  Acc@5: 100.0000 (94.3746)\n",
            "Test: [1800/2354]  Time: 0.122 (0.138)  Loss:  1.4580 (1.2652)  Acc@1: 75.0000 (71.4325)  Acc@5: 100.0000 (94.2393)\n",
            "Test: [1850/2354]  Time: 0.165 (0.138)  Loss:  1.8135 (1.2640)  Acc@1: 50.0000 (71.3803)  Acc@5: 100.0000 (94.2058)\n",
            "Test: [1900/2354]  Time: 0.183 (0.138)  Loss:  1.3906 (1.2673)  Acc@1: 75.0000 (71.4624)  Acc@5: 75.0000 (94.0689)\n",
            "Test: [1950/2354]  Time: 0.131 (0.138)  Loss:  2.7520 (1.2716)  Acc@1:  0.0000 (71.5146)  Acc@5: 75.0000 (93.9390)\n",
            "Test: [2000/2354]  Time: 0.084 (0.138)  Loss:  0.4539 (1.2721)  Acc@1: 100.0000 (71.5267)  Acc@5: 100.0000 (93.9530)\n",
            "Test: [2050/2354]  Time: 0.115 (0.138)  Loss:  2.8105 (1.2848)  Acc@1: 25.0000 (71.0263)  Acc@5: 75.0000 (93.6373)\n",
            "Test: [2100/2354]  Time: 0.105 (0.138)  Loss:  1.3320 (1.2855)  Acc@1: 100.0000 (71.1209)  Acc@5: 100.0000 (93.7173)\n",
            "Test: [2150/2354]  Time: 0.157 (0.138)  Loss:  1.2207 (1.2962)  Acc@1: 100.0000 (71.1065)  Acc@5: 100.0000 (93.6657)\n",
            "Test: [2200/2354]  Time: 0.189 (0.137)  Loss:  2.8359 (1.3056)  Acc@1: 25.0000 (70.7179)  Acc@5: 50.0000 (93.6733)\n",
            "Test: [2250/2354]  Time: 0.107 (0.137)  Loss:  0.7500 (1.3050)  Acc@1: 75.0000 (70.6575)  Acc@5: 100.0000 (93.6473)\n",
            "Test: [2300/2354]  Time: 0.185 (0.137)  Loss:  0.7510 (1.3042)  Acc@1: 100.0000 (70.6975)  Acc@5: 100.0000 (93.5789)\n",
            "Test: [2350/2354]  Time: 0.075 (0.137)  Loss:  2.1055 (1.3066)  Acc@1: 75.0000 (70.6295)  Acc@5: 100.0000 (93.5347)\n",
            "Test: [2354/2354]  Time: 0.064 (0.137)  Loss:  1.0361 (1.3069)  Acc@1: 100.0000 (70.6444)  Acc@5: 100.0000 (93.5450)\n",
            "ERROR: Exception '[Errno 2] No such file or directory: 'drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-0.pth.tar'' while deleting checkpoint\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-10.pth.tar', 70.64444208514705)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-9.pth.tar', 68.34058817284213)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-8.pth.tar', 64.23187174859326)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-7.pth.tar', 58.424461196265995)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-6.pth.tar', 53.67873447368398)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-5.pth.tar', 44.633188236543155)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-4.pth.tar', 39.08058180273915)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-3.pth.tar', 23.29334324238242)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-2.pth.tar', 16.477333050217645)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-1.pth.tar', 6.009130480942775)\n",
            "\n",
            "Train: 11 [   0/2354 (  0%)]  Loss: 3.412 (3.41)  Time: 1.278s,    6.26/s  (1.278s,    6.26/s)  LR: 9.545e-05  Data: 0.538 (0.538)\n",
            "Train: 11 [  50/2354 (  2%)]  Loss: 2.117 (2.89)  Time: 0.464s,   17.23/s  (0.524s,   15.27/s)  LR: 9.545e-05  Data: 0.007 (0.019)\n",
            "Train: 11 [ 100/2354 (  4%)]  Loss: 2.642 (2.82)  Time: 0.459s,   17.41/s  (0.495s,   16.17/s)  LR: 9.545e-05  Data: 0.006 (0.013)\n",
            "Train: 11 [ 150/2354 (  6%)]  Loss: 3.330 (2.79)  Time: 0.461s,   17.37/s  (0.485s,   16.50/s)  LR: 9.545e-05  Data: 0.007 (0.011)\n",
            "Train: 11 [ 200/2354 (  8%)]  Loss: 3.199 (2.77)  Time: 0.477s,   16.77/s  (0.480s,   16.66/s)  LR: 9.545e-05  Data: 0.009 (0.010)\n",
            "Train: 11 [ 250/2354 ( 11%)]  Loss: 2.112 (2.75)  Time: 0.463s,   17.29/s  (0.480s,   16.65/s)  LR: 9.545e-05  Data: 0.007 (0.010)\n",
            "Train: 11 [ 300/2354 ( 13%)]  Loss: 2.563 (2.77)  Time: 0.465s,   17.21/s  (0.478s,   16.74/s)  LR: 9.545e-05  Data: 0.006 (0.010)\n",
            "Train: 11 [ 350/2354 ( 15%)]  Loss: 3.089 (2.74)  Time: 0.465s,   17.19/s  (0.476s,   16.80/s)  LR: 9.545e-05  Data: 0.010 (0.009)\n",
            "Train: 11 [ 400/2354 ( 17%)]  Loss: 3.780 (2.76)  Time: 0.464s,   17.23/s  (0.475s,   16.85/s)  LR: 9.545e-05  Data: 0.006 (0.009)\n",
            "Train: 11 [ 450/2354 ( 19%)]  Loss: 2.243 (2.77)  Time: 0.460s,   17.37/s  (0.474s,   16.88/s)  LR: 9.545e-05  Data: 0.007 (0.009)\n",
            "Train: 11 [ 500/2354 ( 21%)]  Loss: 3.711 (2.77)  Time: 0.467s,   17.12/s  (0.475s,   16.85/s)  LR: 9.545e-05  Data: 0.008 (0.009)\n",
            "Train: 11 [ 550/2354 ( 23%)]  Loss: 3.083 (2.75)  Time: 0.466s,   17.17/s  (0.474s,   16.88/s)  LR: 9.545e-05  Data: 0.009 (0.009)\n",
            "Train: 11 [ 600/2354 ( 25%)]  Loss: 2.372 (2.75)  Time: 0.462s,   17.33/s  (0.473s,   16.91/s)  LR: 9.545e-05  Data: 0.009 (0.009)\n",
            "Train: 11 [ 650/2354 ( 28%)]  Loss: 2.633 (2.75)  Time: 0.467s,   17.14/s  (0.473s,   16.93/s)  LR: 9.545e-05  Data: 0.006 (0.009)\n",
            "Train: 11 [ 700/2354 ( 30%)]  Loss: 2.718 (2.75)  Time: 0.459s,   17.44/s  (0.473s,   16.90/s)  LR: 9.545e-05  Data: 0.006 (0.009)\n",
            "Train: 11 [ 750/2354 ( 32%)]  Loss: 3.734 (2.75)  Time: 0.465s,   17.22/s  (0.473s,   16.92/s)  LR: 9.545e-05  Data: 0.007 (0.009)\n",
            "Train: 11 [ 800/2354 ( 34%)]  Loss: 1.977 (2.75)  Time: 0.476s,   16.79/s  (0.472s,   16.94/s)  LR: 9.545e-05  Data: 0.007 (0.009)\n",
            "Train: 11 [ 850/2354 ( 36%)]  Loss: 3.142 (2.74)  Time: 0.460s,   17.39/s  (0.472s,   16.95/s)  LR: 9.545e-05  Data: 0.007 (0.008)\n",
            "Train: 11 [ 900/2354 ( 38%)]  Loss: 2.786 (2.74)  Time: 0.466s,   17.16/s  (0.472s,   16.96/s)  LR: 9.545e-05  Data: 0.008 (0.008)\n",
            "Train: 11 [ 950/2354 ( 40%)]  Loss: 2.994 (2.73)  Time: 0.469s,   17.07/s  (0.472s,   16.94/s)  LR: 9.545e-05  Data: 0.009 (0.008)\n",
            "Train: 11 [1000/2354 ( 42%)]  Loss: 2.901 (2.73)  Time: 0.465s,   17.22/s  (0.472s,   16.96/s)  LR: 9.545e-05  Data: 0.010 (0.008)\n",
            "Train: 11 [1050/2354 ( 45%)]  Loss: 2.419 (2.73)  Time: 0.460s,   17.39/s  (0.471s,   16.97/s)  LR: 9.545e-05  Data: 0.008 (0.008)\n",
            "Train: 11 [1100/2354 ( 47%)]  Loss: 2.132 (2.73)  Time: 0.469s,   17.04/s  (0.471s,   16.98/s)  LR: 9.545e-05  Data: 0.010 (0.008)\n",
            "Train: 11 [1150/2354 ( 49%)]  Loss: 1.805 (2.72)  Time: 0.653s,   12.25/s  (0.471s,   16.97/s)  LR: 9.545e-05  Data: 0.019 (0.008)\n",
            "Train: 11 [1200/2354 ( 51%)]  Loss: 2.683 (2.72)  Time: 0.472s,   16.94/s  (0.471s,   16.97/s)  LR: 9.545e-05  Data: 0.006 (0.008)\n",
            "Train: 11 [1250/2354 ( 53%)]  Loss: 2.346 (2.73)  Time: 0.474s,   16.89/s  (0.471s,   16.98/s)  LR: 9.545e-05  Data: 0.007 (0.008)\n",
            "Train: 11 [1300/2354 ( 55%)]  Loss: 3.178 (2.74)  Time: 0.463s,   17.28/s  (0.471s,   16.99/s)  LR: 9.545e-05  Data: 0.009 (0.008)\n",
            "Train: 11 [1350/2354 ( 57%)]  Loss: 1.730 (2.73)  Time: 0.475s,   16.85/s  (0.471s,   17.00/s)  LR: 9.545e-05  Data: 0.011 (0.008)\n",
            "Train: 11 [1400/2354 ( 59%)]  Loss: 3.267 (2.74)  Time: 0.462s,   17.31/s  (0.471s,   16.98/s)  LR: 9.545e-05  Data: 0.007 (0.008)\n",
            "Train: 11 [1450/2354 ( 62%)]  Loss: 4.264 (2.74)  Time: 0.466s,   17.16/s  (0.471s,   16.99/s)  LR: 9.545e-05  Data: 0.009 (0.008)\n",
            "Train: 11 [1500/2354 ( 64%)]  Loss: 2.553 (2.74)  Time: 0.463s,   17.29/s  (0.471s,   16.99/s)  LR: 9.545e-05  Data: 0.007 (0.008)\n",
            "Train: 11 [1550/2354 ( 66%)]  Loss: 1.666 (2.74)  Time: 0.464s,   17.25/s  (0.471s,   17.00/s)  LR: 9.545e-05  Data: 0.008 (0.008)\n",
            "Train: 11 [1600/2354 ( 68%)]  Loss: 3.621 (2.75)  Time: 0.637s,   12.55/s  (0.471s,   16.99/s)  LR: 9.545e-05  Data: 0.018 (0.008)\n",
            "Train: 11 [1650/2354 ( 70%)]  Loss: 2.884 (2.75)  Time: 0.460s,   17.38/s  (0.471s,   16.99/s)  LR: 9.545e-05  Data: 0.006 (0.008)\n",
            "Train: 11 [1700/2354 ( 72%)]  Loss: 2.899 (2.74)  Time: 0.475s,   16.83/s  (0.471s,   17.00/s)  LR: 9.545e-05  Data: 0.007 (0.008)\n",
            "Train: 11 [1750/2354 ( 74%)]  Loss: 2.425 (2.74)  Time: 0.464s,   17.25/s  (0.470s,   17.01/s)  LR: 9.545e-05  Data: 0.008 (0.008)\n",
            "Train: 11 [1800/2354 ( 76%)]  Loss: 2.834 (2.75)  Time: 0.466s,   17.15/s  (0.470s,   17.01/s)  LR: 9.545e-05  Data: 0.007 (0.008)\n",
            "Train: 11 [1850/2354 ( 79%)]  Loss: 3.192 (2.75)  Time: 0.474s,   16.89/s  (0.471s,   17.00/s)  LR: 9.545e-05  Data: 0.008 (0.008)\n",
            "Train: 11 [1900/2354 ( 81%)]  Loss: 2.168 (2.75)  Time: 0.471s,   16.97/s  (0.471s,   17.00/s)  LR: 9.545e-05  Data: 0.009 (0.008)\n",
            "Train: 11 [1950/2354 ( 83%)]  Loss: 4.009 (2.75)  Time: 0.467s,   17.13/s  (0.470s,   17.00/s)  LR: 9.545e-05  Data: 0.009 (0.008)\n",
            "Train: 11 [2000/2354 ( 85%)]  Loss: 2.890 (2.75)  Time: 0.460s,   17.41/s  (0.470s,   17.01/s)  LR: 9.545e-05  Data: 0.006 (0.008)\n",
            "Train: 11 [2050/2354 ( 87%)]  Loss: 2.436 (2.75)  Time: 0.465s,   17.21/s  (0.471s,   17.00/s)  LR: 9.545e-05  Data: 0.006 (0.008)\n",
            "Train: 11 [2100/2354 ( 89%)]  Loss: 2.289 (2.76)  Time: 0.464s,   17.23/s  (0.471s,   17.00/s)  LR: 9.545e-05  Data: 0.008 (0.008)\n",
            "Train: 11 [2150/2354 ( 91%)]  Loss: 1.719 (2.76)  Time: 0.468s,   17.08/s  (0.470s,   17.01/s)  LR: 9.545e-05  Data: 0.006 (0.008)\n",
            "Train: 11 [2200/2354 ( 93%)]  Loss: 1.961 (2.75)  Time: 0.463s,   17.27/s  (0.470s,   17.01/s)  LR: 9.545e-05  Data: 0.007 (0.008)\n",
            "Train: 11 [2250/2354 ( 96%)]  Loss: 1.878 (2.75)  Time: 0.459s,   17.43/s  (0.470s,   17.02/s)  LR: 9.545e-05  Data: 0.006 (0.008)\n",
            "Train: 11 [2300/2354 ( 98%)]  Loss: 3.550 (2.75)  Time: 0.468s,   17.11/s  (0.470s,   17.01/s)  LR: 9.545e-05  Data: 0.008 (0.008)\n",
            "Train: 11 [2350/2354 (100%)]  Loss: 2.382 (2.75)  Time: 0.457s,   17.52/s  (0.470s,   17.01/s)  LR: 9.545e-05  Data: 0.005 (0.008)\n",
            "Train: 11 [2353/2354 (100%)]  Loss: 2.583 (2.75)  Time: 0.458s,   17.47/s  (0.470s,   17.01/s)  LR: 9.545e-05  Data: 0.000 (0.008)\n",
            "Test: [   0/2354]  Time: 0.620 (0.620)  Loss:  1.4326 (1.4326)  Acc@1: 50.0000 (50.0000)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [  50/2354]  Time: 0.134 (0.147)  Loss:  3.2891 (1.1966)  Acc@1: 25.0000 (76.4706)  Acc@5: 25.0000 (93.6275)\n",
            "Test: [ 100/2354]  Time: 0.147 (0.140)  Loss:  3.9395 (1.5062)  Acc@1:  0.0000 (70.0495)  Acc@5: 50.0000 (94.0594)\n",
            "Test: [ 150/2354]  Time: 0.166 (0.139)  Loss:  0.6738 (1.4144)  Acc@1: 100.0000 (70.5298)  Acc@5: 100.0000 (94.5364)\n",
            "Test: [ 200/2354]  Time: 0.170 (0.139)  Loss:  1.6572 (1.3044)  Acc@1: 75.0000 (74.1294)  Acc@5: 100.0000 (95.0249)\n",
            "Test: [ 250/2354]  Time: 0.114 (0.138)  Loss:  0.1864 (1.4132)  Acc@1: 100.0000 (70.4183)  Acc@5: 100.0000 (93.4263)\n",
            "Test: [ 300/2354]  Time: 0.095 (0.137)  Loss:  1.9961 (1.3862)  Acc@1: 25.0000 (71.5947)  Acc@5: 100.0000 (93.3555)\n",
            "Test: [ 350/2354]  Time: 0.147 (0.136)  Loss:  2.4746 (1.3923)  Acc@1: 25.0000 (68.5185)  Acc@5: 75.0000 (93.6610)\n",
            "Test: [ 400/2354]  Time: 0.125 (0.136)  Loss:  0.7310 (1.3573)  Acc@1: 75.0000 (69.3267)  Acc@5: 100.0000 (93.9526)\n",
            "Test: [ 450/2354]  Time: 0.161 (0.135)  Loss:  0.2959 (1.3286)  Acc@1: 100.0000 (70.1774)  Acc@5: 100.0000 (93.9024)\n",
            "Test: [ 500/2354]  Time: 0.145 (0.139)  Loss:  0.9126 (1.2902)  Acc@1: 100.0000 (71.2575)  Acc@5: 100.0000 (94.3613)\n",
            "Test: [ 550/2354]  Time: 0.146 (0.138)  Loss:  1.4229 (1.2873)  Acc@1: 75.0000 (70.8711)  Acc@5: 75.0000 (94.4646)\n",
            "Test: [ 600/2354]  Time: 0.138 (0.138)  Loss:  1.0371 (1.2637)  Acc@1: 75.0000 (72.0050)  Acc@5: 100.0000 (94.5923)\n",
            "Test: [ 650/2354]  Time: 0.171 (0.138)  Loss:  0.5918 (1.2340)  Acc@1: 75.0000 (73.0799)  Acc@5: 100.0000 (94.7773)\n",
            "Test: [ 700/2354]  Time: 0.127 (0.137)  Loss:  0.3586 (1.2219)  Acc@1: 100.0000 (73.7161)  Acc@5: 100.0000 (94.9358)\n",
            "Test: [ 750/2354]  Time: 0.161 (0.137)  Loss:  1.1133 (1.2022)  Acc@1: 75.0000 (74.4008)  Acc@5: 100.0000 (95.0399)\n",
            "Test: [ 800/2354]  Time: 0.083 (0.137)  Loss:  0.2966 (1.2215)  Acc@1: 100.0000 (73.8140)  Acc@5: 100.0000 (94.7878)\n",
            "Test: [ 850/2354]  Time: 0.094 (0.137)  Loss:  0.6611 (1.2222)  Acc@1: 100.0000 (73.4724)  Acc@5: 100.0000 (94.8296)\n",
            "Test: [ 900/2354]  Time: 0.162 (0.137)  Loss:  1.3105 (1.2159)  Acc@1: 75.0000 (73.9179)  Acc@5: 75.0000 (94.9501)\n",
            "Test: [ 950/2354]  Time: 0.173 (0.137)  Loss:  0.6611 (1.1957)  Acc@1: 75.0000 (74.2902)  Acc@5: 100.0000 (95.1104)\n",
            "Test: [1000/2354]  Time: 0.129 (0.137)  Loss:  0.4302 (1.1983)  Acc@1: 100.0000 (74.3257)  Acc@5: 100.0000 (95.1049)\n",
            "Test: [1050/2354]  Time: 0.121 (0.136)  Loss:  1.0654 (1.1907)  Acc@1: 75.0000 (74.6670)  Acc@5: 100.0000 (95.1713)\n",
            "Test: [1100/2354]  Time: 0.089 (0.136)  Loss:  0.7764 (1.1811)  Acc@1: 100.0000 (75.0000)  Acc@5: 100.0000 (95.2770)\n",
            "Test: [1150/2354]  Time: 0.160 (0.136)  Loss:  2.7871 (1.1683)  Acc@1: 50.0000 (75.3910)  Acc@5: 100.0000 (95.3301)\n",
            "Test: [1200/2354]  Time: 0.164 (0.136)  Loss:  0.9131 (1.1589)  Acc@1: 75.0000 (75.4580)  Acc@5: 100.0000 (95.3997)\n",
            "Test: [1250/2354]  Time: 0.165 (0.137)  Loss:  0.2362 (1.1413)  Acc@1: 100.0000 (75.9992)  Acc@5: 100.0000 (95.4436)\n",
            "Test: [1300/2354]  Time: 0.093 (0.137)  Loss:  0.2610 (1.1344)  Acc@1: 100.0000 (75.7302)  Acc@5: 100.0000 (95.4650)\n",
            "Test: [1350/2354]  Time: 0.149 (0.137)  Loss:  0.3079 (1.1335)  Acc@1: 100.0000 (75.8882)  Acc@5: 100.0000 (95.5033)\n",
            "Test: [1400/2354]  Time: 0.091 (0.137)  Loss:  0.9893 (1.1343)  Acc@1: 50.0000 (75.9279)  Acc@5: 100.0000 (95.4675)\n",
            "Test: [1450/2354]  Time: 0.079 (0.137)  Loss:  0.7090 (1.1230)  Acc@1: 100.0000 (76.0510)  Acc@5: 100.0000 (95.5720)\n",
            "Test: [1500/2354]  Time: 0.158 (0.137)  Loss:  0.9619 (1.1273)  Acc@1: 100.0000 (75.9494)  Acc@5: 100.0000 (95.5363)\n",
            "Test: [1550/2354]  Time: 0.077 (0.136)  Loss:  0.4026 (1.1365)  Acc@1: 100.0000 (75.5803)  Acc@5: 100.0000 (95.5190)\n",
            "Test: [1600/2354]  Time: 0.147 (0.136)  Loss:  0.4734 (1.1400)  Acc@1: 100.0000 (75.3435)  Acc@5: 100.0000 (95.5028)\n",
            "Test: [1650/2354]  Time: 0.143 (0.136)  Loss:  1.2520 (1.1422)  Acc@1: 50.0000 (75.1969)  Acc@5: 100.0000 (95.4573)\n",
            "Test: [1700/2354]  Time: 0.142 (0.136)  Loss:  0.2766 (1.1405)  Acc@1: 100.0000 (75.2352)  Acc@5: 100.0000 (95.4733)\n",
            "Test: [1750/2354]  Time: 0.100 (0.136)  Loss:  0.8589 (1.1410)  Acc@1: 100.0000 (75.0714)  Acc@5: 100.0000 (95.5454)\n",
            "Test: [1800/2354]  Time: 0.113 (0.136)  Loss:  1.9082 (1.1356)  Acc@1: 75.0000 (75.2776)  Acc@5: 75.0000 (95.5441)\n",
            "Test: [1850/2354]  Time: 0.156 (0.136)  Loss:  0.6934 (1.1309)  Acc@1: 100.0000 (75.4457)  Acc@5: 100.0000 (95.5565)\n",
            "Test: [1900/2354]  Time: 0.112 (0.136)  Loss:  1.2539 (1.1302)  Acc@1: 75.0000 (75.5655)  Acc@5: 75.0000 (95.5024)\n",
            "Test: [1950/2354]  Time: 0.109 (0.136)  Loss:  0.9277 (1.1312)  Acc@1: 100.0000 (75.7048)  Acc@5: 100.0000 (95.4511)\n",
            "Test: [2000/2354]  Time: 0.164 (0.137)  Loss:  1.1562 (1.1328)  Acc@1: 75.0000 (75.8496)  Acc@5: 100.0000 (95.4398)\n",
            "Test: [2050/2354]  Time: 0.118 (0.137)  Loss:  3.2930 (1.1466)  Acc@1:  0.0000 (75.0488)  Acc@5: 50.0000 (95.3437)\n",
            "Test: [2100/2354]  Time: 0.161 (0.137)  Loss:  1.2334 (1.1473)  Acc@1: 50.0000 (75.0714)  Acc@5: 100.0000 (95.3594)\n",
            "Test: [2150/2354]  Time: 0.104 (0.137)  Loss:  1.9551 (1.1595)  Acc@1: 50.0000 (74.6281)  Acc@5: 100.0000 (95.2580)\n",
            "Test: [2200/2354]  Time: 0.151 (0.136)  Loss:  0.5054 (1.1657)  Acc@1: 100.0000 (74.4207)  Acc@5: 100.0000 (95.2181)\n",
            "Test: [2250/2354]  Time: 0.121 (0.136)  Loss:  0.2136 (1.1678)  Acc@1: 100.0000 (74.3892)  Acc@5: 100.0000 (95.1577)\n",
            "Test: [2300/2354]  Time: 0.198 (0.136)  Loss:  0.5522 (1.1674)  Acc@1: 100.0000 (74.5437)  Acc@5: 100.0000 (95.1651)\n",
            "Test: [2350/2354]  Time: 0.075 (0.136)  Loss:  1.2217 (1.1674)  Acc@1: 75.0000 (74.4577)  Acc@5: 100.0000 (95.1935)\n",
            "Test: [2354/2354]  Time: 0.064 (0.136)  Loss:  1.0371 (1.1672)  Acc@1: 66.6667 (74.4877)  Acc@5: 100.0000 (95.2012)\n",
            "ERROR: Exception '[Errno 2] No such file or directory: 'drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-1.pth.tar'' while deleting checkpoint\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-11.pth.tar', 74.48773755337709)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-10.pth.tar', 70.64444208514705)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-9.pth.tar', 68.34058817284213)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-8.pth.tar', 64.23187174859326)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-7.pth.tar', 58.424461196265995)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-6.pth.tar', 53.67873447368398)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-5.pth.tar', 44.633188236543155)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-4.pth.tar', 39.08058180273915)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-3.pth.tar', 23.29334324238242)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-2.pth.tar', 16.477333050217645)\n",
            "\n",
            "Train: 12 [   0/2354 (  0%)]  Loss: 3.470 (3.47)  Time: 1.281s,    6.24/s  (1.281s,    6.24/s)  LR: 9.460e-05  Data: 0.503 (0.503)\n",
            "Train: 12 [  50/2354 (  2%)]  Loss: 1.849 (2.60)  Time: 0.458s,   17.46/s  (0.497s,   16.09/s)  LR: 9.460e-05  Data: 0.007 (0.017)\n",
            "Train: 12 [ 100/2354 (  4%)]  Loss: 1.474 (2.59)  Time: 0.466s,   17.18/s  (0.481s,   16.65/s)  LR: 9.460e-05  Data: 0.007 (0.013)\n",
            "Train: 12 [ 150/2354 (  6%)]  Loss: 2.156 (2.56)  Time: 0.470s,   17.03/s  (0.480s,   16.67/s)  LR: 9.460e-05  Data: 0.007 (0.011)\n",
            "Train: 12 [ 200/2354 (  8%)]  Loss: 2.086 (2.57)  Time: 0.456s,   17.55/s  (0.476s,   16.82/s)  LR: 9.460e-05  Data: 0.006 (0.010)\n",
            "Train: 12 [ 250/2354 ( 11%)]  Loss: 3.839 (2.59)  Time: 0.464s,   17.26/s  (0.473s,   16.91/s)  LR: 9.460e-05  Data: 0.007 (0.010)\n",
            "Train: 12 [ 300/2354 ( 13%)]  Loss: 2.685 (2.60)  Time: 0.464s,   17.24/s  (0.471s,   16.97/s)  LR: 9.460e-05  Data: 0.009 (0.009)\n",
            "Train: 12 [ 350/2354 ( 15%)]  Loss: 1.771 (2.59)  Time: 0.460s,   17.38/s  (0.473s,   16.93/s)  LR: 9.460e-05  Data: 0.007 (0.009)\n",
            "Train: 12 [ 400/2354 ( 17%)]  Loss: 2.417 (2.60)  Time: 0.459s,   17.44/s  (0.471s,   16.97/s)  LR: 9.460e-05  Data: 0.007 (0.009)\n",
            "Train: 12 [ 450/2354 ( 19%)]  Loss: 3.183 (2.62)  Time: 0.462s,   17.32/s  (0.470s,   17.01/s)  LR: 9.460e-05  Data: 0.006 (0.009)\n",
            "Train: 12 [ 500/2354 ( 21%)]  Loss: 3.023 (2.64)  Time: 0.461s,   17.35/s  (0.469s,   17.04/s)  LR: 9.460e-05  Data: 0.006 (0.009)\n",
            "Train: 12 [ 550/2354 ( 23%)]  Loss: 3.310 (2.64)  Time: 0.462s,   17.32/s  (0.469s,   17.06/s)  LR: 9.460e-05  Data: 0.007 (0.009)\n",
            "Train: 12 [ 600/2354 ( 25%)]  Loss: 2.632 (2.63)  Time: 0.468s,   17.09/s  (0.470s,   17.03/s)  LR: 9.460e-05  Data: 0.007 (0.009)\n",
            "Train: 12 [ 650/2354 ( 28%)]  Loss: 1.766 (2.63)  Time: 0.465s,   17.20/s  (0.469s,   17.05/s)  LR: 9.460e-05  Data: 0.009 (0.009)\n",
            "Train: 12 [ 700/2354 ( 30%)]  Loss: 2.172 (2.63)  Time: 0.460s,   17.39/s  (0.469s,   17.07/s)  LR: 9.460e-05  Data: 0.007 (0.009)\n",
            "Train: 12 [ 750/2354 ( 32%)]  Loss: 3.434 (2.64)  Time: 0.461s,   17.34/s  (0.468s,   17.08/s)  LR: 9.460e-05  Data: 0.009 (0.008)\n",
            "Train: 12 [ 800/2354 ( 34%)]  Loss: 4.072 (2.64)  Time: 0.462s,   17.32/s  (0.469s,   17.05/s)  LR: 9.460e-05  Data: 0.007 (0.008)\n",
            "Train: 12 [ 850/2354 ( 36%)]  Loss: 2.791 (2.65)  Time: 0.462s,   17.30/s  (0.469s,   17.07/s)  LR: 9.460e-05  Data: 0.007 (0.008)\n",
            "Train: 12 [ 900/2354 ( 38%)]  Loss: 1.532 (2.65)  Time: 0.465s,   17.22/s  (0.468s,   17.08/s)  LR: 9.460e-05  Data: 0.008 (0.008)\n",
            "Train: 12 [ 950/2354 ( 40%)]  Loss: 3.656 (2.65)  Time: 0.462s,   17.32/s  (0.468s,   17.09/s)  LR: 9.460e-05  Data: 0.006 (0.008)\n",
            "Train: 12 [1000/2354 ( 42%)]  Loss: 2.833 (2.65)  Time: 0.623s,   12.85/s  (0.468s,   17.09/s)  LR: 9.460e-05  Data: 0.019 (0.008)\n",
            "Train: 12 [1050/2354 ( 45%)]  Loss: 3.676 (2.65)  Time: 0.460s,   17.41/s  (0.468s,   17.08/s)  LR: 9.460e-05  Data: 0.007 (0.008)\n",
            "Train: 12 [1100/2354 ( 47%)]  Loss: 2.829 (2.66)  Time: 0.459s,   17.42/s  (0.468s,   17.09/s)  LR: 9.460e-05  Data: 0.007 (0.008)\n",
            "Train: 12 [1150/2354 ( 49%)]  Loss: 4.718 (2.66)  Time: 0.464s,   17.25/s  (0.468s,   17.10/s)  LR: 9.460e-05  Data: 0.010 (0.008)\n",
            "Train: 12 [1200/2354 ( 51%)]  Loss: 2.801 (2.66)  Time: 0.462s,   17.32/s  (0.468s,   17.11/s)  LR: 9.460e-05  Data: 0.007 (0.008)\n",
            "Train: 12 [1250/2354 ( 53%)]  Loss: 2.382 (2.66)  Time: 0.455s,   17.60/s  (0.468s,   17.09/s)  LR: 9.460e-05  Data: 0.007 (0.008)\n",
            "Train: 12 [1300/2354 ( 55%)]  Loss: 2.177 (2.66)  Time: 0.471s,   16.97/s  (0.468s,   17.09/s)  LR: 9.460e-05  Data: 0.009 (0.008)\n",
            "Train: 12 [1350/2354 ( 57%)]  Loss: 2.950 (2.66)  Time: 0.473s,   16.90/s  (0.468s,   17.10/s)  LR: 9.460e-05  Data: 0.006 (0.008)\n",
            "Train: 12 [1400/2354 ( 59%)]  Loss: 1.812 (2.66)  Time: 0.459s,   17.42/s  (0.468s,   17.11/s)  LR: 9.460e-05  Data: 0.007 (0.008)\n",
            "Train: 12 [1450/2354 ( 62%)]  Loss: 1.692 (2.66)  Time: 0.468s,   17.11/s  (0.468s,   17.09/s)  LR: 9.460e-05  Data: 0.008 (0.008)\n",
            "Train: 12 [1500/2354 ( 64%)]  Loss: 3.844 (2.66)  Time: 0.462s,   17.31/s  (0.468s,   17.10/s)  LR: 9.460e-05  Data: 0.009 (0.008)\n",
            "Train: 12 [1550/2354 ( 66%)]  Loss: 2.861 (2.66)  Time: 0.463s,   17.27/s  (0.468s,   17.10/s)  LR: 9.460e-05  Data: 0.010 (0.008)\n",
            "Train: 12 [1600/2354 ( 68%)]  Loss: 1.922 (2.66)  Time: 0.458s,   17.47/s  (0.468s,   17.11/s)  LR: 9.460e-05  Data: 0.009 (0.008)\n",
            "Train: 12 [1650/2354 ( 70%)]  Loss: 2.670 (2.66)  Time: 0.461s,   17.36/s  (0.468s,   17.11/s)  LR: 9.460e-05  Data: 0.007 (0.008)\n",
            "Train: 12 [1700/2354 ( 72%)]  Loss: 2.141 (2.66)  Time: 0.465s,   17.20/s  (0.468s,   17.10/s)  LR: 9.460e-05  Data: 0.010 (0.008)\n",
            "Train: 12 [1750/2354 ( 74%)]  Loss: 1.743 (2.66)  Time: 0.473s,   16.90/s  (0.468s,   17.11/s)  LR: 9.460e-05  Data: 0.007 (0.008)\n",
            "Train: 12 [1800/2354 ( 76%)]  Loss: 3.562 (2.66)  Time: 0.459s,   17.43/s  (0.467s,   17.11/s)  LR: 9.460e-05  Data: 0.006 (0.008)\n",
            "Train: 12 [1850/2354 ( 79%)]  Loss: 3.439 (2.66)  Time: 0.464s,   17.24/s  (0.467s,   17.12/s)  LR: 9.460e-05  Data: 0.010 (0.008)\n",
            "Train: 12 [1900/2354 ( 81%)]  Loss: 2.224 (2.66)  Time: 0.463s,   17.27/s  (0.468s,   17.11/s)  LR: 9.460e-05  Data: 0.009 (0.008)\n",
            "Train: 12 [1950/2354 ( 83%)]  Loss: 2.544 (2.66)  Time: 0.455s,   17.59/s  (0.468s,   17.11/s)  LR: 9.460e-05  Data: 0.007 (0.008)\n",
            "Train: 12 [2000/2354 ( 85%)]  Loss: 3.167 (2.66)  Time: 0.461s,   17.35/s  (0.467s,   17.12/s)  LR: 9.460e-05  Data: 0.006 (0.008)\n",
            "Train: 12 [2050/2354 ( 87%)]  Loss: 1.790 (2.66)  Time: 0.469s,   17.07/s  (0.467s,   17.12/s)  LR: 9.460e-05  Data: 0.010 (0.008)\n",
            "Train: 12 [2100/2354 ( 89%)]  Loss: 2.980 (2.66)  Time: 0.548s,   14.60/s  (0.467s,   17.11/s)  LR: 9.460e-05  Data: 0.007 (0.008)\n",
            "Train: 12 [2150/2354 ( 91%)]  Loss: 2.221 (2.66)  Time: 0.461s,   17.37/s  (0.467s,   17.11/s)  LR: 9.460e-05  Data: 0.008 (0.008)\n",
            "Train: 12 [2200/2354 ( 93%)]  Loss: 3.734 (2.66)  Time: 0.462s,   17.32/s  (0.467s,   17.12/s)  LR: 9.460e-05  Data: 0.008 (0.008)\n",
            "Train: 12 [2250/2354 ( 96%)]  Loss: 3.653 (2.66)  Time: 0.467s,   17.14/s  (0.467s,   17.12/s)  LR: 9.460e-05  Data: 0.007 (0.008)\n",
            "Train: 12 [2300/2354 ( 98%)]  Loss: 2.463 (2.65)  Time: 0.455s,   17.58/s  (0.467s,   17.13/s)  LR: 9.460e-05  Data: 0.007 (0.008)\n",
            "Train: 12 [2350/2354 (100%)]  Loss: 3.218 (2.66)  Time: 0.457s,   17.51/s  (0.467s,   17.12/s)  LR: 9.460e-05  Data: 0.005 (0.008)\n",
            "Train: 12 [2353/2354 (100%)]  Loss: 1.767 (2.66)  Time: 0.451s,   17.73/s  (0.467s,   17.12/s)  LR: 9.460e-05  Data: 0.000 (0.008)\n",
            "Test: [   0/2354]  Time: 0.618 (0.618)  Loss:  1.5049 (1.5049)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [  50/2354]  Time: 0.162 (0.147)  Loss:  3.4004 (1.1788)  Acc@1: 25.0000 (77.9412)  Acc@5: 25.0000 (93.1373)\n",
            "Test: [ 100/2354]  Time: 0.132 (0.140)  Loss:  2.6074 (1.3764)  Acc@1: 25.0000 (71.2871)  Acc@5: 75.0000 (92.0792)\n",
            "Test: [ 150/2354]  Time: 0.113 (0.138)  Loss:  0.2126 (1.2058)  Acc@1: 100.0000 (73.5099)  Acc@5: 100.0000 (94.3709)\n",
            "Test: [ 200/2354]  Time: 0.104 (0.137)  Loss:  2.7656 (1.1242)  Acc@1:  0.0000 (75.6219)  Acc@5: 75.0000 (94.9005)\n",
            "Test: [ 250/2354]  Time: 0.195 (0.137)  Loss:  0.2137 (1.1865)  Acc@1: 100.0000 (74.1036)  Acc@5: 100.0000 (94.7211)\n",
            "Test: [ 300/2354]  Time: 0.125 (0.137)  Loss:  1.0254 (1.1252)  Acc@1: 100.0000 (76.1628)  Acc@5: 100.0000 (94.9336)\n",
            "Test: [ 350/2354]  Time: 0.088 (0.136)  Loss:  1.4180 (1.1271)  Acc@1: 75.0000 (75.2137)  Acc@5: 100.0000 (95.2991)\n",
            "Test: [ 400/2354]  Time: 0.115 (0.136)  Loss:  0.6299 (1.1203)  Acc@1: 100.0000 (75.3117)  Acc@5: 100.0000 (95.0748)\n",
            "Test: [ 450/2354]  Time: 0.134 (0.136)  Loss:  0.3123 (1.0853)  Acc@1: 100.0000 (76.2749)  Acc@5: 100.0000 (95.3991)\n",
            "Test: [ 500/2354]  Time: 0.115 (0.136)  Loss:  1.0400 (1.0537)  Acc@1: 100.0000 (77.2455)  Acc@5: 100.0000 (95.6088)\n",
            "Test: [ 550/2354]  Time: 0.118 (0.136)  Loss:  2.0410 (1.0416)  Acc@1: 75.0000 (77.2686)  Acc@5: 75.0000 (95.8258)\n",
            "Test: [ 600/2354]  Time: 0.278 (0.136)  Loss:  0.5073 (1.0363)  Acc@1: 100.0000 (78.0366)  Acc@5: 100.0000 (95.9651)\n",
            "Test: [ 650/2354]  Time: 0.154 (0.138)  Loss:  0.3420 (1.0143)  Acc@1: 100.0000 (78.3026)  Acc@5: 100.0000 (96.0829)\n",
            "Test: [ 700/2354]  Time: 0.143 (0.138)  Loss:  0.5562 (1.0042)  Acc@1: 100.0000 (78.8160)  Acc@5: 100.0000 (96.0414)\n",
            "Test: [ 750/2354]  Time: 0.129 (0.138)  Loss:  0.7783 (1.0063)  Acc@1: 75.0000 (78.5619)  Acc@5: 100.0000 (96.0053)\n",
            "Test: [ 800/2354]  Time: 0.137 (0.138)  Loss:  0.4626 (1.0301)  Acc@1: 100.0000 (77.7154)  Acc@5: 100.0000 (95.8801)\n",
            "Test: [ 850/2354]  Time: 0.198 (0.138)  Loss:  0.1938 (1.0251)  Acc@1: 100.0000 (77.7615)  Acc@5: 100.0000 (95.8578)\n",
            "Test: [ 900/2354]  Time: 0.206 (0.138)  Loss:  0.8813 (1.0080)  Acc@1: 75.0000 (78.2464)  Acc@5: 75.0000 (96.0322)\n",
            "Test: [ 950/2354]  Time: 0.161 (0.138)  Loss:  0.4250 (0.9867)  Acc@1: 100.0000 (79.0484)  Acc@5: 100.0000 (96.1882)\n",
            "Test: [1000/2354]  Time: 0.162 (0.137)  Loss:  0.4565 (0.9868)  Acc@1: 100.0000 (79.3207)  Acc@5: 100.0000 (96.2787)\n",
            "Test: [1050/2354]  Time: 0.129 (0.137)  Loss:  1.2256 (0.9906)  Acc@1: 50.0000 (79.1865)  Acc@5: 100.0000 (96.2179)\n",
            "Test: [1100/2354]  Time: 0.128 (0.137)  Loss:  1.4619 (0.9915)  Acc@1: 75.0000 (79.1099)  Acc@5: 100.0000 (96.3442)\n",
            "Test: [1150/2354]  Time: 0.086 (0.137)  Loss:  2.1758 (0.9839)  Acc@1: 100.0000 (79.3440)  Acc@5: 100.0000 (96.3076)\n",
            "Test: [1200/2354]  Time: 0.153 (0.137)  Loss:  1.0977 (0.9864)  Acc@1: 75.0000 (79.3505)  Acc@5: 75.0000 (96.2739)\n",
            "Test: [1250/2354]  Time: 0.120 (0.137)  Loss:  0.2472 (0.9720)  Acc@1: 100.0000 (79.8161)  Acc@5: 100.0000 (96.2830)\n",
            "Test: [1300/2354]  Time: 0.083 (0.137)  Loss:  0.7783 (0.9756)  Acc@1: 100.0000 (79.4581)  Acc@5: 100.0000 (96.2913)\n",
            "Test: [1350/2354]  Time: 0.278 (0.138)  Loss:  0.1721 (0.9759)  Acc@1: 100.0000 (79.5522)  Acc@5: 100.0000 (96.2620)\n",
            "Test: [1400/2354]  Time: 0.167 (0.138)  Loss:  0.4548 (0.9782)  Acc@1: 100.0000 (79.5503)  Acc@5: 100.0000 (96.2170)\n",
            "Test: [1450/2354]  Time: 0.092 (0.138)  Loss:  1.0664 (0.9686)  Acc@1: 75.0000 (79.6692)  Acc@5: 100.0000 (96.2957)\n",
            "Test: [1500/2354]  Time: 0.156 (0.138)  Loss:  1.5176 (0.9755)  Acc@1: 50.0000 (79.4137)  Acc@5: 100.0000 (96.2358)\n",
            "Test: [1550/2354]  Time: 0.166 (0.138)  Loss:  0.5425 (1.0002)  Acc@1: 100.0000 (78.6267)  Acc@5: 100.0000 (96.0187)\n",
            "Test: [1600/2354]  Time: 0.091 (0.137)  Loss:  0.3821 (1.0099)  Acc@1: 100.0000 (78.2636)  Acc@5: 100.0000 (95.9869)\n",
            "Test: [1650/2354]  Time: 0.162 (0.137)  Loss:  0.3384 (1.0162)  Acc@1: 100.0000 (77.8619)  Acc@5: 100.0000 (95.9570)\n",
            "Test: [1700/2354]  Time: 0.159 (0.137)  Loss:  0.0492 (1.0077)  Acc@1: 100.0000 (78.1893)  Acc@5: 100.0000 (96.0024)\n",
            "Test: [1750/2354]  Time: 0.118 (0.137)  Loss:  1.3594 (1.0219)  Acc@1: 75.0000 (77.2844)  Acc@5: 100.0000 (96.0594)\n",
            "Test: [1800/2354]  Time: 0.109 (0.137)  Loss:  1.3486 (1.0275)  Acc@1: 75.0000 (77.2765)  Acc@5: 100.0000 (95.9883)\n",
            "Test: [1850/2354]  Time: 0.186 (0.137)  Loss:  0.5454 (1.0229)  Acc@1: 100.0000 (77.3906)  Acc@5: 100.0000 (96.0562)\n",
            "Test: [1900/2354]  Time: 0.172 (0.137)  Loss:  1.5322 (1.0275)  Acc@1: 75.0000 (77.3935)  Acc@5: 75.0000 (95.9890)\n",
            "Test: [1950/2354]  Time: 0.145 (0.137)  Loss:  1.3848 (1.0315)  Acc@1: 100.0000 (77.4218)  Acc@5: 100.0000 (95.9380)\n",
            "Test: [2000/2354]  Time: 0.125 (0.137)  Loss:  0.4216 (1.0303)  Acc@1: 100.0000 (77.5362)  Acc@5: 100.0000 (95.9770)\n",
            "Test: [2050/2354]  Time: 0.158 (0.137)  Loss:  2.3848 (1.0467)  Acc@1: 25.0000 (76.8771)  Acc@5: 75.0000 (95.7582)\n",
            "Test: [2100/2354]  Time: 0.112 (0.138)  Loss:  1.5957 (1.0453)  Acc@1: 75.0000 (76.8801)  Acc@5: 100.0000 (95.7996)\n",
            "Test: [2150/2354]  Time: 0.114 (0.137)  Loss:  2.6250 (1.0633)  Acc@1:  0.0000 (76.1971)  Acc@5: 75.0000 (95.6299)\n",
            "Test: [2200/2354]  Time: 0.124 (0.137)  Loss:  1.9248 (1.0656)  Acc@1: 75.0000 (76.1699)  Acc@5: 100.0000 (95.6383)\n",
            "Test: [2250/2354]  Time: 0.126 (0.137)  Loss:  0.2974 (1.0655)  Acc@1: 100.0000 (76.3327)  Acc@5: 100.0000 (95.6464)\n",
            "Test: [2300/2354]  Time: 0.128 (0.137)  Loss:  0.2375 (1.0631)  Acc@1: 100.0000 (76.4668)  Acc@5: 100.0000 (95.6432)\n",
            "Test: [2350/2354]  Time: 0.075 (0.137)  Loss:  1.4121 (1.0631)  Acc@1: 75.0000 (76.3930)  Acc@5: 100.0000 (95.6614)\n",
            "Test: [2354/2354]  Time: 0.068 (0.137)  Loss:  1.5195 (1.0642)  Acc@1: 66.6667 (76.3882)  Acc@5: 100.0000 (95.6471)\n",
            "ERROR: Exception '[Errno 2] No such file or directory: 'drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-2.pth.tar'' while deleting checkpoint\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-12.pth.tar', 76.388151610071)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-11.pth.tar', 74.48773755337709)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-10.pth.tar', 70.64444208514705)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-9.pth.tar', 68.34058817284213)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-8.pth.tar', 64.23187174859326)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-7.pth.tar', 58.424461196265995)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-6.pth.tar', 53.67873447368398)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-5.pth.tar', 44.633188236543155)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-4.pth.tar', 39.08058180273915)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-3.pth.tar', 23.29334324238242)\n",
            "\n",
            "Train: 13 [   0/2354 (  0%)]  Loss: 2.245 (2.25)  Time: 1.207s,    6.63/s  (1.207s,    6.63/s)  LR: 9.369e-05  Data: 0.488 (0.488)\n",
            "Train: 13 [  50/2354 (  2%)]  Loss: 3.509 (2.57)  Time: 0.478s,   16.75/s  (0.499s,   16.04/s)  LR: 9.369e-05  Data: 0.010 (0.018)\n",
            "Train: 13 [ 100/2354 (  4%)]  Loss: 1.543 (2.66)  Time: 0.466s,   17.17/s  (0.483s,   16.58/s)  LR: 9.369e-05  Data: 0.008 (0.013)\n",
            "Train: 13 [ 150/2354 (  6%)]  Loss: 3.461 (2.60)  Time: 0.459s,   17.42/s  (0.482s,   16.60/s)  LR: 9.369e-05  Data: 0.006 (0.011)\n",
            "Train: 13 [ 200/2354 (  8%)]  Loss: 3.597 (2.62)  Time: 0.465s,   17.20/s  (0.478s,   16.74/s)  LR: 9.369e-05  Data: 0.006 (0.010)\n",
            "Train: 13 [ 250/2354 ( 11%)]  Loss: 1.700 (2.61)  Time: 0.460s,   17.40/s  (0.475s,   16.84/s)  LR: 9.369e-05  Data: 0.007 (0.010)\n",
            "Train: 13 [ 300/2354 ( 13%)]  Loss: 1.960 (2.61)  Time: 0.463s,   17.29/s  (0.473s,   16.90/s)  LR: 9.369e-05  Data: 0.009 (0.009)\n",
            "Train: 13 [ 350/2354 ( 15%)]  Loss: 2.072 (2.58)  Time: 0.472s,   16.94/s  (0.475s,   16.86/s)  LR: 9.369e-05  Data: 0.008 (0.009)\n",
            "Train: 13 [ 400/2354 ( 17%)]  Loss: 2.929 (2.57)  Time: 0.475s,   16.83/s  (0.474s,   16.89/s)  LR: 9.369e-05  Data: 0.006 (0.009)\n",
            "Train: 13 [ 450/2354 ( 19%)]  Loss: 2.934 (2.57)  Time: 0.467s,   17.14/s  (0.473s,   16.92/s)  LR: 9.369e-05  Data: 0.009 (0.009)\n",
            "Train: 13 [ 500/2354 ( 21%)]  Loss: 2.680 (2.56)  Time: 0.460s,   17.39/s  (0.472s,   16.95/s)  LR: 9.369e-05  Data: 0.006 (0.009)\n",
            "Train: 13 [ 550/2354 ( 23%)]  Loss: 2.778 (2.58)  Time: 0.670s,   11.94/s  (0.472s,   16.94/s)  LR: 9.369e-05  Data: 0.013 (0.009)\n",
            "Train: 13 [ 600/2354 ( 25%)]  Loss: 2.347 (2.58)  Time: 0.467s,   17.12/s  (0.472s,   16.94/s)  LR: 9.369e-05  Data: 0.010 (0.009)\n",
            "Train: 13 [ 650/2354 ( 28%)]  Loss: 3.762 (2.59)  Time: 0.463s,   17.28/s  (0.472s,   16.96/s)  LR: 9.369e-05  Data: 0.006 (0.009)\n",
            "Train: 13 [ 700/2354 ( 30%)]  Loss: 1.979 (2.59)  Time: 0.469s,   17.05/s  (0.471s,   16.97/s)  LR: 9.369e-05  Data: 0.010 (0.009)\n",
            "Train: 13 [ 750/2354 ( 32%)]  Loss: 1.595 (2.58)  Time: 0.467s,   17.12/s  (0.471s,   16.99/s)  LR: 9.369e-05  Data: 0.010 (0.008)\n",
            "Train: 13 [ 800/2354 ( 34%)]  Loss: 3.495 (2.59)  Time: 0.467s,   17.13/s  (0.472s,   16.96/s)  LR: 9.369e-05  Data: 0.007 (0.008)\n",
            "Train: 13 [ 850/2354 ( 36%)]  Loss: 2.274 (2.57)  Time: 0.460s,   17.38/s  (0.471s,   16.97/s)  LR: 9.369e-05  Data: 0.007 (0.008)\n",
            "Train: 13 [ 900/2354 ( 38%)]  Loss: 2.611 (2.56)  Time: 0.462s,   17.30/s  (0.471s,   16.98/s)  LR: 9.369e-05  Data: 0.007 (0.008)\n",
            "Train: 13 [ 950/2354 ( 40%)]  Loss: 3.910 (2.57)  Time: 0.469s,   17.05/s  (0.471s,   16.99/s)  LR: 9.369e-05  Data: 0.008 (0.008)\n",
            "Train: 13 [1000/2354 ( 42%)]  Loss: 2.117 (2.58)  Time: 0.473s,   16.93/s  (0.471s,   16.97/s)  LR: 9.369e-05  Data: 0.007 (0.008)\n",
            "Train: 13 [1050/2354 ( 45%)]  Loss: 2.511 (2.58)  Time: 0.465s,   17.19/s  (0.471s,   16.98/s)  LR: 9.369e-05  Data: 0.007 (0.008)\n",
            "Train: 13 [1100/2354 ( 47%)]  Loss: 1.742 (2.59)  Time: 0.475s,   16.84/s  (0.471s,   16.99/s)  LR: 9.369e-05  Data: 0.009 (0.008)\n",
            "Train: 13 [1150/2354 ( 49%)]  Loss: 1.771 (2.59)  Time: 0.467s,   17.12/s  (0.471s,   17.00/s)  LR: 9.369e-05  Data: 0.006 (0.008)\n",
            "Train: 13 [1200/2354 ( 51%)]  Loss: 2.077 (2.60)  Time: 0.479s,   16.68/s  (0.471s,   16.97/s)  LR: 9.369e-05  Data: 0.018 (0.008)\n",
            "Train: 13 [1250/2354 ( 53%)]  Loss: 2.625 (2.59)  Time: 0.462s,   17.32/s  (0.471s,   16.98/s)  LR: 9.369e-05  Data: 0.008 (0.008)\n",
            "Train: 13 [1300/2354 ( 55%)]  Loss: 2.337 (2.59)  Time: 0.465s,   17.20/s  (0.471s,   16.98/s)  LR: 9.369e-05  Data: 0.010 (0.008)\n",
            "Train: 13 [1350/2354 ( 57%)]  Loss: 2.027 (2.59)  Time: 0.464s,   17.23/s  (0.471s,   16.99/s)  LR: 9.369e-05  Data: 0.009 (0.008)\n",
            "Train: 13 [1400/2354 ( 59%)]  Loss: 1.988 (2.59)  Time: 0.464s,   17.23/s  (0.471s,   16.98/s)  LR: 9.369e-05  Data: 0.006 (0.008)\n",
            "Train: 13 [1450/2354 ( 62%)]  Loss: 2.455 (2.58)  Time: 0.467s,   17.12/s  (0.471s,   16.99/s)  LR: 9.369e-05  Data: 0.010 (0.008)\n",
            "Train: 13 [1500/2354 ( 64%)]  Loss: 2.371 (2.58)  Time: 0.469s,   17.07/s  (0.471s,   16.99/s)  LR: 9.369e-05  Data: 0.009 (0.008)\n",
            "Train: 13 [1550/2354 ( 66%)]  Loss: 3.545 (2.58)  Time: 0.460s,   17.38/s  (0.471s,   17.00/s)  LR: 9.369e-05  Data: 0.007 (0.008)\n",
            "Train: 13 [1600/2354 ( 68%)]  Loss: 2.465 (2.59)  Time: 0.609s,   13.13/s  (0.471s,   17.00/s)  LR: 9.369e-05  Data: 0.019 (0.008)\n",
            "Train: 13 [1650/2354 ( 70%)]  Loss: 1.625 (2.59)  Time: 0.461s,   17.35/s  (0.471s,   16.99/s)  LR: 9.369e-05  Data: 0.009 (0.008)\n",
            "Train: 13 [1700/2354 ( 72%)]  Loss: 2.854 (2.59)  Time: 0.462s,   17.32/s  (0.471s,   17.00/s)  LR: 9.369e-05  Data: 0.008 (0.008)\n",
            "Train: 13 [1750/2354 ( 74%)]  Loss: 2.964 (2.59)  Time: 0.467s,   17.14/s  (0.471s,   17.00/s)  LR: 9.369e-05  Data: 0.007 (0.008)\n",
            "Train: 13 [1800/2354 ( 76%)]  Loss: 2.793 (2.59)  Time: 0.463s,   17.27/s  (0.470s,   17.01/s)  LR: 9.369e-05  Data: 0.006 (0.008)\n",
            "Train: 13 [1850/2354 ( 79%)]  Loss: 2.792 (2.59)  Time: 0.471s,   16.98/s  (0.471s,   17.00/s)  LR: 9.369e-05  Data: 0.008 (0.008)\n",
            "Train: 13 [1900/2354 ( 81%)]  Loss: 3.007 (2.59)  Time: 0.465s,   17.20/s  (0.471s,   17.00/s)  LR: 9.369e-05  Data: 0.010 (0.008)\n",
            "Train: 13 [1950/2354 ( 83%)]  Loss: 3.337 (2.59)  Time: 0.466s,   17.18/s  (0.470s,   17.00/s)  LR: 9.369e-05  Data: 0.010 (0.008)\n",
            "Train: 13 [2000/2354 ( 85%)]  Loss: 2.236 (2.60)  Time: 0.464s,   17.25/s  (0.470s,   17.01/s)  LR: 9.369e-05  Data: 0.009 (0.008)\n",
            "Train: 13 [2050/2354 ( 87%)]  Loss: 2.299 (2.59)  Time: 0.462s,   17.30/s  (0.471s,   17.00/s)  LR: 9.369e-05  Data: 0.007 (0.008)\n",
            "Train: 13 [2100/2354 ( 89%)]  Loss: 4.490 (2.59)  Time: 0.465s,   17.20/s  (0.471s,   17.00/s)  LR: 9.369e-05  Data: 0.007 (0.008)\n",
            "Train: 13 [2150/2354 ( 91%)]  Loss: 1.602 (2.60)  Time: 0.474s,   16.89/s  (0.470s,   17.00/s)  LR: 9.369e-05  Data: 0.010 (0.008)\n",
            "Train: 13 [2200/2354 ( 93%)]  Loss: 2.393 (2.59)  Time: 0.484s,   16.54/s  (0.470s,   17.01/s)  LR: 9.369e-05  Data: 0.007 (0.008)\n",
            "Train: 13 [2250/2354 ( 96%)]  Loss: 2.639 (2.59)  Time: 0.461s,   17.35/s  (0.471s,   17.00/s)  LR: 9.369e-05  Data: 0.007 (0.008)\n",
            "Train: 13 [2300/2354 ( 98%)]  Loss: 3.888 (2.59)  Time: 0.462s,   17.32/s  (0.471s,   17.00/s)  LR: 9.369e-05  Data: 0.007 (0.008)\n",
            "Train: 13 [2350/2354 (100%)]  Loss: 3.836 (2.60)  Time: 0.465s,   17.21/s  (0.470s,   17.00/s)  LR: 9.369e-05  Data: 0.005 (0.008)\n",
            "Train: 13 [2353/2354 (100%)]  Loss: 2.137 (2.60)  Time: 0.453s,   17.65/s  (0.470s,   17.00/s)  LR: 9.369e-05  Data: 0.000 (0.008)\n",
            "Test: [   0/2354]  Time: 0.666 (0.666)  Loss:  0.2932 (0.2932)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [  50/2354]  Time: 0.137 (0.154)  Loss:  2.6836 (0.8809)  Acc@1: 25.0000 (82.8431)  Acc@5: 50.0000 (95.5882)\n",
            "Test: [ 100/2354]  Time: 0.088 (0.145)  Loss:  1.9424 (1.1149)  Acc@1: 25.0000 (74.7525)  Acc@5: 100.0000 (95.7921)\n",
            "Test: [ 150/2354]  Time: 0.143 (0.142)  Loss:  1.5146 (0.9666)  Acc@1: 25.0000 (77.4834)  Acc@5: 100.0000 (97.1854)\n",
            "Test: [ 200/2354]  Time: 0.127 (0.140)  Loss:  2.3887 (0.9402)  Acc@1: 25.0000 (75.4975)  Acc@5: 100.0000 (97.1393)\n",
            "Test: [ 250/2354]  Time: 0.101 (0.139)  Loss:  0.5288 (1.0265)  Acc@1: 100.0000 (73.8048)  Acc@5: 100.0000 (96.8127)\n",
            "Test: [ 300/2354]  Time: 0.168 (0.145)  Loss:  0.8491 (1.0019)  Acc@1: 100.0000 (75.4983)  Acc@5: 100.0000 (96.7608)\n",
            "Test: [ 350/2354]  Time: 0.163 (0.144)  Loss:  1.8506 (1.0099)  Acc@1: 25.0000 (74.9288)  Acc@5: 100.0000 (96.9373)\n",
            "Test: [ 400/2354]  Time: 0.120 (0.143)  Loss:  0.3115 (1.0196)  Acc@1: 100.0000 (74.6883)  Acc@5: 100.0000 (96.5087)\n",
            "Test: [ 450/2354]  Time: 0.177 (0.142)  Loss:  0.2986 (0.9736)  Acc@1: 100.0000 (76.2195)  Acc@5: 100.0000 (96.7295)\n",
            "Test: [ 500/2354]  Time: 0.077 (0.141)  Loss:  0.4326 (0.9271)  Acc@1: 100.0000 (77.4950)  Acc@5: 100.0000 (96.9062)\n",
            "Test: [ 550/2354]  Time: 0.144 (0.141)  Loss:  2.2949 (0.9297)  Acc@1: 50.0000 (76.7241)  Acc@5: 75.0000 (96.8240)\n",
            "Test: [ 600/2354]  Time: 0.117 (0.141)  Loss:  1.3418 (0.9225)  Acc@1: 75.0000 (77.1631)  Acc@5: 75.0000 (96.8386)\n",
            "Test: [ 650/2354]  Time: 0.101 (0.140)  Loss:  0.5840 (0.9068)  Acc@1: 75.0000 (77.6882)  Acc@5: 100.0000 (96.8510)\n",
            "Test: [ 700/2354]  Time: 0.086 (0.140)  Loss:  0.6982 (0.8896)  Acc@1: 100.0000 (78.6733)  Acc@5: 100.0000 (96.9330)\n",
            "Test: [ 750/2354]  Time: 0.124 (0.140)  Loss:  1.0605 (0.8964)  Acc@1: 75.0000 (78.8615)  Acc@5: 100.0000 (96.8708)\n",
            "Test: [ 800/2354]  Time: 0.153 (0.140)  Loss:  0.3037 (0.9207)  Acc@1: 100.0000 (77.8714)  Acc@5: 100.0000 (96.7228)\n",
            "Test: [ 850/2354]  Time: 0.153 (0.140)  Loss:  0.2173 (0.9104)  Acc@1: 100.0000 (78.4665)  Acc@5: 100.0000 (96.7098)\n",
            "Test: [ 900/2354]  Time: 0.125 (0.139)  Loss:  1.2900 (0.8884)  Acc@1: 75.0000 (79.2730)  Acc@5: 75.0000 (96.8646)\n",
            "Test: [ 950/2354]  Time: 0.186 (0.139)  Loss:  0.3879 (0.8730)  Acc@1: 100.0000 (79.9159)  Acc@5: 100.0000 (96.9506)\n",
            "Test: [1000/2354]  Time: 0.190 (0.139)  Loss:  0.9556 (0.8795)  Acc@1: 75.0000 (79.8202)  Acc@5: 100.0000 (96.9031)\n",
            "Test: [1050/2354]  Time: 0.117 (0.139)  Loss:  1.0273 (0.8835)  Acc@1: 75.0000 (79.8525)  Acc@5: 100.0000 (96.8839)\n",
            "Test: [1100/2354]  Time: 0.087 (0.139)  Loss:  0.5435 (0.8914)  Acc@1: 100.0000 (79.6322)  Acc@5: 100.0000 (96.9119)\n",
            "Test: [1150/2354]  Time: 0.089 (0.139)  Loss:  1.4951 (0.8788)  Acc@1: 100.0000 (79.9957)  Acc@5: 100.0000 (96.8723)\n",
            "Test: [1200/2354]  Time: 0.151 (0.139)  Loss:  0.8887 (0.8756)  Acc@1: 75.0000 (80.2040)  Acc@5: 100.0000 (96.8360)\n",
            "Test: [1250/2354]  Time: 0.090 (0.139)  Loss:  0.1847 (0.8664)  Acc@1: 100.0000 (80.6155)  Acc@5: 100.0000 (96.8425)\n",
            "Test: [1300/2354]  Time: 0.099 (0.139)  Loss:  0.2727 (0.8601)  Acc@1: 100.0000 (80.5342)  Acc@5: 100.0000 (96.8486)\n",
            "Test: [1350/2354]  Time: 0.178 (0.138)  Loss:  0.1197 (0.8615)  Acc@1: 100.0000 (80.6070)  Acc@5: 100.0000 (96.8542)\n",
            "Test: [1400/2354]  Time: 0.155 (0.140)  Loss:  0.6006 (0.8589)  Acc@1: 100.0000 (80.7994)  Acc@5: 100.0000 (96.8059)\n",
            "Test: [1450/2354]  Time: 0.097 (0.139)  Loss:  0.8818 (0.8554)  Acc@1: 100.0000 (80.9442)  Acc@5: 100.0000 (96.8815)\n",
            "Test: [1500/2354]  Time: 0.095 (0.139)  Loss:  1.6875 (0.8568)  Acc@1: 50.0000 (80.9127)  Acc@5: 100.0000 (96.8854)\n",
            "Test: [1550/2354]  Time: 0.172 (0.139)  Loss:  0.3013 (0.8758)  Acc@1: 100.0000 (80.2063)  Acc@5: 100.0000 (96.8085)\n",
            "Test: [1600/2354]  Time: 0.162 (0.139)  Loss:  0.3501 (0.8819)  Acc@1: 100.0000 (79.8407)  Acc@5: 100.0000 (96.8145)\n",
            "Test: [1650/2354]  Time: 0.182 (0.139)  Loss:  1.2607 (0.8920)  Acc@1: 75.0000 (79.5730)  Acc@5: 100.0000 (96.7747)\n",
            "Test: [1700/2354]  Time: 0.116 (0.139)  Loss:  0.0521 (0.8883)  Acc@1: 100.0000 (79.7325)  Acc@5: 100.0000 (96.7519)\n",
            "Test: [1750/2354]  Time: 0.168 (0.139)  Loss:  1.6445 (0.8941)  Acc@1: 25.0000 (79.4118)  Acc@5: 100.0000 (96.8018)\n",
            "Test: [1800/2354]  Time: 0.141 (0.139)  Loss:  0.4077 (0.8915)  Acc@1: 100.0000 (79.6224)  Acc@5: 100.0000 (96.8629)\n",
            "Test: [1850/2354]  Time: 0.123 (0.139)  Loss:  0.7544 (0.8865)  Acc@1: 100.0000 (79.7002)  Acc@5: 100.0000 (96.9206)\n",
            "Test: [1900/2354]  Time: 0.184 (0.139)  Loss:  1.5889 (0.8849)  Acc@1: 50.0000 (79.8001)  Acc@5: 75.0000 (96.9095)\n",
            "Test: [1950/2354]  Time: 0.216 (0.139)  Loss:  0.6660 (0.8848)  Acc@1: 75.0000 (79.7924)  Acc@5: 100.0000 (96.8478)\n",
            "Test: [2000/2354]  Time: 0.135 (0.139)  Loss:  0.8135 (0.8837)  Acc@1: 75.0000 (79.7726)  Acc@5: 100.0000 (96.8266)\n",
            "Test: [2050/2354]  Time: 0.248 (0.139)  Loss:  2.5410 (0.8960)  Acc@1: 25.0000 (79.4856)  Acc@5: 75.0000 (96.6602)\n",
            "Test: [2100/2354]  Time: 0.148 (0.140)  Loss:  1.2344 (0.8980)  Acc@1: 100.0000 (79.4265)  Acc@5: 100.0000 (96.7040)\n",
            "Test: [2150/2354]  Time: 0.176 (0.140)  Loss:  0.9028 (0.9127)  Acc@1: 100.0000 (79.0795)  Acc@5: 100.0000 (96.6062)\n",
            "Test: [2200/2354]  Time: 0.125 (0.140)  Loss:  1.1777 (0.9267)  Acc@1: 100.0000 (78.4871)  Acc@5: 100.0000 (96.6152)\n",
            "Test: [2250/2354]  Time: 0.138 (0.140)  Loss:  0.2773 (0.9275)  Acc@1: 100.0000 (78.4429)  Acc@5: 100.0000 (96.6459)\n",
            "Test: [2300/2354]  Time: 0.188 (0.139)  Loss:  0.1750 (0.9269)  Acc@1: 100.0000 (78.4876)  Acc@5: 100.0000 (96.6754)\n",
            "Test: [2350/2354]  Time: 0.075 (0.139)  Loss:  1.4482 (0.9285)  Acc@1: 50.0000 (78.2858)  Acc@5: 100.0000 (96.7035)\n",
            "Test: [2354/2354]  Time: 0.065 (0.139)  Loss:  0.6553 (0.9285)  Acc@1: 100.0000 (78.2992)  Acc@5: 100.0000 (96.7088)\n",
            "ERROR: Exception '[Errno 2] No such file or directory: 'drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-3.pth.tar'' while deleting checkpoint\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-13.pth.tar', 78.29918250345047)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-12.pth.tar', 76.388151610071)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-11.pth.tar', 74.48773755337709)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-10.pth.tar', 70.64444208514705)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-9.pth.tar', 68.34058817284213)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-8.pth.tar', 64.23187174859326)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-7.pth.tar', 58.424461196265995)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-6.pth.tar', 53.67873447368398)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-5.pth.tar', 44.633188236543155)\n",
            " ('drive/MyDrive/UNC/H2022/output/20220721-160046-sequencer2d_l-600/checkpoint-4.pth.tar', 39.08058180273915)\n",
            "\n",
            "Train: 14 [   0/2354 (  0%)]  Loss: 2.351 (2.35)  Time: 1.226s,    6.53/s  (1.226s,    6.53/s)  LR: 9.271e-05  Data: 0.496 (0.496)\n",
            "Train: 14 [  50/2354 (  2%)]  Loss: 2.705 (2.39)  Time: 0.465s,   17.19/s  (0.497s,   16.09/s)  LR: 9.271e-05  Data: 0.010 (0.017)\n",
            "Train: 14 [ 100/2354 (  4%)]  Loss: 2.905 (2.37)  Time: 0.467s,   17.13/s  (0.482s,   16.60/s)  LR: 9.271e-05  Data: 0.007 (0.013)\n",
            "Train: 14 [ 150/2354 (  6%)]  Loss: 2.398 (2.39)  Time: 0.470s,   17.01/s  (0.481s,   16.63/s)  LR: 9.271e-05  Data: 0.006 (0.011)\n",
            "Train: 14 [ 200/2354 (  8%)]  Loss: 2.872 (2.40)  Time: 0.459s,   17.44/s  (0.477s,   16.78/s)  LR: 9.271e-05  Data: 0.006 (0.010)\n",
            "Train: 14 [ 250/2354 ( 11%)]  Loss: 3.713 (2.43)  Time: 0.465s,   17.19/s  (0.475s,   16.86/s)  LR: 9.271e-05  Data: 0.007 (0.010)\n",
            "Train: 14 [ 300/2354 ( 13%)]  Loss: 2.498 (2.42)  Time: 0.466s,   17.18/s  (0.473s,   16.91/s)  LR: 9.271e-05  Data: 0.009 (0.009)\n",
            "Train: 14 [ 350/2354 ( 15%)]  Loss: 2.175 (2.43)  Time: 0.476s,   16.80/s  (0.474s,   16.86/s)  LR: 9.271e-05  Data: 0.007 (0.009)\n",
            "Train: 14 [ 400/2354 ( 17%)]  Loss: 2.812 (2.43)  Time: 0.462s,   17.31/s  (0.474s,   16.89/s)  LR: 9.271e-05  Data: 0.007 (0.009)\n",
            "Train: 14 [ 450/2354 ( 19%)]  Loss: 2.319 (2.42)  Time: 0.464s,   17.26/s  (0.473s,   16.93/s)  LR: 9.271e-05  Data: 0.007 (0.009)\n",
            "Train: 14 [ 500/2354 ( 21%)]  Loss: 2.458 (2.43)  Time: 0.459s,   17.42/s  (0.472s,   16.95/s)  LR: 9.271e-05  Data: 0.006 (0.009)\n",
            "Train: 14 [ 550/2354 ( 23%)]  Loss: 4.076 (2.44)  Time: 0.462s,   17.32/s  (0.473s,   16.92/s)  LR: 9.271e-05  Data: 0.007 (0.009)\n",
            "Train: 14 [ 600/2354 ( 25%)]  Loss: 1.871 (2.44)  Time: 0.465s,   17.22/s  (0.472s,   16.94/s)  LR: 9.271e-05  Data: 0.007 (0.009)\n",
            "Train: 14 [ 650/2354 ( 28%)]  Loss: 1.846 (2.45)  Time: 0.473s,   16.92/s  (0.472s,   16.96/s)  LR: 9.271e-05  Data: 0.008 (0.009)\n",
            "Train: 14 [ 700/2354 ( 30%)]  Loss: 2.091 (2.46)  Time: 0.464s,   17.23/s  (0.471s,   16.98/s)  LR: 9.271e-05  Data: 0.006 (0.008)\n",
            "Train: 14 [ 750/2354 ( 32%)]  Loss: 2.306 (2.47)  Time: 0.467s,   17.15/s  (0.472s,   16.95/s)  LR: 9.271e-05  Data: 0.010 (0.008)\n",
            "Train: 14 [ 800/2354 ( 34%)]  Loss: 2.645 (2.46)  Time: 0.459s,   17.43/s  (0.472s,   16.97/s)  LR: 9.271e-05  Data: 0.006 (0.008)\n",
            "Train: 14 [ 850/2354 ( 36%)]  Loss: 1.763 (2.47)  Time: 0.460s,   17.39/s  (0.471s,   16.98/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [ 900/2354 ( 38%)]  Loss: 2.900 (2.47)  Time: 0.463s,   17.29/s  (0.471s,   16.99/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [ 950/2354 ( 40%)]  Loss: 2.574 (2.48)  Time: 0.471s,   16.98/s  (0.471s,   16.97/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [1000/2354 ( 42%)]  Loss: 1.912 (2.48)  Time: 0.464s,   17.25/s  (0.471s,   16.98/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [1050/2354 ( 45%)]  Loss: 3.046 (2.48)  Time: 0.467s,   17.12/s  (0.471s,   16.99/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [1100/2354 ( 47%)]  Loss: 1.892 (2.48)  Time: 0.464s,   17.24/s  (0.471s,   17.00/s)  LR: 9.271e-05  Data: 0.009 (0.008)\n",
            "Train: 14 [1150/2354 ( 49%)]  Loss: 2.015 (2.48)  Time: 0.456s,   17.53/s  (0.471s,   16.98/s)  LR: 9.271e-05  Data: 0.006 (0.008)\n",
            "Train: 14 [1200/2354 ( 51%)]  Loss: 2.004 (2.49)  Time: 0.464s,   17.26/s  (0.471s,   16.99/s)  LR: 9.271e-05  Data: 0.009 (0.008)\n",
            "Train: 14 [1250/2354 ( 53%)]  Loss: 1.896 (2.49)  Time: 0.463s,   17.28/s  (0.471s,   16.99/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [1300/2354 ( 55%)]  Loss: 2.877 (2.50)  Time: 0.465s,   17.20/s  (0.471s,   17.00/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [1350/2354 ( 57%)]  Loss: 2.460 (2.50)  Time: 0.463s,   17.29/s  (0.471s,   16.98/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [1400/2354 ( 59%)]  Loss: 2.603 (2.49)  Time: 0.469s,   17.06/s  (0.471s,   16.99/s)  LR: 9.271e-05  Data: 0.011 (0.008)\n",
            "Train: 14 [1450/2354 ( 62%)]  Loss: 1.860 (2.49)  Time: 0.467s,   17.12/s  (0.471s,   17.00/s)  LR: 9.271e-05  Data: 0.010 (0.008)\n",
            "Train: 14 [1500/2354 ( 64%)]  Loss: 2.957 (2.49)  Time: 0.477s,   16.76/s  (0.470s,   17.00/s)  LR: 9.271e-05  Data: 0.010 (0.008)\n",
            "Train: 14 [1550/2354 ( 66%)]  Loss: 2.417 (2.48)  Time: 0.461s,   17.35/s  (0.471s,   16.99/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [1600/2354 ( 68%)]  Loss: 2.096 (2.49)  Time: 0.465s,   17.20/s  (0.471s,   16.99/s)  LR: 9.271e-05  Data: 0.009 (0.008)\n",
            "Train: 14 [1650/2354 ( 70%)]  Loss: 2.019 (2.49)  Time: 0.469s,   17.07/s  (0.471s,   17.00/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [1700/2354 ( 72%)]  Loss: 1.588 (2.49)  Time: 0.472s,   16.96/s  (0.471s,   17.00/s)  LR: 9.271e-05  Data: 0.009 (0.008)\n",
            "Train: 14 [1750/2354 ( 74%)]  Loss: 1.780 (2.49)  Time: 0.465s,   17.20/s  (0.471s,   16.99/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [1800/2354 ( 76%)]  Loss: 2.414 (2.49)  Time: 0.470s,   17.02/s  (0.471s,   16.99/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [1850/2354 ( 79%)]  Loss: 2.827 (2.50)  Time: 0.460s,   17.41/s  (0.471s,   17.00/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [1900/2354 ( 81%)]  Loss: 1.525 (2.50)  Time: 0.465s,   17.21/s  (0.471s,   17.00/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [1950/2354 ( 83%)]  Loss: 2.927 (2.51)  Time: 0.462s,   17.33/s  (0.471s,   16.99/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [2000/2354 ( 85%)]  Loss: 2.384 (2.51)  Time: 0.473s,   16.90/s  (0.471s,   17.00/s)  LR: 9.271e-05  Data: 0.007 (0.008)\n",
            "Train: 14 [2050/2354 ( 87%)]  Loss: 2.941 (2.51)  Time: 0.475s,   16.85/s  (0.471s,   17.00/s)  LR: 9.271e-05  Data: 0.009 (0.008)\n",
            "Train: 14 [2100/2354 ( 89%)]  Loss: 2.234 (2.51)  Time: 0.468s,   17.10/s  (0.470s,   17.01/s)  LR: 9.271e-05  Data: 0.010 (0.008)\n"
          ]
        }
      ],
      "source": [
        "! python -u -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 ./pytorch-image-models/train.py orchidaceae_train --model sequencer2d_l --opt adabelief --lr 0.0001 --epochs 80 --aa v0r-mstd0.5 --aug-splits 2 --jsd-loss --decay-epochs 3 --cooldown-epochs 0 --weight-decay 1e-4 --sched cosine -b 4 --input-size 3 600 600 --num-classes=300 --vflip 0.5 --hflip 0.5 --amp --pretrained --output drive/MyDrive/UNC/H2022/output/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhBwjDtON9GP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "[Summer] GPU-Sequencer2D",
      "provenance": [],
      "mount_file_id": "1lhmCcyiAhsKUJKf_Uzlw5ARR7lkYrMg2",
      "authorship_tag": "ABX9TyOxtEKL3hxyNJbxGpMqfO1x",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}