{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dakilaledesma/Atomized_NN_Experiments/blob/main/resolution_differences/Sequencer2D_G4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RBv2CGN3gJv",
        "outputId": "3e045b38-9f6c-4d98-f6df-b1edf8b1e523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 21.3 ms, sys: 26 ms, total: 47.3 ms\n",
            "Wall time: 2.44 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "! unzip -q drive/MyDrive/UNC/S2022P/GBIF_O_21_fourth.zip -d /content/temp\n",
        "! mv /content/temp/content/dataset/train /content/train\n",
        "! rm -rf /content/temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXOHMDgapMby",
        "outputId": "ff03e09c-727a-4562-99db-1c2287a479e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-image-models'...\n",
            "remote: Enumerating objects: 10798, done.\u001b[K\n",
            "remote: Counting objects: 100% (325/325), done.\u001b[K\n",
            "remote: Compressing objects: 100% (164/164), done.\u001b[K\n",
            "remote: Total 10798 (delta 184), reused 239 (delta 153), pack-reused 10473\u001b[K\n",
            "Receiving objects: 100% (10798/10798), 20.44 MiB | 19.55 MiB/s, done.\n",
            "Resolving deltas: 100% (7899/7899), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/rwightman/pytorch-image-models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slckThLBilky"
      },
      "outputs": [],
      "source": [
        "import fileinput\n",
        "import sys\n",
        "\n",
        "def replacement(file, previousw, nextw):\n",
        "   for line in fileinput.input(file, inplace=1):\n",
        "       line = line.replace(previousw, nextw)\n",
        "       sys.stdout.write(line)\n",
        "\n",
        "file = \"/content/pytorch-image-models/timm/utils/checkpoint_saver.py\"\n",
        "replacement(file, \"if os.path.exists(last_save_path):\", \"# if os.path.exists(last_save_path):\")\n",
        "replacement(file, \"os.unlink(last_save_path)  # required for Windows support.\", \"# os.unlink(last_save_path)  # required for Windows support.\")\n",
        "replacement(file, \"os.link(last_save_path, save_path)\", \"# os.link(last_save_path, save_path)\")\n",
        "replacement(file, \"os.unlink(best_save_path)\", \"# os.unlink(best_save_path)\")\n",
        "replacement(file, \"os.link(last_save_path, best_save_path)\", \"# os.link(last_save_path, best_save_path)\")\n",
        "replacement(file, \"if os.path.exists(best_save_path):\", \"# if os.path.exists(best_save_path):\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kIBgHy6pSFD",
        "outputId": "27a4c8b6-5a1f-41e6-942c-ebd26c7902d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use_env is set by default in torchrun.\n",
            "If your script expects `--local_rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  FutureWarning,\n",
            "Training with a single process on 1 GPUs.\n",
            "Loading pretrained weights from url (https://github.com/okojoalg/sequencer/releases/download/weights/sequencer2d_l.pth)\n",
            "Downloading: \"https://github.com/okojoalg/sequencer/releases/download/weights/sequencer2d_l.pth\" to /root/.cache/torch/hub/checkpoints/sequencer2d_l.pth\n",
            "Model sequencer2d_l created, param count:54028716\n",
            "Data processing configuration for current model + dataset:\n",
            "\tinput_size: (3, 600, 600)\n",
            "\tinterpolation: bicubic\n",
            "\tmean: (0.485, 0.456, 0.406)\n",
            "\tstd: (0.229, 0.224, 0.225)\n",
            "\tcrop_pct: 0.875\n",
            "Using native Torch AMP. Training in mixed precision.\n",
            "Scheduled epochs: 80\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Train: 0 [   0/649 (  0%)]  Loss: 5.704 (5.70)  Time: 6.774s,    1.18/s  (6.774s,    1.18/s)  LR: 1.000e-04  Data: 0.711 (0.711)\n",
            "Train: 0 [  50/649 (  8%)]  Loss: 5.706 (5.68)  Time: 0.467s,   17.13/s  (0.595s,   13.45/s)  LR: 1.000e-04  Data: 0.006 (0.021)\n",
            "Train: 0 [ 100/649 ( 15%)]  Loss: 5.276 (5.63)  Time: 0.463s,   17.27/s  (0.533s,   15.02/s)  LR: 1.000e-04  Data: 0.008 (0.014)\n",
            "Train: 0 [ 150/649 ( 23%)]  Loss: 5.607 (5.60)  Time: 0.464s,   17.22/s  (0.512s,   15.63/s)  LR: 1.000e-04  Data: 0.006 (0.012)\n",
            "Train: 0 [ 200/649 ( 31%)]  Loss: 5.300 (5.56)  Time: 0.477s,   16.76/s  (0.501s,   15.97/s)  LR: 1.000e-04  Data: 0.007 (0.011)\n",
            "Train: 0 [ 250/649 ( 39%)]  Loss: 5.328 (5.53)  Time: 0.482s,   16.61/s  (0.495s,   16.17/s)  LR: 1.000e-04  Data: 0.009 (0.010)\n",
            "Train: 0 [ 300/649 ( 46%)]  Loss: 5.285 (5.52)  Time: 0.469s,   17.05/s  (0.494s,   16.21/s)  LR: 1.000e-04  Data: 0.008 (0.010)\n",
            "Train: 0 [ 350/649 ( 54%)]  Loss: 5.291 (5.50)  Time: 0.465s,   17.20/s  (0.490s,   16.32/s)  LR: 1.000e-04  Data: 0.007 (0.010)\n",
            "Train: 0 [ 400/649 ( 62%)]  Loss: 5.327 (5.48)  Time: 0.468s,   17.08/s  (0.488s,   16.41/s)  LR: 1.000e-04  Data: 0.007 (0.010)\n",
            "Train: 0 [ 450/649 ( 69%)]  Loss: 5.317 (5.47)  Time: 0.470s,   17.02/s  (0.486s,   16.47/s)  LR: 1.000e-04  Data: 0.010 (0.009)\n",
            "Train: 0 [ 500/649 ( 77%)]  Loss: 5.249 (5.46)  Time: 0.466s,   17.17/s  (0.484s,   16.51/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 0 [ 550/649 ( 85%)]  Loss: 5.047 (5.44)  Time: 0.476s,   16.82/s  (0.483s,   16.56/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 0 [ 600/649 ( 93%)]  Loss: 5.218 (5.43)  Time: 0.624s,   12.83/s  (0.483s,   16.56/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n"
          ]
        }
      ],
      "source": [
        "! python -u -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 ./pytorch-image-models/train.py train --model sequencer2d_l --opt adabelief --lr 0.0001 --epochs 80 --aa v0r-mstd0.5 --aug-splits 2 --jsd-loss --decay-epochs 3 --cooldown-epochs 0 --weight-decay 1e-4 --sched cosine -b 4 --input-size 3 600 600 --num-classes=300 --vflip 0.5 --hflip 0.5 --amp --pretrained --output drive/MyDrive/UNC/S2022P/output/ "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mMHrwfTNvpQI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "GPU-Sequencer2D-G4",
      "provenance": [],
      "mount_file_id": "1b5NI7_Qu3oPDExxm5cq3AnHQJuM-MJ04",
      "authorship_tag": "ABX9TyMjT1xwXYveyQ38UB9SRkQM",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}